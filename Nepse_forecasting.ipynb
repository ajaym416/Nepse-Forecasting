{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nepse_forecasting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXiuwNx6mO7vpNaBZpFLwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaym416/Nepse-Forecasting/blob/main/Nepse_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UGeVP9a38BH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0BBQvS529Er",
        "outputId": "20b17df0-ff99-4093-d41e-afe89bc1f47e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ0kok0g2913"
      },
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"/content/drive/MyDrive/floorsheeet.zip\",\"/content\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zj6S0izPK6m"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CycIcc4y4ClZ"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/floorsheeet.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eiUzUXJN4OvV",
        "outputId": "981f26e2-8d70-4038-db9a-a67df40d764b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SN</th>\n",
              "      <th>ContractNo</th>\n",
              "      <th>Symbol</th>\n",
              "      <th>BuyerBroker</th>\n",
              "      <th>SellerBroker</th>\n",
              "      <th>ShareQty</th>\n",
              "      <th>Rate</th>\n",
              "      <th>Amount</th>\n",
              "      <th>CompanyName</th>\n",
              "      <th>Date</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2020121401015813</td>\n",
              "      <td>RHPL</td>\n",
              "      <td>16</td>\n",
              "      <td>58</td>\n",
              "      <td>40</td>\n",
              "      <td>296</td>\n",
              "      <td>11840</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/14/2020 12:00:00 AM</td>\n",
              "      <td>1303274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2020121401015812</td>\n",
              "      <td>RHPL</td>\n",
              "      <td>16</td>\n",
              "      <td>34</td>\n",
              "      <td>10</td>\n",
              "      <td>296</td>\n",
              "      <td>2960</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/14/2020 12:00:00 AM</td>\n",
              "      <td>1303275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2020121401015775</td>\n",
              "      <td>RHPL</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>40</td>\n",
              "      <td>295</td>\n",
              "      <td>11800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/14/2020 12:00:00 AM</td>\n",
              "      <td>1303276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2020121401015765</td>\n",
              "      <td>RHPL</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>40</td>\n",
              "      <td>295</td>\n",
              "      <td>11800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/14/2020 12:00:00 AM</td>\n",
              "      <td>1303277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2020121401015980</td>\n",
              "      <td>RRHP</td>\n",
              "      <td>43</td>\n",
              "      <td>50</td>\n",
              "      <td>10</td>\n",
              "      <td>262</td>\n",
              "      <td>2620</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/14/2020 12:00:00 AM</td>\n",
              "      <td>1303278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SN        ContractNo Symbol  ...  CompanyName                    Date       ID\n",
              "0   1  2020121401015813   RHPL  ...          NaN  12/14/2020 12:00:00 AM  1303274\n",
              "1   2  2020121401015812   RHPL  ...          NaN  12/14/2020 12:00:00 AM  1303275\n",
              "2   3  2020121401015775   RHPL  ...          NaN  12/14/2020 12:00:00 AM  1303276\n",
              "3   4  2020121401015765   RHPL  ...          NaN  12/14/2020 12:00:00 AM  1303277\n",
              "4   5  2020121401015980   RRHP  ...          NaN  12/14/2020 12:00:00 AM  1303278\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "7X9HKha44QLa",
        "outputId": "21fa6d51-7cef-4c37-e878-8ba0e2e516af"
      },
      "source": [
        "\n",
        "df.drop(['SN','ContractNo','CompanyName','ID'] ,axis=1,inplace=True)\n",
        "df['Date']=pd.to_datetime(df['Date'])\n",
        "df.set_index('Date')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>BuyerBroker</th>\n",
              "      <th>SellerBroker</th>\n",
              "      <th>ShareQty</th>\n",
              "      <th>Rate</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-12-14</th>\n",
              "      <td>RHPL</td>\n",
              "      <td>16</td>\n",
              "      <td>58</td>\n",
              "      <td>40</td>\n",
              "      <td>296</td>\n",
              "      <td>11840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-14</th>\n",
              "      <td>RHPL</td>\n",
              "      <td>16</td>\n",
              "      <td>34</td>\n",
              "      <td>10</td>\n",
              "      <td>296</td>\n",
              "      <td>2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-14</th>\n",
              "      <td>RHPL</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>40</td>\n",
              "      <td>295</td>\n",
              "      <td>11800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-14</th>\n",
              "      <td>RHPL</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>40</td>\n",
              "      <td>295</td>\n",
              "      <td>11800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-12-14</th>\n",
              "      <td>RRHP</td>\n",
              "      <td>43</td>\n",
              "      <td>50</td>\n",
              "      <td>10</td>\n",
              "      <td>262</td>\n",
              "      <td>2620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>STC</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>10</td>\n",
              "      <td>10800</td>\n",
              "      <td>108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NRN</td>\n",
              "      <td>45</td>\n",
              "      <td>29</td>\n",
              "      <td>10</td>\n",
              "      <td>410</td>\n",
              "      <td>4100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>SFCL</td>\n",
              "      <td>44</td>\n",
              "      <td>34</td>\n",
              "      <td>10</td>\n",
              "      <td>203</td>\n",
              "      <td>2030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>UPPER</td>\n",
              "      <td>13</td>\n",
              "      <td>58</td>\n",
              "      <td>18</td>\n",
              "      <td>632</td>\n",
              "      <td>11376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>AHPC</td>\n",
              "      <td>34</td>\n",
              "      <td>46</td>\n",
              "      <td>100</td>\n",
              "      <td>329</td>\n",
              "      <td>32900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7634866 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  BuyerBroker  SellerBroker  ShareQty   Rate  Amount\n",
              "Date                                                                 \n",
              "2020-12-14   RHPL           16            58        40    296   11840\n",
              "2020-12-14   RHPL           16            34        10    296    2960\n",
              "2020-12-14   RHPL            3            22        40    295   11800\n",
              "2020-12-14   RHPL            3            28        40    295   11800\n",
              "2020-12-14   RRHP           43            50        10    262    2620\n",
              "...           ...          ...           ...       ...    ...     ...\n",
              "2021-03-17    STC            3            16        10  10800  108000\n",
              "2021-03-17    NRN           45            29        10    410    4100\n",
              "2021-03-17   SFCL           44            34        10    203    2030\n",
              "2021-03-17  UPPER           13            58        18    632   11376\n",
              "2021-03-17   AHPC           34            46       100    329   32900\n",
              "\n",
              "[7634866 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ED8j1QfEYyc",
        "outputId": "a8bccc91-cf76-4b66-cdde-bdaad383f0d6"
      },
      "source": [
        "df['BuyerBroker'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16,  3, 43, 34, 39, 42, 58, 55, 56, 25, 44, 50, 22, 10, 49, 41, 52,\n",
              "       19, 35, 40, 47,  6, 18, 54, 45, 48, 29, 59, 26, 46, 32, 38, 51, 14,\n",
              "       13, 33, 36, 37, 17,  4, 53, 21, 28,  5,  8,  1, 11, 57, 20,  7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5OMwP_5L7nw",
        "outputId": "7dcb387c-2683-4da2-cf9d-e0d000c269c8"
      },
      "source": [
        "buying_and_selling_broker_list=[]\n",
        "for index in df['BuyerBroker'].unique():\n",
        "  buying_and_selling_broker_list.append(f\"BB_{index}\")\n",
        "  buying_and_selling_broker_list.append(f\"SB_{index}\")\n",
        "buying_and_selling_broker_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BB_16',\n",
              " 'SB_16',\n",
              " 'BB_3',\n",
              " 'SB_3',\n",
              " 'BB_43',\n",
              " 'SB_43',\n",
              " 'BB_34',\n",
              " 'SB_34',\n",
              " 'BB_39',\n",
              " 'SB_39',\n",
              " 'BB_42',\n",
              " 'SB_42',\n",
              " 'BB_58',\n",
              " 'SB_58',\n",
              " 'BB_55',\n",
              " 'SB_55',\n",
              " 'BB_56',\n",
              " 'SB_56',\n",
              " 'BB_25',\n",
              " 'SB_25',\n",
              " 'BB_44',\n",
              " 'SB_44',\n",
              " 'BB_50',\n",
              " 'SB_50',\n",
              " 'BB_22',\n",
              " 'SB_22',\n",
              " 'BB_10',\n",
              " 'SB_10',\n",
              " 'BB_49',\n",
              " 'SB_49',\n",
              " 'BB_41',\n",
              " 'SB_41',\n",
              " 'BB_52',\n",
              " 'SB_52',\n",
              " 'BB_19',\n",
              " 'SB_19',\n",
              " 'BB_35',\n",
              " 'SB_35',\n",
              " 'BB_40',\n",
              " 'SB_40',\n",
              " 'BB_47',\n",
              " 'SB_47',\n",
              " 'BB_6',\n",
              " 'SB_6',\n",
              " 'BB_18',\n",
              " 'SB_18',\n",
              " 'BB_54',\n",
              " 'SB_54',\n",
              " 'BB_45',\n",
              " 'SB_45',\n",
              " 'BB_48',\n",
              " 'SB_48',\n",
              " 'BB_29',\n",
              " 'SB_29',\n",
              " 'BB_59',\n",
              " 'SB_59',\n",
              " 'BB_26',\n",
              " 'SB_26',\n",
              " 'BB_46',\n",
              " 'SB_46',\n",
              " 'BB_32',\n",
              " 'SB_32',\n",
              " 'BB_38',\n",
              " 'SB_38',\n",
              " 'BB_51',\n",
              " 'SB_51',\n",
              " 'BB_14',\n",
              " 'SB_14',\n",
              " 'BB_13',\n",
              " 'SB_13',\n",
              " 'BB_33',\n",
              " 'SB_33',\n",
              " 'BB_36',\n",
              " 'SB_36',\n",
              " 'BB_37',\n",
              " 'SB_37',\n",
              " 'BB_17',\n",
              " 'SB_17',\n",
              " 'BB_4',\n",
              " 'SB_4',\n",
              " 'BB_53',\n",
              " 'SB_53',\n",
              " 'BB_21',\n",
              " 'SB_21',\n",
              " 'BB_28',\n",
              " 'SB_28',\n",
              " 'BB_5',\n",
              " 'SB_5',\n",
              " 'BB_8',\n",
              " 'SB_8',\n",
              " 'BB_1',\n",
              " 'SB_1',\n",
              " 'BB_11',\n",
              " 'SB_11',\n",
              " 'BB_57',\n",
              " 'SB_57',\n",
              " 'BB_20',\n",
              " 'SB_20',\n",
              " 'BB_7',\n",
              " 'SB_7']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UxvS6SOrzcL"
      },
      "source": [
        "final_df = pd.DataFrame(columns=['Date','Symbol','Open','Close','High','Low','High_quantity_transaction',*buying_and_selling_broker_list])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "nbgVljQhM_Mq",
        "outputId": "ffea319e-eb51-4f90-bd5a-74cc7fdd72d2"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>BB_16</th>\n",
              "      <th>SB_16</th>\n",
              "      <th>BB_3</th>\n",
              "      <th>SB_3</th>\n",
              "      <th>BB_43</th>\n",
              "      <th>SB_43</th>\n",
              "      <th>BB_34</th>\n",
              "      <th>SB_34</th>\n",
              "      <th>BB_39</th>\n",
              "      <th>SB_39</th>\n",
              "      <th>BB_42</th>\n",
              "      <th>SB_42</th>\n",
              "      <th>BB_58</th>\n",
              "      <th>SB_58</th>\n",
              "      <th>BB_55</th>\n",
              "      <th>SB_55</th>\n",
              "      <th>BB_56</th>\n",
              "      <th>SB_56</th>\n",
              "      <th>BB_25</th>\n",
              "      <th>SB_25</th>\n",
              "      <th>BB_44</th>\n",
              "      <th>SB_44</th>\n",
              "      <th>BB_50</th>\n",
              "      <th>SB_50</th>\n",
              "      <th>BB_22</th>\n",
              "      <th>SB_22</th>\n",
              "      <th>BB_10</th>\n",
              "      <th>SB_10</th>\n",
              "      <th>BB_49</th>\n",
              "      <th>SB_49</th>\n",
              "      <th>BB_41</th>\n",
              "      <th>SB_41</th>\n",
              "      <th>BB_52</th>\n",
              "      <th>...</th>\n",
              "      <th>BB_32</th>\n",
              "      <th>SB_32</th>\n",
              "      <th>BB_38</th>\n",
              "      <th>SB_38</th>\n",
              "      <th>BB_51</th>\n",
              "      <th>SB_51</th>\n",
              "      <th>BB_14</th>\n",
              "      <th>SB_14</th>\n",
              "      <th>BB_13</th>\n",
              "      <th>SB_13</th>\n",
              "      <th>BB_33</th>\n",
              "      <th>SB_33</th>\n",
              "      <th>BB_36</th>\n",
              "      <th>SB_36</th>\n",
              "      <th>BB_37</th>\n",
              "      <th>SB_37</th>\n",
              "      <th>BB_17</th>\n",
              "      <th>SB_17</th>\n",
              "      <th>BB_4</th>\n",
              "      <th>SB_4</th>\n",
              "      <th>BB_53</th>\n",
              "      <th>SB_53</th>\n",
              "      <th>BB_21</th>\n",
              "      <th>SB_21</th>\n",
              "      <th>BB_28</th>\n",
              "      <th>SB_28</th>\n",
              "      <th>BB_5</th>\n",
              "      <th>SB_5</th>\n",
              "      <th>BB_8</th>\n",
              "      <th>SB_8</th>\n",
              "      <th>BB_1</th>\n",
              "      <th>SB_1</th>\n",
              "      <th>BB_11</th>\n",
              "      <th>SB_11</th>\n",
              "      <th>BB_57</th>\n",
              "      <th>SB_57</th>\n",
              "      <th>BB_20</th>\n",
              "      <th>SB_20</th>\n",
              "      <th>BB_7</th>\n",
              "      <th>SB_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Date, Symbol, Open, Close, High, Low, High_quantity_transaction, BB_16, SB_16, BB_3, SB_3, BB_43, SB_43, BB_34, SB_34, BB_39, SB_39, BB_42, SB_42, BB_58, SB_58, BB_55, SB_55, BB_56, SB_56, BB_25, SB_25, BB_44, SB_44, BB_50, SB_50, BB_22, SB_22, BB_10, SB_10, BB_49, SB_49, BB_41, SB_41, BB_52, SB_52, BB_19, SB_19, BB_35, SB_35, BB_40, SB_40, BB_47, SB_47, BB_6, SB_6, BB_18, SB_18, BB_54, SB_54, BB_45, SB_45, BB_48, SB_48, BB_29, SB_29, BB_59, SB_59, BB_26, SB_26, BB_46, SB_46, BB_32, SB_32, BB_38, SB_38, BB_51, SB_51, BB_14, SB_14, BB_13, SB_13, BB_33, SB_33, BB_36, SB_36, BB_37, SB_37, BB_17, SB_17, BB_4, SB_4, BB_53, SB_53, BB_21, SB_21, BB_28, SB_28, BB_5, SB_5, BB_8, SB_8, BB_1, SB_1, BB_11, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsvQv-6dNEan"
      },
      "source": [
        "Company_symbol = 'NICA'\n",
        "company_df=df[df['Symbol']==Company_symbol]\n",
        "for index,date in enumerate(company_df['Date'].unique()):\n",
        "  Open=company_df[company_df['Date']==date].iloc[0,4]\n",
        "  Close=company_df[company_df['Date']==date].iloc[-1,4]\n",
        "  High=max(company_df[company_df['Date']==date].iloc[:,4])\n",
        "  Low=min(company_df[company_df['Date']==date].iloc[:,4])\n",
        "  High_quantity_value=company_df['ShareQty'].describe()['75%']*4\n",
        "  High_quantity_transaction=np.sum(company_df[company_df['Date']==date]['ShareQty']>High_quantity_value)\n",
        "  Buyer_broker_holding=company_df[company_df['Date']==date].groupby(['BuyerBroker'])['ShareQty'].sum()\n",
        "  Seller_broker_holding=company_df[company_df['Date']==date].groupby(['SellerBroker'])['ShareQty'].sum()\n",
        "  final_df.loc[index,['Date','Symbol','Open','Close','High','Low','High_quantity_transaction']]=[date,Company_symbol,Open,Close,High,Low,High_quantity_transaction]\n",
        "  for key,values in Buyer_broker_holding.items():\n",
        "    final_df.loc[index,f\"BB_{key}\"]=values\n",
        "  for key,values in Seller_broker_holding.items():\n",
        "     final_df.loc[index,f\"SB_{key}\"]=values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyZ7J9LDZBPi"
      },
      "source": [
        "final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "final_df.set_index('Date' , inplace=True)\n",
        "final_df.sort_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Zpfwte_YZYwF",
        "outputId": "849c9bc1-9b9b-43cd-e25a-929acbbc3e91"
      },
      "source": [
        "final_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>BB_16</th>\n",
              "      <th>SB_16</th>\n",
              "      <th>BB_3</th>\n",
              "      <th>SB_3</th>\n",
              "      <th>BB_43</th>\n",
              "      <th>SB_43</th>\n",
              "      <th>BB_34</th>\n",
              "      <th>SB_34</th>\n",
              "      <th>BB_39</th>\n",
              "      <th>SB_39</th>\n",
              "      <th>BB_42</th>\n",
              "      <th>SB_42</th>\n",
              "      <th>BB_58</th>\n",
              "      <th>SB_58</th>\n",
              "      <th>BB_55</th>\n",
              "      <th>SB_55</th>\n",
              "      <th>BB_56</th>\n",
              "      <th>SB_56</th>\n",
              "      <th>BB_25</th>\n",
              "      <th>SB_25</th>\n",
              "      <th>BB_44</th>\n",
              "      <th>SB_44</th>\n",
              "      <th>BB_50</th>\n",
              "      <th>SB_50</th>\n",
              "      <th>BB_22</th>\n",
              "      <th>SB_22</th>\n",
              "      <th>BB_10</th>\n",
              "      <th>SB_10</th>\n",
              "      <th>BB_49</th>\n",
              "      <th>SB_49</th>\n",
              "      <th>BB_41</th>\n",
              "      <th>SB_41</th>\n",
              "      <th>BB_52</th>\n",
              "      <th>SB_52</th>\n",
              "      <th>...</th>\n",
              "      <th>BB_32</th>\n",
              "      <th>SB_32</th>\n",
              "      <th>BB_38</th>\n",
              "      <th>SB_38</th>\n",
              "      <th>BB_51</th>\n",
              "      <th>SB_51</th>\n",
              "      <th>BB_14</th>\n",
              "      <th>SB_14</th>\n",
              "      <th>BB_13</th>\n",
              "      <th>SB_13</th>\n",
              "      <th>BB_33</th>\n",
              "      <th>SB_33</th>\n",
              "      <th>BB_36</th>\n",
              "      <th>SB_36</th>\n",
              "      <th>BB_37</th>\n",
              "      <th>SB_37</th>\n",
              "      <th>BB_17</th>\n",
              "      <th>SB_17</th>\n",
              "      <th>BB_4</th>\n",
              "      <th>SB_4</th>\n",
              "      <th>BB_53</th>\n",
              "      <th>SB_53</th>\n",
              "      <th>BB_21</th>\n",
              "      <th>SB_21</th>\n",
              "      <th>BB_28</th>\n",
              "      <th>SB_28</th>\n",
              "      <th>BB_5</th>\n",
              "      <th>SB_5</th>\n",
              "      <th>BB_8</th>\n",
              "      <th>SB_8</th>\n",
              "      <th>BB_1</th>\n",
              "      <th>SB_1</th>\n",
              "      <th>BB_11</th>\n",
              "      <th>SB_11</th>\n",
              "      <th>BB_57</th>\n",
              "      <th>SB_57</th>\n",
              "      <th>BB_20</th>\n",
              "      <th>SB_20</th>\n",
              "      <th>BB_7</th>\n",
              "      <th>SB_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>168</td>\n",
              "      <td>162</td>\n",
              "      <td>131</td>\n",
              "      <td>129</td>\n",
              "      <td>165</td>\n",
              "      <td>147</td>\n",
              "      <td>173</td>\n",
              "      <td>175</td>\n",
              "      <td>161</td>\n",
              "      <td>161</td>\n",
              "      <td>175</td>\n",
              "      <td>172</td>\n",
              "      <td>179</td>\n",
              "      <td>176</td>\n",
              "      <td>168</td>\n",
              "      <td>162</td>\n",
              "      <td>166</td>\n",
              "      <td>159</td>\n",
              "      <td>166</td>\n",
              "      <td>169</td>\n",
              "      <td>169</td>\n",
              "      <td>165</td>\n",
              "      <td>166</td>\n",
              "      <td>173</td>\n",
              "      <td>170</td>\n",
              "      <td>170</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>175</td>\n",
              "      <td>175</td>\n",
              "      <td>172</td>\n",
              "      <td>169</td>\n",
              "      <td>163</td>\n",
              "      <td>164</td>\n",
              "      <td>...</td>\n",
              "      <td>171</td>\n",
              "      <td>167</td>\n",
              "      <td>171</td>\n",
              "      <td>167</td>\n",
              "      <td>165</td>\n",
              "      <td>164</td>\n",
              "      <td>173</td>\n",
              "      <td>167</td>\n",
              "      <td>158</td>\n",
              "      <td>153</td>\n",
              "      <td>176</td>\n",
              "      <td>172</td>\n",
              "      <td>174</td>\n",
              "      <td>173</td>\n",
              "      <td>145</td>\n",
              "      <td>142</td>\n",
              "      <td>171</td>\n",
              "      <td>172</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>165</td>\n",
              "      <td>159</td>\n",
              "      <td>161</td>\n",
              "      <td>157</td>\n",
              "      <td>171</td>\n",
              "      <td>175</td>\n",
              "      <td>144</td>\n",
              "      <td>155</td>\n",
              "      <td>149</td>\n",
              "      <td>139</td>\n",
              "      <td>167</td>\n",
              "      <td>159</td>\n",
              "      <td>148</td>\n",
              "      <td>161</td>\n",
              "      <td>175</td>\n",
              "      <td>170</td>\n",
              "      <td>114</td>\n",
              "      <td>117</td>\n",
              "      <td>149</td>\n",
              "      <td>148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1</td>\n",
              "      <td>116</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>120</td>\n",
              "      <td>46</td>\n",
              "      <td>150</td>\n",
              "      <td>149</td>\n",
              "      <td>102</td>\n",
              "      <td>101</td>\n",
              "      <td>143</td>\n",
              "      <td>138</td>\n",
              "      <td>166</td>\n",
              "      <td>170</td>\n",
              "      <td>137</td>\n",
              "      <td>149</td>\n",
              "      <td>168</td>\n",
              "      <td>165</td>\n",
              "      <td>172</td>\n",
              "      <td>175</td>\n",
              "      <td>139</td>\n",
              "      <td>150</td>\n",
              "      <td>147</td>\n",
              "      <td>149</td>\n",
              "      <td>154</td>\n",
              "      <td>164</td>\n",
              "      <td>166</td>\n",
              "      <td>161</td>\n",
              "      <td>155</td>\n",
              "      <td>163</td>\n",
              "      <td>153</td>\n",
              "      <td>162</td>\n",
              "      <td>114</td>\n",
              "      <td>140</td>\n",
              "      <td>169</td>\n",
              "      <td>173</td>\n",
              "      <td>162</td>\n",
              "      <td>167</td>\n",
              "      <td>148</td>\n",
              "      <td>150</td>\n",
              "      <td>...</td>\n",
              "      <td>160</td>\n",
              "      <td>153</td>\n",
              "      <td>164</td>\n",
              "      <td>158</td>\n",
              "      <td>138</td>\n",
              "      <td>148</td>\n",
              "      <td>159</td>\n",
              "      <td>158</td>\n",
              "      <td>126</td>\n",
              "      <td>142</td>\n",
              "      <td>169</td>\n",
              "      <td>164</td>\n",
              "      <td>164</td>\n",
              "      <td>165</td>\n",
              "      <td>96</td>\n",
              "      <td>124</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>124</td>\n",
              "      <td>142</td>\n",
              "      <td>127</td>\n",
              "      <td>141</td>\n",
              "      <td>137</td>\n",
              "      <td>146</td>\n",
              "      <td>154</td>\n",
              "      <td>168</td>\n",
              "      <td>104</td>\n",
              "      <td>144</td>\n",
              "      <td>108</td>\n",
              "      <td>119</td>\n",
              "      <td>149</td>\n",
              "      <td>145</td>\n",
              "      <td>106</td>\n",
              "      <td>148</td>\n",
              "      <td>161</td>\n",
              "      <td>166</td>\n",
              "      <td>83</td>\n",
              "      <td>100</td>\n",
              "      <td>105</td>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NICA</td>\n",
              "      <td>580</td>\n",
              "      <td>580</td>\n",
              "      <td>582</td>\n",
              "      <td>570</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>500</td>\n",
              "      <td>200</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>300</td>\n",
              "      <td>1400</td>\n",
              "      <td>20</td>\n",
              "      <td>100</td>\n",
              "      <td>250</td>\n",
              "      <td>1700</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>1100</td>\n",
              "      <td>100</td>\n",
              "      <td>1000</td>\n",
              "      <td>200</td>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>300</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>500</td>\n",
              "      <td>200</td>\n",
              "      <td>100</td>\n",
              "      <td>1280</td>\n",
              "      <td>200</td>\n",
              "      <td>600</td>\n",
              "      <td>100</td>\n",
              "      <td>2850</td>\n",
              "      <td>2100</td>\n",
              "      <td>960</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>600</td>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>200</td>\n",
              "      <td>600</td>\n",
              "      <td>880</td>\n",
              "      <td>300</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>700</td>\n",
              "      <td>1020</td>\n",
              "      <td>500</td>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "      <td>100</td>\n",
              "      <td>150</td>\n",
              "      <td>50</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>50</td>\n",
              "      <td>300</td>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>50</td>\n",
              "      <td>200</td>\n",
              "      <td>300</td>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>500</td>\n",
              "      <td>1000</td>\n",
              "      <td>319</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>180</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Symbol  Open  Close  High  Low  ...  SB_57  BB_20  SB_20  BB_7  SB_7\n",
              "count     180   180    180   180  180  ...    170    114    117   149   148\n",
              "unique      1   116    120   122  120  ...    166     83    100   105   134\n",
              "top      NICA   580    580   582  570  ...    319    500    100   100   100\n",
              "freq      180     5      7     8    6  ...      2      5      5    10     7\n",
              "\n",
              "[4 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIr4U80bakC8",
        "outputId": "b1f371a4-50db-4936-daff-c31eeb2a555d"
      },
      "source": [
        "final_df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Symbol     0\n",
              "Open       0\n",
              "Close      0\n",
              "High       0\n",
              "Low        0\n",
              "          ..\n",
              "SB_57     10\n",
              "BB_20     66\n",
              "SB_20     63\n",
              "BB_7      31\n",
              "SB_7      32\n",
              "Length: 106, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n6CO_qScYpY"
      },
      "source": [
        "final_df.fillna(0 ,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc1G-07awpGV"
      },
      "source": [
        "# RSI 14 DAYS\n",
        "n = 14\n",
        "def rma(x, n, y0):\n",
        "    a = (n-1) / n\n",
        "    ak = a**np.arange(len(x)-1, -1, -1)\n",
        "    return np.r_[np.full(n, np.nan), y0, np.cumsum(ak * x) / ak / n + y0 * a**np.arange(1, len(x)+1)]\n",
        "\n",
        "final_df['change'] = final_df['Close'].diff()\n",
        "final_df['gain'] = final_df.change.mask(final_df.change < 0, 0.0)\n",
        "final_df['loss'] = -final_df.change.mask(final_df.change > 0, -0.0)\n",
        "final_df['avg_gain'] = rma(final_df.gain[n+1:].to_numpy(), n, np.nansum(final_df.gain.to_numpy()[:n+1])/n)\n",
        "final_df['avg_loss'] = rma(final_df.loss[n+1:].to_numpy(), n, np.nansum(final_df.loss.to_numpy()[:n+1])/n)\n",
        "final_df['rs'] = final_df.avg_gain / final_df.avg_loss\n",
        "final_df['rsi_14'] = 100 - (100 / (1 + final_df.rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "Q0N_Rnd5zH9j",
        "outputId": "5008a392-1365-4498-bac6-1064fe4ac367"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>BB_16</th>\n",
              "      <th>SB_16</th>\n",
              "      <th>BB_3</th>\n",
              "      <th>SB_3</th>\n",
              "      <th>BB_43</th>\n",
              "      <th>SB_43</th>\n",
              "      <th>BB_34</th>\n",
              "      <th>SB_34</th>\n",
              "      <th>BB_39</th>\n",
              "      <th>SB_39</th>\n",
              "      <th>BB_42</th>\n",
              "      <th>SB_42</th>\n",
              "      <th>BB_58</th>\n",
              "      <th>SB_58</th>\n",
              "      <th>BB_55</th>\n",
              "      <th>SB_55</th>\n",
              "      <th>BB_56</th>\n",
              "      <th>SB_56</th>\n",
              "      <th>BB_25</th>\n",
              "      <th>SB_25</th>\n",
              "      <th>BB_44</th>\n",
              "      <th>SB_44</th>\n",
              "      <th>BB_50</th>\n",
              "      <th>SB_50</th>\n",
              "      <th>BB_22</th>\n",
              "      <th>SB_22</th>\n",
              "      <th>BB_10</th>\n",
              "      <th>SB_10</th>\n",
              "      <th>BB_49</th>\n",
              "      <th>SB_49</th>\n",
              "      <th>BB_41</th>\n",
              "      <th>SB_41</th>\n",
              "      <th>BB_52</th>\n",
              "      <th>SB_52</th>\n",
              "      <th>...</th>\n",
              "      <th>SB_14</th>\n",
              "      <th>BB_13</th>\n",
              "      <th>SB_13</th>\n",
              "      <th>BB_33</th>\n",
              "      <th>SB_33</th>\n",
              "      <th>BB_36</th>\n",
              "      <th>SB_36</th>\n",
              "      <th>BB_37</th>\n",
              "      <th>SB_37</th>\n",
              "      <th>BB_17</th>\n",
              "      <th>SB_17</th>\n",
              "      <th>BB_4</th>\n",
              "      <th>SB_4</th>\n",
              "      <th>BB_53</th>\n",
              "      <th>SB_53</th>\n",
              "      <th>BB_21</th>\n",
              "      <th>SB_21</th>\n",
              "      <th>BB_28</th>\n",
              "      <th>SB_28</th>\n",
              "      <th>BB_5</th>\n",
              "      <th>SB_5</th>\n",
              "      <th>BB_8</th>\n",
              "      <th>SB_8</th>\n",
              "      <th>BB_1</th>\n",
              "      <th>SB_1</th>\n",
              "      <th>BB_11</th>\n",
              "      <th>SB_11</th>\n",
              "      <th>BB_57</th>\n",
              "      <th>SB_57</th>\n",
              "      <th>BB_20</th>\n",
              "      <th>SB_20</th>\n",
              "      <th>BB_7</th>\n",
              "      <th>SB_7</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>575</td>\n",
              "      <td>572</td>\n",
              "      <td>589</td>\n",
              "      <td>560</td>\n",
              "      <td>17</td>\n",
              "      <td>4650</td>\n",
              "      <td>4461</td>\n",
              "      <td>0</td>\n",
              "      <td>1555</td>\n",
              "      <td>1050</td>\n",
              "      <td>2591</td>\n",
              "      <td>45185</td>\n",
              "      <td>2508</td>\n",
              "      <td>2250</td>\n",
              "      <td>572</td>\n",
              "      <td>140</td>\n",
              "      <td>2034</td>\n",
              "      <td>8747</td>\n",
              "      <td>3116</td>\n",
              "      <td>2700</td>\n",
              "      <td>2342</td>\n",
              "      <td>6308</td>\n",
              "      <td>1058</td>\n",
              "      <td>1551</td>\n",
              "      <td>2732</td>\n",
              "      <td>2510</td>\n",
              "      <td>6505</td>\n",
              "      <td>10170</td>\n",
              "      <td>6522</td>\n",
              "      <td>2550</td>\n",
              "      <td>1774</td>\n",
              "      <td>400</td>\n",
              "      <td>4000</td>\n",
              "      <td>8129</td>\n",
              "      <td>3248</td>\n",
              "      <td>1494</td>\n",
              "      <td>1824</td>\n",
              "      <td>400</td>\n",
              "      <td>940</td>\n",
              "      <td>...</td>\n",
              "      <td>6574</td>\n",
              "      <td>378</td>\n",
              "      <td>2406</td>\n",
              "      <td>8213</td>\n",
              "      <td>11449</td>\n",
              "      <td>1020</td>\n",
              "      <td>12268</td>\n",
              "      <td>2100</td>\n",
              "      <td>5796</td>\n",
              "      <td>100</td>\n",
              "      <td>6170</td>\n",
              "      <td>4800</td>\n",
              "      <td>4600</td>\n",
              "      <td>3510</td>\n",
              "      <td>647</td>\n",
              "      <td>400</td>\n",
              "      <td>3812</td>\n",
              "      <td>15550</td>\n",
              "      <td>9709</td>\n",
              "      <td>500</td>\n",
              "      <td>1254</td>\n",
              "      <td>200</td>\n",
              "      <td>600</td>\n",
              "      <td>2050</td>\n",
              "      <td>2278</td>\n",
              "      <td>270</td>\n",
              "      <td>507</td>\n",
              "      <td>2152</td>\n",
              "      <td>3336</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>580</td>\n",
              "      <td>196</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-10</th>\n",
              "      <td>NICA</td>\n",
              "      <td>526</td>\n",
              "      <td>549</td>\n",
              "      <td>564</td>\n",
              "      <td>526</td>\n",
              "      <td>19</td>\n",
              "      <td>1354</td>\n",
              "      <td>0</td>\n",
              "      <td>550</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>870</td>\n",
              "      <td>10732</td>\n",
              "      <td>3870</td>\n",
              "      <td>710</td>\n",
              "      <td>566</td>\n",
              "      <td>9295</td>\n",
              "      <td>2299</td>\n",
              "      <td>10678</td>\n",
              "      <td>20</td>\n",
              "      <td>200</td>\n",
              "      <td>1000</td>\n",
              "      <td>1150</td>\n",
              "      <td>13199</td>\n",
              "      <td>200</td>\n",
              "      <td>3605</td>\n",
              "      <td>15123</td>\n",
              "      <td>2689</td>\n",
              "      <td>8197</td>\n",
              "      <td>2622</td>\n",
              "      <td>1350</td>\n",
              "      <td>810</td>\n",
              "      <td>0</td>\n",
              "      <td>4150</td>\n",
              "      <td>4867</td>\n",
              "      <td>2350</td>\n",
              "      <td>1025</td>\n",
              "      <td>11729</td>\n",
              "      <td>210</td>\n",
              "      <td>16925</td>\n",
              "      <td>...</td>\n",
              "      <td>3685</td>\n",
              "      <td>0</td>\n",
              "      <td>1772</td>\n",
              "      <td>6940</td>\n",
              "      <td>4240</td>\n",
              "      <td>2500</td>\n",
              "      <td>6970</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>2564</td>\n",
              "      <td>11000</td>\n",
              "      <td>1160</td>\n",
              "      <td>11086</td>\n",
              "      <td>5144</td>\n",
              "      <td>3500</td>\n",
              "      <td>1421</td>\n",
              "      <td>350</td>\n",
              "      <td>2026</td>\n",
              "      <td>901</td>\n",
              "      <td>121</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "      <td>200</td>\n",
              "      <td>3070</td>\n",
              "      <td>1712</td>\n",
              "      <td>2015</td>\n",
              "      <td>420</td>\n",
              "      <td>1640</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>1235</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-11</th>\n",
              "      <td>NICA</td>\n",
              "      <td>567</td>\n",
              "      <td>549</td>\n",
              "      <td>569</td>\n",
              "      <td>520</td>\n",
              "      <td>29</td>\n",
              "      <td>2900</td>\n",
              "      <td>5450</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>550</td>\n",
              "      <td>2583</td>\n",
              "      <td>4490</td>\n",
              "      <td>1660</td>\n",
              "      <td>7811</td>\n",
              "      <td>3306</td>\n",
              "      <td>6470</td>\n",
              "      <td>4039</td>\n",
              "      <td>23698</td>\n",
              "      <td>17959</td>\n",
              "      <td>2010</td>\n",
              "      <td>3667</td>\n",
              "      <td>2710</td>\n",
              "      <td>3422</td>\n",
              "      <td>6550</td>\n",
              "      <td>15787</td>\n",
              "      <td>4259</td>\n",
              "      <td>11540</td>\n",
              "      <td>6108</td>\n",
              "      <td>4142</td>\n",
              "      <td>2040</td>\n",
              "      <td>3750</td>\n",
              "      <td>850</td>\n",
              "      <td>3740</td>\n",
              "      <td>6942</td>\n",
              "      <td>2565</td>\n",
              "      <td>3510</td>\n",
              "      <td>3903</td>\n",
              "      <td>3498</td>\n",
              "      <td>7615</td>\n",
              "      <td>...</td>\n",
              "      <td>5251</td>\n",
              "      <td>1325</td>\n",
              "      <td>712</td>\n",
              "      <td>6580</td>\n",
              "      <td>5554</td>\n",
              "      <td>9050</td>\n",
              "      <td>2612</td>\n",
              "      <td>1400</td>\n",
              "      <td>1500</td>\n",
              "      <td>10485</td>\n",
              "      <td>50365</td>\n",
              "      <td>46250</td>\n",
              "      <td>5031</td>\n",
              "      <td>6050</td>\n",
              "      <td>3460</td>\n",
              "      <td>3688</td>\n",
              "      <td>1303</td>\n",
              "      <td>6612</td>\n",
              "      <td>34884</td>\n",
              "      <td>230</td>\n",
              "      <td>150</td>\n",
              "      <td>400</td>\n",
              "      <td>2200</td>\n",
              "      <td>2370</td>\n",
              "      <td>1410</td>\n",
              "      <td>450</td>\n",
              "      <td>648</td>\n",
              "      <td>4850</td>\n",
              "      <td>3240</td>\n",
              "      <td>2560</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-12</th>\n",
              "      <td>NICA</td>\n",
              "      <td>545</td>\n",
              "      <td>564</td>\n",
              "      <td>569</td>\n",
              "      <td>545</td>\n",
              "      <td>18</td>\n",
              "      <td>2380</td>\n",
              "      <td>1411</td>\n",
              "      <td>0</td>\n",
              "      <td>400</td>\n",
              "      <td>4600</td>\n",
              "      <td>588</td>\n",
              "      <td>4975</td>\n",
              "      <td>1687</td>\n",
              "      <td>3815</td>\n",
              "      <td>2298</td>\n",
              "      <td>4253</td>\n",
              "      <td>3564</td>\n",
              "      <td>12678</td>\n",
              "      <td>22444</td>\n",
              "      <td>3433</td>\n",
              "      <td>821</td>\n",
              "      <td>3637</td>\n",
              "      <td>3621</td>\n",
              "      <td>2710</td>\n",
              "      <td>9936</td>\n",
              "      <td>10666</td>\n",
              "      <td>10252</td>\n",
              "      <td>5550</td>\n",
              "      <td>15414</td>\n",
              "      <td>1330</td>\n",
              "      <td>1000</td>\n",
              "      <td>4748</td>\n",
              "      <td>1331</td>\n",
              "      <td>9487</td>\n",
              "      <td>8477</td>\n",
              "      <td>7146</td>\n",
              "      <td>743</td>\n",
              "      <td>3920</td>\n",
              "      <td>8799</td>\n",
              "      <td>...</td>\n",
              "      <td>7035</td>\n",
              "      <td>460</td>\n",
              "      <td>2119</td>\n",
              "      <td>2510</td>\n",
              "      <td>5455</td>\n",
              "      <td>8101</td>\n",
              "      <td>1700</td>\n",
              "      <td>1600</td>\n",
              "      <td>1346</td>\n",
              "      <td>3410</td>\n",
              "      <td>2087</td>\n",
              "      <td>19550</td>\n",
              "      <td>8120</td>\n",
              "      <td>1915</td>\n",
              "      <td>10478</td>\n",
              "      <td>1270</td>\n",
              "      <td>500</td>\n",
              "      <td>1400</td>\n",
              "      <td>17466</td>\n",
              "      <td>1402</td>\n",
              "      <td>4162</td>\n",
              "      <td>950</td>\n",
              "      <td>0</td>\n",
              "      <td>880</td>\n",
              "      <td>547</td>\n",
              "      <td>700</td>\n",
              "      <td>250</td>\n",
              "      <td>4400</td>\n",
              "      <td>5112</td>\n",
              "      <td>1300</td>\n",
              "      <td>764</td>\n",
              "      <td>700</td>\n",
              "      <td>1075</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-15</th>\n",
              "      <td>NICA</td>\n",
              "      <td>521</td>\n",
              "      <td>545</td>\n",
              "      <td>547</td>\n",
              "      <td>518</td>\n",
              "      <td>16</td>\n",
              "      <td>5060</td>\n",
              "      <td>1500</td>\n",
              "      <td>200</td>\n",
              "      <td>250</td>\n",
              "      <td>125</td>\n",
              "      <td>1150</td>\n",
              "      <td>5875</td>\n",
              "      <td>1310</td>\n",
              "      <td>4317</td>\n",
              "      <td>6251</td>\n",
              "      <td>1955</td>\n",
              "      <td>880</td>\n",
              "      <td>2010</td>\n",
              "      <td>7031</td>\n",
              "      <td>1886</td>\n",
              "      <td>901</td>\n",
              "      <td>4368</td>\n",
              "      <td>4666</td>\n",
              "      <td>11018</td>\n",
              "      <td>1110</td>\n",
              "      <td>18412</td>\n",
              "      <td>14993</td>\n",
              "      <td>2100</td>\n",
              "      <td>3765</td>\n",
              "      <td>665</td>\n",
              "      <td>6186</td>\n",
              "      <td>2200</td>\n",
              "      <td>1236</td>\n",
              "      <td>6310</td>\n",
              "      <td>12230</td>\n",
              "      <td>4657</td>\n",
              "      <td>8784</td>\n",
              "      <td>5000</td>\n",
              "      <td>213</td>\n",
              "      <td>...</td>\n",
              "      <td>2802</td>\n",
              "      <td>740</td>\n",
              "      <td>120</td>\n",
              "      <td>4700</td>\n",
              "      <td>1579</td>\n",
              "      <td>7880</td>\n",
              "      <td>4808</td>\n",
              "      <td>1500</td>\n",
              "      <td>806</td>\n",
              "      <td>3278</td>\n",
              "      <td>14487</td>\n",
              "      <td>16540</td>\n",
              "      <td>35223</td>\n",
              "      <td>5200</td>\n",
              "      <td>2000</td>\n",
              "      <td>1364</td>\n",
              "      <td>386</td>\n",
              "      <td>2700</td>\n",
              "      <td>3300</td>\n",
              "      <td>1850</td>\n",
              "      <td>7417</td>\n",
              "      <td>700</td>\n",
              "      <td>6181</td>\n",
              "      <td>366</td>\n",
              "      <td>2529</td>\n",
              "      <td>450</td>\n",
              "      <td>1050</td>\n",
              "      <td>1870</td>\n",
              "      <td>2450</td>\n",
              "      <td>800</td>\n",
              "      <td>34</td>\n",
              "      <td>470</td>\n",
              "      <td>1962</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>892</td>\n",
              "      <td>887</td>\n",
              "      <td>927</td>\n",
              "      <td>861</td>\n",
              "      <td>14</td>\n",
              "      <td>3630</td>\n",
              "      <td>600</td>\n",
              "      <td>2380</td>\n",
              "      <td>3050</td>\n",
              "      <td>1270</td>\n",
              "      <td>5632</td>\n",
              "      <td>2719</td>\n",
              "      <td>12166</td>\n",
              "      <td>3053</td>\n",
              "      <td>2367</td>\n",
              "      <td>3450</td>\n",
              "      <td>1597</td>\n",
              "      <td>3756</td>\n",
              "      <td>13677</td>\n",
              "      <td>1110</td>\n",
              "      <td>652</td>\n",
              "      <td>10696</td>\n",
              "      <td>4604</td>\n",
              "      <td>600</td>\n",
              "      <td>9337</td>\n",
              "      <td>11136</td>\n",
              "      <td>20876</td>\n",
              "      <td>13575</td>\n",
              "      <td>2186</td>\n",
              "      <td>750</td>\n",
              "      <td>639</td>\n",
              "      <td>2355</td>\n",
              "      <td>0</td>\n",
              "      <td>28732</td>\n",
              "      <td>1422</td>\n",
              "      <td>2210</td>\n",
              "      <td>1644</td>\n",
              "      <td>1683</td>\n",
              "      <td>2372</td>\n",
              "      <td>...</td>\n",
              "      <td>2575</td>\n",
              "      <td>4345</td>\n",
              "      <td>800</td>\n",
              "      <td>21863</td>\n",
              "      <td>7394</td>\n",
              "      <td>2510</td>\n",
              "      <td>500</td>\n",
              "      <td>886</td>\n",
              "      <td>3570</td>\n",
              "      <td>11932</td>\n",
              "      <td>2888</td>\n",
              "      <td>3080</td>\n",
              "      <td>11256</td>\n",
              "      <td>1110</td>\n",
              "      <td>4722</td>\n",
              "      <td>2279</td>\n",
              "      <td>1847</td>\n",
              "      <td>1970</td>\n",
              "      <td>18725</td>\n",
              "      <td>1000</td>\n",
              "      <td>78</td>\n",
              "      <td>2070</td>\n",
              "      <td>0</td>\n",
              "      <td>955</td>\n",
              "      <td>685</td>\n",
              "      <td>280</td>\n",
              "      <td>911</td>\n",
              "      <td>2940</td>\n",
              "      <td>5101</td>\n",
              "      <td>3270</td>\n",
              "      <td>5750</td>\n",
              "      <td>30</td>\n",
              "      <td>785</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.956010</td>\n",
              "      <td>9.032390</td>\n",
              "      <td>0.880831</td>\n",
              "      <td>46.832016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>880</td>\n",
              "      <td>890</td>\n",
              "      <td>909</td>\n",
              "      <td>862</td>\n",
              "      <td>14</td>\n",
              "      <td>3383</td>\n",
              "      <td>1749</td>\n",
              "      <td>3970</td>\n",
              "      <td>1431</td>\n",
              "      <td>8972</td>\n",
              "      <td>0</td>\n",
              "      <td>3369</td>\n",
              "      <td>21202</td>\n",
              "      <td>2528</td>\n",
              "      <td>2351</td>\n",
              "      <td>8089</td>\n",
              "      <td>1824</td>\n",
              "      <td>4385</td>\n",
              "      <td>46906</td>\n",
              "      <td>2169</td>\n",
              "      <td>1834</td>\n",
              "      <td>1299</td>\n",
              "      <td>3803</td>\n",
              "      <td>3257</td>\n",
              "      <td>1616</td>\n",
              "      <td>4400</td>\n",
              "      <td>8473</td>\n",
              "      <td>2092</td>\n",
              "      <td>7157</td>\n",
              "      <td>2435</td>\n",
              "      <td>600</td>\n",
              "      <td>1958</td>\n",
              "      <td>219</td>\n",
              "      <td>19812</td>\n",
              "      <td>6547</td>\n",
              "      <td>5234</td>\n",
              "      <td>1569</td>\n",
              "      <td>4714</td>\n",
              "      <td>3581</td>\n",
              "      <td>...</td>\n",
              "      <td>3008</td>\n",
              "      <td>1850</td>\n",
              "      <td>5017</td>\n",
              "      <td>13965</td>\n",
              "      <td>14942</td>\n",
              "      <td>1060</td>\n",
              "      <td>13005</td>\n",
              "      <td>200</td>\n",
              "      <td>90</td>\n",
              "      <td>4926</td>\n",
              "      <td>3208</td>\n",
              "      <td>4616</td>\n",
              "      <td>3347</td>\n",
              "      <td>5901</td>\n",
              "      <td>2276</td>\n",
              "      <td>1882</td>\n",
              "      <td>1570</td>\n",
              "      <td>3561</td>\n",
              "      <td>6031</td>\n",
              "      <td>1150</td>\n",
              "      <td>182</td>\n",
              "      <td>950</td>\n",
              "      <td>0</td>\n",
              "      <td>5891</td>\n",
              "      <td>2065</td>\n",
              "      <td>930</td>\n",
              "      <td>883</td>\n",
              "      <td>7600</td>\n",
              "      <td>6819</td>\n",
              "      <td>7764</td>\n",
              "      <td>216</td>\n",
              "      <td>1350</td>\n",
              "      <td>930</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602009</td>\n",
              "      <td>8.387219</td>\n",
              "      <td>0.906380</td>\n",
              "      <td>47.544567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>NICA</td>\n",
              "      <td>855</td>\n",
              "      <td>862</td>\n",
              "      <td>876</td>\n",
              "      <td>853</td>\n",
              "      <td>25</td>\n",
              "      <td>3365</td>\n",
              "      <td>2579</td>\n",
              "      <td>6377</td>\n",
              "      <td>310</td>\n",
              "      <td>475</td>\n",
              "      <td>6318</td>\n",
              "      <td>15183</td>\n",
              "      <td>7300</td>\n",
              "      <td>3026</td>\n",
              "      <td>4484</td>\n",
              "      <td>2000</td>\n",
              "      <td>7254</td>\n",
              "      <td>2946</td>\n",
              "      <td>12602</td>\n",
              "      <td>1688</td>\n",
              "      <td>1250</td>\n",
              "      <td>5859</td>\n",
              "      <td>1399</td>\n",
              "      <td>2883</td>\n",
              "      <td>10758</td>\n",
              "      <td>17491</td>\n",
              "      <td>8413</td>\n",
              "      <td>30492</td>\n",
              "      <td>4272</td>\n",
              "      <td>1830</td>\n",
              "      <td>3125</td>\n",
              "      <td>11643</td>\n",
              "      <td>402</td>\n",
              "      <td>17537</td>\n",
              "      <td>6836</td>\n",
              "      <td>2720</td>\n",
              "      <td>11679</td>\n",
              "      <td>11614</td>\n",
              "      <td>2447</td>\n",
              "      <td>...</td>\n",
              "      <td>7118</td>\n",
              "      <td>725</td>\n",
              "      <td>875</td>\n",
              "      <td>16190</td>\n",
              "      <td>5751</td>\n",
              "      <td>1240</td>\n",
              "      <td>1560</td>\n",
              "      <td>238</td>\n",
              "      <td>1590</td>\n",
              "      <td>4903</td>\n",
              "      <td>7256</td>\n",
              "      <td>2089</td>\n",
              "      <td>2682</td>\n",
              "      <td>1550</td>\n",
              "      <td>2217</td>\n",
              "      <td>1720</td>\n",
              "      <td>1209</td>\n",
              "      <td>4972</td>\n",
              "      <td>5115</td>\n",
              "      <td>1060</td>\n",
              "      <td>798</td>\n",
              "      <td>188</td>\n",
              "      <td>2000</td>\n",
              "      <td>947</td>\n",
              "      <td>8068</td>\n",
              "      <td>233</td>\n",
              "      <td>1697</td>\n",
              "      <td>6135</td>\n",
              "      <td>8193</td>\n",
              "      <td>3366</td>\n",
              "      <td>6900</td>\n",
              "      <td>2340</td>\n",
              "      <td>668</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.059009</td>\n",
              "      <td>9.788132</td>\n",
              "      <td>0.721180</td>\n",
              "      <td>41.900337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>NICA</td>\n",
              "      <td>893</td>\n",
              "      <td>878</td>\n",
              "      <td>913</td>\n",
              "      <td>868</td>\n",
              "      <td>13</td>\n",
              "      <td>1229</td>\n",
              "      <td>3870</td>\n",
              "      <td>340</td>\n",
              "      <td>2220</td>\n",
              "      <td>1077</td>\n",
              "      <td>4739</td>\n",
              "      <td>611</td>\n",
              "      <td>2648</td>\n",
              "      <td>2265</td>\n",
              "      <td>889</td>\n",
              "      <td>1700</td>\n",
              "      <td>1888</td>\n",
              "      <td>2691</td>\n",
              "      <td>3198</td>\n",
              "      <td>1050</td>\n",
              "      <td>3475</td>\n",
              "      <td>5937</td>\n",
              "      <td>9769</td>\n",
              "      <td>2098</td>\n",
              "      <td>5547</td>\n",
              "      <td>10388</td>\n",
              "      <td>4386</td>\n",
              "      <td>28557</td>\n",
              "      <td>1420</td>\n",
              "      <td>1260</td>\n",
              "      <td>2327</td>\n",
              "      <td>1300</td>\n",
              "      <td>1995</td>\n",
              "      <td>3685</td>\n",
              "      <td>6724</td>\n",
              "      <td>2554</td>\n",
              "      <td>2771</td>\n",
              "      <td>650</td>\n",
              "      <td>11112</td>\n",
              "      <td>...</td>\n",
              "      <td>1584</td>\n",
              "      <td>360</td>\n",
              "      <td>1130</td>\n",
              "      <td>3499</td>\n",
              "      <td>12309</td>\n",
              "      <td>2688</td>\n",
              "      <td>4125</td>\n",
              "      <td>88</td>\n",
              "      <td>473</td>\n",
              "      <td>611</td>\n",
              "      <td>2583</td>\n",
              "      <td>46735</td>\n",
              "      <td>1710</td>\n",
              "      <td>0</td>\n",
              "      <td>1295</td>\n",
              "      <td>1744</td>\n",
              "      <td>1903</td>\n",
              "      <td>2272</td>\n",
              "      <td>6358</td>\n",
              "      <td>1325</td>\n",
              "      <td>2749</td>\n",
              "      <td>1402</td>\n",
              "      <td>1885</td>\n",
              "      <td>1155</td>\n",
              "      <td>1382</td>\n",
              "      <td>590</td>\n",
              "      <td>2242</td>\n",
              "      <td>3717</td>\n",
              "      <td>1276</td>\n",
              "      <td>546</td>\n",
              "      <td>2490</td>\n",
              "      <td>3626</td>\n",
              "      <td>1591</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.697651</td>\n",
              "      <td>9.088980</td>\n",
              "      <td>0.846921</td>\n",
              "      <td>45.855843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NICA</td>\n",
              "      <td>863</td>\n",
              "      <td>865</td>\n",
              "      <td>874</td>\n",
              "      <td>854</td>\n",
              "      <td>4</td>\n",
              "      <td>3019</td>\n",
              "      <td>760</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>814</td>\n",
              "      <td>723</td>\n",
              "      <td>2923</td>\n",
              "      <td>2964</td>\n",
              "      <td>130</td>\n",
              "      <td>1010</td>\n",
              "      <td>2910</td>\n",
              "      <td>702</td>\n",
              "      <td>876</td>\n",
              "      <td>2911</td>\n",
              "      <td>50</td>\n",
              "      <td>2200</td>\n",
              "      <td>1496</td>\n",
              "      <td>250</td>\n",
              "      <td>400</td>\n",
              "      <td>2826</td>\n",
              "      <td>3375</td>\n",
              "      <td>0</td>\n",
              "      <td>6800</td>\n",
              "      <td>1578</td>\n",
              "      <td>410</td>\n",
              "      <td>2664</td>\n",
              "      <td>1300</td>\n",
              "      <td>2278</td>\n",
              "      <td>6660</td>\n",
              "      <td>3552</td>\n",
              "      <td>920</td>\n",
              "      <td>591</td>\n",
              "      <td>350</td>\n",
              "      <td>210</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>749</td>\n",
              "      <td>738</td>\n",
              "      <td>1840</td>\n",
              "      <td>8119</td>\n",
              "      <td>550</td>\n",
              "      <td>648</td>\n",
              "      <td>1043</td>\n",
              "      <td>85</td>\n",
              "      <td>2440</td>\n",
              "      <td>1167</td>\n",
              "      <td>3368</td>\n",
              "      <td>100</td>\n",
              "      <td>510</td>\n",
              "      <td>38</td>\n",
              "      <td>1055</td>\n",
              "      <td>342</td>\n",
              "      <td>230</td>\n",
              "      <td>3187</td>\n",
              "      <td>770</td>\n",
              "      <td>1339</td>\n",
              "      <td>45</td>\n",
              "      <td>1224</td>\n",
              "      <td>227</td>\n",
              "      <td>365</td>\n",
              "      <td>150</td>\n",
              "      <td>30</td>\n",
              "      <td>1773</td>\n",
              "      <td>1159</td>\n",
              "      <td>1156</td>\n",
              "      <td>501</td>\n",
              "      <td>440</td>\n",
              "      <td>176</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.147819</td>\n",
              "      <td>9.368338</td>\n",
              "      <td>0.762976</td>\n",
              "      <td>43.277736</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 113 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  Open  Close  High  ...  avg_gain  avg_loss        rs     rsi_14\n",
              "Date                                  ...                                         \n",
              "2020-03-05   NICA   575    572   589  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-10   NICA   526    549   564  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-11   NICA   567    549   569  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-12   NICA   545    564   569  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-15   NICA   521    545   547  ...       NaN       NaN       NaN        NaN\n",
              "...           ...   ...    ...   ...  ...       ...       ...       ...        ...\n",
              "2021-03-01   NICA   892    887   927  ...  7.956010  9.032390  0.880831  46.832016\n",
              "2021-03-02   NICA   880    890   909  ...  7.602009  8.387219  0.906380  47.544567\n",
              "2021-03-03   NICA   855    862   876  ...  7.059009  9.788132  0.721180  41.900337\n",
              "2021-03-04   NICA   893    878   913  ...  7.697651  9.088980  0.846921  45.855843\n",
              "2021-03-17   NICA   863    865   874  ...  7.147819  9.368338  0.762976  43.277736\n",
              "\n",
              "[180 rows x 113 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOwyhcZD1QKB"
      },
      "source": [
        "# Calculating _moving_average\n",
        "final_df['MA_7'] = final_df['Close'].rolling(window=7).mean()\n",
        "final_df['MA_14'] = final_df['Close'].rolling(window=14).mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "xale4DuJ35zs",
        "outputId": "d8e7a2a0-642e-4540-90b8-18129cd8479c"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>BB_16</th>\n",
              "      <th>SB_16</th>\n",
              "      <th>BB_3</th>\n",
              "      <th>SB_3</th>\n",
              "      <th>BB_43</th>\n",
              "      <th>SB_43</th>\n",
              "      <th>BB_34</th>\n",
              "      <th>SB_34</th>\n",
              "      <th>BB_39</th>\n",
              "      <th>SB_39</th>\n",
              "      <th>BB_42</th>\n",
              "      <th>SB_42</th>\n",
              "      <th>BB_58</th>\n",
              "      <th>SB_58</th>\n",
              "      <th>BB_55</th>\n",
              "      <th>SB_55</th>\n",
              "      <th>BB_56</th>\n",
              "      <th>SB_56</th>\n",
              "      <th>BB_25</th>\n",
              "      <th>SB_25</th>\n",
              "      <th>BB_44</th>\n",
              "      <th>SB_44</th>\n",
              "      <th>BB_50</th>\n",
              "      <th>SB_50</th>\n",
              "      <th>BB_22</th>\n",
              "      <th>SB_22</th>\n",
              "      <th>BB_10</th>\n",
              "      <th>SB_10</th>\n",
              "      <th>BB_49</th>\n",
              "      <th>SB_49</th>\n",
              "      <th>BB_41</th>\n",
              "      <th>SB_41</th>\n",
              "      <th>BB_52</th>\n",
              "      <th>SB_52</th>\n",
              "      <th>...</th>\n",
              "      <th>SB_13</th>\n",
              "      <th>BB_33</th>\n",
              "      <th>SB_33</th>\n",
              "      <th>BB_36</th>\n",
              "      <th>SB_36</th>\n",
              "      <th>BB_37</th>\n",
              "      <th>SB_37</th>\n",
              "      <th>BB_17</th>\n",
              "      <th>SB_17</th>\n",
              "      <th>BB_4</th>\n",
              "      <th>SB_4</th>\n",
              "      <th>BB_53</th>\n",
              "      <th>SB_53</th>\n",
              "      <th>BB_21</th>\n",
              "      <th>SB_21</th>\n",
              "      <th>BB_28</th>\n",
              "      <th>SB_28</th>\n",
              "      <th>BB_5</th>\n",
              "      <th>SB_5</th>\n",
              "      <th>BB_8</th>\n",
              "      <th>SB_8</th>\n",
              "      <th>BB_1</th>\n",
              "      <th>SB_1</th>\n",
              "      <th>BB_11</th>\n",
              "      <th>SB_11</th>\n",
              "      <th>BB_57</th>\n",
              "      <th>SB_57</th>\n",
              "      <th>BB_20</th>\n",
              "      <th>SB_20</th>\n",
              "      <th>BB_7</th>\n",
              "      <th>SB_7</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "      <th>MA_7</th>\n",
              "      <th>MA_14</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>575</td>\n",
              "      <td>572</td>\n",
              "      <td>589</td>\n",
              "      <td>560</td>\n",
              "      <td>17</td>\n",
              "      <td>4650</td>\n",
              "      <td>4461</td>\n",
              "      <td>0</td>\n",
              "      <td>1555</td>\n",
              "      <td>1050</td>\n",
              "      <td>2591</td>\n",
              "      <td>45185</td>\n",
              "      <td>2508</td>\n",
              "      <td>2250</td>\n",
              "      <td>572</td>\n",
              "      <td>140</td>\n",
              "      <td>2034</td>\n",
              "      <td>8747</td>\n",
              "      <td>3116</td>\n",
              "      <td>2700</td>\n",
              "      <td>2342</td>\n",
              "      <td>6308</td>\n",
              "      <td>1058</td>\n",
              "      <td>1551</td>\n",
              "      <td>2732</td>\n",
              "      <td>2510</td>\n",
              "      <td>6505</td>\n",
              "      <td>10170</td>\n",
              "      <td>6522</td>\n",
              "      <td>2550</td>\n",
              "      <td>1774</td>\n",
              "      <td>400</td>\n",
              "      <td>4000</td>\n",
              "      <td>8129</td>\n",
              "      <td>3248</td>\n",
              "      <td>1494</td>\n",
              "      <td>1824</td>\n",
              "      <td>400</td>\n",
              "      <td>940</td>\n",
              "      <td>...</td>\n",
              "      <td>2406</td>\n",
              "      <td>8213</td>\n",
              "      <td>11449</td>\n",
              "      <td>1020</td>\n",
              "      <td>12268</td>\n",
              "      <td>2100</td>\n",
              "      <td>5796</td>\n",
              "      <td>100</td>\n",
              "      <td>6170</td>\n",
              "      <td>4800</td>\n",
              "      <td>4600</td>\n",
              "      <td>3510</td>\n",
              "      <td>647</td>\n",
              "      <td>400</td>\n",
              "      <td>3812</td>\n",
              "      <td>15550</td>\n",
              "      <td>9709</td>\n",
              "      <td>500</td>\n",
              "      <td>1254</td>\n",
              "      <td>200</td>\n",
              "      <td>600</td>\n",
              "      <td>2050</td>\n",
              "      <td>2278</td>\n",
              "      <td>270</td>\n",
              "      <td>507</td>\n",
              "      <td>2152</td>\n",
              "      <td>3336</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>580</td>\n",
              "      <td>196</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-10</th>\n",
              "      <td>NICA</td>\n",
              "      <td>526</td>\n",
              "      <td>549</td>\n",
              "      <td>564</td>\n",
              "      <td>526</td>\n",
              "      <td>19</td>\n",
              "      <td>1354</td>\n",
              "      <td>0</td>\n",
              "      <td>550</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>870</td>\n",
              "      <td>10732</td>\n",
              "      <td>3870</td>\n",
              "      <td>710</td>\n",
              "      <td>566</td>\n",
              "      <td>9295</td>\n",
              "      <td>2299</td>\n",
              "      <td>10678</td>\n",
              "      <td>20</td>\n",
              "      <td>200</td>\n",
              "      <td>1000</td>\n",
              "      <td>1150</td>\n",
              "      <td>13199</td>\n",
              "      <td>200</td>\n",
              "      <td>3605</td>\n",
              "      <td>15123</td>\n",
              "      <td>2689</td>\n",
              "      <td>8197</td>\n",
              "      <td>2622</td>\n",
              "      <td>1350</td>\n",
              "      <td>810</td>\n",
              "      <td>0</td>\n",
              "      <td>4150</td>\n",
              "      <td>4867</td>\n",
              "      <td>2350</td>\n",
              "      <td>1025</td>\n",
              "      <td>11729</td>\n",
              "      <td>210</td>\n",
              "      <td>16925</td>\n",
              "      <td>...</td>\n",
              "      <td>1772</td>\n",
              "      <td>6940</td>\n",
              "      <td>4240</td>\n",
              "      <td>2500</td>\n",
              "      <td>6970</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>2564</td>\n",
              "      <td>11000</td>\n",
              "      <td>1160</td>\n",
              "      <td>11086</td>\n",
              "      <td>5144</td>\n",
              "      <td>3500</td>\n",
              "      <td>1421</td>\n",
              "      <td>350</td>\n",
              "      <td>2026</td>\n",
              "      <td>901</td>\n",
              "      <td>121</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "      <td>200</td>\n",
              "      <td>3070</td>\n",
              "      <td>1712</td>\n",
              "      <td>2015</td>\n",
              "      <td>420</td>\n",
              "      <td>1640</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>1235</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-11</th>\n",
              "      <td>NICA</td>\n",
              "      <td>567</td>\n",
              "      <td>549</td>\n",
              "      <td>569</td>\n",
              "      <td>520</td>\n",
              "      <td>29</td>\n",
              "      <td>2900</td>\n",
              "      <td>5450</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>550</td>\n",
              "      <td>2583</td>\n",
              "      <td>4490</td>\n",
              "      <td>1660</td>\n",
              "      <td>7811</td>\n",
              "      <td>3306</td>\n",
              "      <td>6470</td>\n",
              "      <td>4039</td>\n",
              "      <td>23698</td>\n",
              "      <td>17959</td>\n",
              "      <td>2010</td>\n",
              "      <td>3667</td>\n",
              "      <td>2710</td>\n",
              "      <td>3422</td>\n",
              "      <td>6550</td>\n",
              "      <td>15787</td>\n",
              "      <td>4259</td>\n",
              "      <td>11540</td>\n",
              "      <td>6108</td>\n",
              "      <td>4142</td>\n",
              "      <td>2040</td>\n",
              "      <td>3750</td>\n",
              "      <td>850</td>\n",
              "      <td>3740</td>\n",
              "      <td>6942</td>\n",
              "      <td>2565</td>\n",
              "      <td>3510</td>\n",
              "      <td>3903</td>\n",
              "      <td>3498</td>\n",
              "      <td>7615</td>\n",
              "      <td>...</td>\n",
              "      <td>712</td>\n",
              "      <td>6580</td>\n",
              "      <td>5554</td>\n",
              "      <td>9050</td>\n",
              "      <td>2612</td>\n",
              "      <td>1400</td>\n",
              "      <td>1500</td>\n",
              "      <td>10485</td>\n",
              "      <td>50365</td>\n",
              "      <td>46250</td>\n",
              "      <td>5031</td>\n",
              "      <td>6050</td>\n",
              "      <td>3460</td>\n",
              "      <td>3688</td>\n",
              "      <td>1303</td>\n",
              "      <td>6612</td>\n",
              "      <td>34884</td>\n",
              "      <td>230</td>\n",
              "      <td>150</td>\n",
              "      <td>400</td>\n",
              "      <td>2200</td>\n",
              "      <td>2370</td>\n",
              "      <td>1410</td>\n",
              "      <td>450</td>\n",
              "      <td>648</td>\n",
              "      <td>4850</td>\n",
              "      <td>3240</td>\n",
              "      <td>2560</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-12</th>\n",
              "      <td>NICA</td>\n",
              "      <td>545</td>\n",
              "      <td>564</td>\n",
              "      <td>569</td>\n",
              "      <td>545</td>\n",
              "      <td>18</td>\n",
              "      <td>2380</td>\n",
              "      <td>1411</td>\n",
              "      <td>0</td>\n",
              "      <td>400</td>\n",
              "      <td>4600</td>\n",
              "      <td>588</td>\n",
              "      <td>4975</td>\n",
              "      <td>1687</td>\n",
              "      <td>3815</td>\n",
              "      <td>2298</td>\n",
              "      <td>4253</td>\n",
              "      <td>3564</td>\n",
              "      <td>12678</td>\n",
              "      <td>22444</td>\n",
              "      <td>3433</td>\n",
              "      <td>821</td>\n",
              "      <td>3637</td>\n",
              "      <td>3621</td>\n",
              "      <td>2710</td>\n",
              "      <td>9936</td>\n",
              "      <td>10666</td>\n",
              "      <td>10252</td>\n",
              "      <td>5550</td>\n",
              "      <td>15414</td>\n",
              "      <td>1330</td>\n",
              "      <td>1000</td>\n",
              "      <td>4748</td>\n",
              "      <td>1331</td>\n",
              "      <td>9487</td>\n",
              "      <td>8477</td>\n",
              "      <td>7146</td>\n",
              "      <td>743</td>\n",
              "      <td>3920</td>\n",
              "      <td>8799</td>\n",
              "      <td>...</td>\n",
              "      <td>2119</td>\n",
              "      <td>2510</td>\n",
              "      <td>5455</td>\n",
              "      <td>8101</td>\n",
              "      <td>1700</td>\n",
              "      <td>1600</td>\n",
              "      <td>1346</td>\n",
              "      <td>3410</td>\n",
              "      <td>2087</td>\n",
              "      <td>19550</td>\n",
              "      <td>8120</td>\n",
              "      <td>1915</td>\n",
              "      <td>10478</td>\n",
              "      <td>1270</td>\n",
              "      <td>500</td>\n",
              "      <td>1400</td>\n",
              "      <td>17466</td>\n",
              "      <td>1402</td>\n",
              "      <td>4162</td>\n",
              "      <td>950</td>\n",
              "      <td>0</td>\n",
              "      <td>880</td>\n",
              "      <td>547</td>\n",
              "      <td>700</td>\n",
              "      <td>250</td>\n",
              "      <td>4400</td>\n",
              "      <td>5112</td>\n",
              "      <td>1300</td>\n",
              "      <td>764</td>\n",
              "      <td>700</td>\n",
              "      <td>1075</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-15</th>\n",
              "      <td>NICA</td>\n",
              "      <td>521</td>\n",
              "      <td>545</td>\n",
              "      <td>547</td>\n",
              "      <td>518</td>\n",
              "      <td>16</td>\n",
              "      <td>5060</td>\n",
              "      <td>1500</td>\n",
              "      <td>200</td>\n",
              "      <td>250</td>\n",
              "      <td>125</td>\n",
              "      <td>1150</td>\n",
              "      <td>5875</td>\n",
              "      <td>1310</td>\n",
              "      <td>4317</td>\n",
              "      <td>6251</td>\n",
              "      <td>1955</td>\n",
              "      <td>880</td>\n",
              "      <td>2010</td>\n",
              "      <td>7031</td>\n",
              "      <td>1886</td>\n",
              "      <td>901</td>\n",
              "      <td>4368</td>\n",
              "      <td>4666</td>\n",
              "      <td>11018</td>\n",
              "      <td>1110</td>\n",
              "      <td>18412</td>\n",
              "      <td>14993</td>\n",
              "      <td>2100</td>\n",
              "      <td>3765</td>\n",
              "      <td>665</td>\n",
              "      <td>6186</td>\n",
              "      <td>2200</td>\n",
              "      <td>1236</td>\n",
              "      <td>6310</td>\n",
              "      <td>12230</td>\n",
              "      <td>4657</td>\n",
              "      <td>8784</td>\n",
              "      <td>5000</td>\n",
              "      <td>213</td>\n",
              "      <td>...</td>\n",
              "      <td>120</td>\n",
              "      <td>4700</td>\n",
              "      <td>1579</td>\n",
              "      <td>7880</td>\n",
              "      <td>4808</td>\n",
              "      <td>1500</td>\n",
              "      <td>806</td>\n",
              "      <td>3278</td>\n",
              "      <td>14487</td>\n",
              "      <td>16540</td>\n",
              "      <td>35223</td>\n",
              "      <td>5200</td>\n",
              "      <td>2000</td>\n",
              "      <td>1364</td>\n",
              "      <td>386</td>\n",
              "      <td>2700</td>\n",
              "      <td>3300</td>\n",
              "      <td>1850</td>\n",
              "      <td>7417</td>\n",
              "      <td>700</td>\n",
              "      <td>6181</td>\n",
              "      <td>366</td>\n",
              "      <td>2529</td>\n",
              "      <td>450</td>\n",
              "      <td>1050</td>\n",
              "      <td>1870</td>\n",
              "      <td>2450</td>\n",
              "      <td>800</td>\n",
              "      <td>34</td>\n",
              "      <td>470</td>\n",
              "      <td>1962</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>892</td>\n",
              "      <td>887</td>\n",
              "      <td>927</td>\n",
              "      <td>861</td>\n",
              "      <td>14</td>\n",
              "      <td>3630</td>\n",
              "      <td>600</td>\n",
              "      <td>2380</td>\n",
              "      <td>3050</td>\n",
              "      <td>1270</td>\n",
              "      <td>5632</td>\n",
              "      <td>2719</td>\n",
              "      <td>12166</td>\n",
              "      <td>3053</td>\n",
              "      <td>2367</td>\n",
              "      <td>3450</td>\n",
              "      <td>1597</td>\n",
              "      <td>3756</td>\n",
              "      <td>13677</td>\n",
              "      <td>1110</td>\n",
              "      <td>652</td>\n",
              "      <td>10696</td>\n",
              "      <td>4604</td>\n",
              "      <td>600</td>\n",
              "      <td>9337</td>\n",
              "      <td>11136</td>\n",
              "      <td>20876</td>\n",
              "      <td>13575</td>\n",
              "      <td>2186</td>\n",
              "      <td>750</td>\n",
              "      <td>639</td>\n",
              "      <td>2355</td>\n",
              "      <td>0</td>\n",
              "      <td>28732</td>\n",
              "      <td>1422</td>\n",
              "      <td>2210</td>\n",
              "      <td>1644</td>\n",
              "      <td>1683</td>\n",
              "      <td>2372</td>\n",
              "      <td>...</td>\n",
              "      <td>800</td>\n",
              "      <td>21863</td>\n",
              "      <td>7394</td>\n",
              "      <td>2510</td>\n",
              "      <td>500</td>\n",
              "      <td>886</td>\n",
              "      <td>3570</td>\n",
              "      <td>11932</td>\n",
              "      <td>2888</td>\n",
              "      <td>3080</td>\n",
              "      <td>11256</td>\n",
              "      <td>1110</td>\n",
              "      <td>4722</td>\n",
              "      <td>2279</td>\n",
              "      <td>1847</td>\n",
              "      <td>1970</td>\n",
              "      <td>18725</td>\n",
              "      <td>1000</td>\n",
              "      <td>78</td>\n",
              "      <td>2070</td>\n",
              "      <td>0</td>\n",
              "      <td>955</td>\n",
              "      <td>685</td>\n",
              "      <td>280</td>\n",
              "      <td>911</td>\n",
              "      <td>2940</td>\n",
              "      <td>5101</td>\n",
              "      <td>3270</td>\n",
              "      <td>5750</td>\n",
              "      <td>30</td>\n",
              "      <td>785</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.956010</td>\n",
              "      <td>9.032390</td>\n",
              "      <td>0.880831</td>\n",
              "      <td>46.832016</td>\n",
              "      <td>924.857143</td>\n",
              "      <td>916.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>880</td>\n",
              "      <td>890</td>\n",
              "      <td>909</td>\n",
              "      <td>862</td>\n",
              "      <td>14</td>\n",
              "      <td>3383</td>\n",
              "      <td>1749</td>\n",
              "      <td>3970</td>\n",
              "      <td>1431</td>\n",
              "      <td>8972</td>\n",
              "      <td>0</td>\n",
              "      <td>3369</td>\n",
              "      <td>21202</td>\n",
              "      <td>2528</td>\n",
              "      <td>2351</td>\n",
              "      <td>8089</td>\n",
              "      <td>1824</td>\n",
              "      <td>4385</td>\n",
              "      <td>46906</td>\n",
              "      <td>2169</td>\n",
              "      <td>1834</td>\n",
              "      <td>1299</td>\n",
              "      <td>3803</td>\n",
              "      <td>3257</td>\n",
              "      <td>1616</td>\n",
              "      <td>4400</td>\n",
              "      <td>8473</td>\n",
              "      <td>2092</td>\n",
              "      <td>7157</td>\n",
              "      <td>2435</td>\n",
              "      <td>600</td>\n",
              "      <td>1958</td>\n",
              "      <td>219</td>\n",
              "      <td>19812</td>\n",
              "      <td>6547</td>\n",
              "      <td>5234</td>\n",
              "      <td>1569</td>\n",
              "      <td>4714</td>\n",
              "      <td>3581</td>\n",
              "      <td>...</td>\n",
              "      <td>5017</td>\n",
              "      <td>13965</td>\n",
              "      <td>14942</td>\n",
              "      <td>1060</td>\n",
              "      <td>13005</td>\n",
              "      <td>200</td>\n",
              "      <td>90</td>\n",
              "      <td>4926</td>\n",
              "      <td>3208</td>\n",
              "      <td>4616</td>\n",
              "      <td>3347</td>\n",
              "      <td>5901</td>\n",
              "      <td>2276</td>\n",
              "      <td>1882</td>\n",
              "      <td>1570</td>\n",
              "      <td>3561</td>\n",
              "      <td>6031</td>\n",
              "      <td>1150</td>\n",
              "      <td>182</td>\n",
              "      <td>950</td>\n",
              "      <td>0</td>\n",
              "      <td>5891</td>\n",
              "      <td>2065</td>\n",
              "      <td>930</td>\n",
              "      <td>883</td>\n",
              "      <td>7600</td>\n",
              "      <td>6819</td>\n",
              "      <td>7764</td>\n",
              "      <td>216</td>\n",
              "      <td>1350</td>\n",
              "      <td>930</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602009</td>\n",
              "      <td>8.387219</td>\n",
              "      <td>0.906380</td>\n",
              "      <td>47.544567</td>\n",
              "      <td>913.142857</td>\n",
              "      <td>916.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>NICA</td>\n",
              "      <td>855</td>\n",
              "      <td>862</td>\n",
              "      <td>876</td>\n",
              "      <td>853</td>\n",
              "      <td>25</td>\n",
              "      <td>3365</td>\n",
              "      <td>2579</td>\n",
              "      <td>6377</td>\n",
              "      <td>310</td>\n",
              "      <td>475</td>\n",
              "      <td>6318</td>\n",
              "      <td>15183</td>\n",
              "      <td>7300</td>\n",
              "      <td>3026</td>\n",
              "      <td>4484</td>\n",
              "      <td>2000</td>\n",
              "      <td>7254</td>\n",
              "      <td>2946</td>\n",
              "      <td>12602</td>\n",
              "      <td>1688</td>\n",
              "      <td>1250</td>\n",
              "      <td>5859</td>\n",
              "      <td>1399</td>\n",
              "      <td>2883</td>\n",
              "      <td>10758</td>\n",
              "      <td>17491</td>\n",
              "      <td>8413</td>\n",
              "      <td>30492</td>\n",
              "      <td>4272</td>\n",
              "      <td>1830</td>\n",
              "      <td>3125</td>\n",
              "      <td>11643</td>\n",
              "      <td>402</td>\n",
              "      <td>17537</td>\n",
              "      <td>6836</td>\n",
              "      <td>2720</td>\n",
              "      <td>11679</td>\n",
              "      <td>11614</td>\n",
              "      <td>2447</td>\n",
              "      <td>...</td>\n",
              "      <td>875</td>\n",
              "      <td>16190</td>\n",
              "      <td>5751</td>\n",
              "      <td>1240</td>\n",
              "      <td>1560</td>\n",
              "      <td>238</td>\n",
              "      <td>1590</td>\n",
              "      <td>4903</td>\n",
              "      <td>7256</td>\n",
              "      <td>2089</td>\n",
              "      <td>2682</td>\n",
              "      <td>1550</td>\n",
              "      <td>2217</td>\n",
              "      <td>1720</td>\n",
              "      <td>1209</td>\n",
              "      <td>4972</td>\n",
              "      <td>5115</td>\n",
              "      <td>1060</td>\n",
              "      <td>798</td>\n",
              "      <td>188</td>\n",
              "      <td>2000</td>\n",
              "      <td>947</td>\n",
              "      <td>8068</td>\n",
              "      <td>233</td>\n",
              "      <td>1697</td>\n",
              "      <td>6135</td>\n",
              "      <td>8193</td>\n",
              "      <td>3366</td>\n",
              "      <td>6900</td>\n",
              "      <td>2340</td>\n",
              "      <td>668</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.059009</td>\n",
              "      <td>9.788132</td>\n",
              "      <td>0.721180</td>\n",
              "      <td>41.900337</td>\n",
              "      <td>901.142857</td>\n",
              "      <td>914.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>NICA</td>\n",
              "      <td>893</td>\n",
              "      <td>878</td>\n",
              "      <td>913</td>\n",
              "      <td>868</td>\n",
              "      <td>13</td>\n",
              "      <td>1229</td>\n",
              "      <td>3870</td>\n",
              "      <td>340</td>\n",
              "      <td>2220</td>\n",
              "      <td>1077</td>\n",
              "      <td>4739</td>\n",
              "      <td>611</td>\n",
              "      <td>2648</td>\n",
              "      <td>2265</td>\n",
              "      <td>889</td>\n",
              "      <td>1700</td>\n",
              "      <td>1888</td>\n",
              "      <td>2691</td>\n",
              "      <td>3198</td>\n",
              "      <td>1050</td>\n",
              "      <td>3475</td>\n",
              "      <td>5937</td>\n",
              "      <td>9769</td>\n",
              "      <td>2098</td>\n",
              "      <td>5547</td>\n",
              "      <td>10388</td>\n",
              "      <td>4386</td>\n",
              "      <td>28557</td>\n",
              "      <td>1420</td>\n",
              "      <td>1260</td>\n",
              "      <td>2327</td>\n",
              "      <td>1300</td>\n",
              "      <td>1995</td>\n",
              "      <td>3685</td>\n",
              "      <td>6724</td>\n",
              "      <td>2554</td>\n",
              "      <td>2771</td>\n",
              "      <td>650</td>\n",
              "      <td>11112</td>\n",
              "      <td>...</td>\n",
              "      <td>1130</td>\n",
              "      <td>3499</td>\n",
              "      <td>12309</td>\n",
              "      <td>2688</td>\n",
              "      <td>4125</td>\n",
              "      <td>88</td>\n",
              "      <td>473</td>\n",
              "      <td>611</td>\n",
              "      <td>2583</td>\n",
              "      <td>46735</td>\n",
              "      <td>1710</td>\n",
              "      <td>0</td>\n",
              "      <td>1295</td>\n",
              "      <td>1744</td>\n",
              "      <td>1903</td>\n",
              "      <td>2272</td>\n",
              "      <td>6358</td>\n",
              "      <td>1325</td>\n",
              "      <td>2749</td>\n",
              "      <td>1402</td>\n",
              "      <td>1885</td>\n",
              "      <td>1155</td>\n",
              "      <td>1382</td>\n",
              "      <td>590</td>\n",
              "      <td>2242</td>\n",
              "      <td>3717</td>\n",
              "      <td>1276</td>\n",
              "      <td>546</td>\n",
              "      <td>2490</td>\n",
              "      <td>3626</td>\n",
              "      <td>1591</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.697651</td>\n",
              "      <td>9.088980</td>\n",
              "      <td>0.846921</td>\n",
              "      <td>45.855843</td>\n",
              "      <td>895.428571</td>\n",
              "      <td>912.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NICA</td>\n",
              "      <td>863</td>\n",
              "      <td>865</td>\n",
              "      <td>874</td>\n",
              "      <td>854</td>\n",
              "      <td>4</td>\n",
              "      <td>3019</td>\n",
              "      <td>760</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>814</td>\n",
              "      <td>723</td>\n",
              "      <td>2923</td>\n",
              "      <td>2964</td>\n",
              "      <td>130</td>\n",
              "      <td>1010</td>\n",
              "      <td>2910</td>\n",
              "      <td>702</td>\n",
              "      <td>876</td>\n",
              "      <td>2911</td>\n",
              "      <td>50</td>\n",
              "      <td>2200</td>\n",
              "      <td>1496</td>\n",
              "      <td>250</td>\n",
              "      <td>400</td>\n",
              "      <td>2826</td>\n",
              "      <td>3375</td>\n",
              "      <td>0</td>\n",
              "      <td>6800</td>\n",
              "      <td>1578</td>\n",
              "      <td>410</td>\n",
              "      <td>2664</td>\n",
              "      <td>1300</td>\n",
              "      <td>2278</td>\n",
              "      <td>6660</td>\n",
              "      <td>3552</td>\n",
              "      <td>920</td>\n",
              "      <td>591</td>\n",
              "      <td>350</td>\n",
              "      <td>210</td>\n",
              "      <td>...</td>\n",
              "      <td>738</td>\n",
              "      <td>1840</td>\n",
              "      <td>8119</td>\n",
              "      <td>550</td>\n",
              "      <td>648</td>\n",
              "      <td>1043</td>\n",
              "      <td>85</td>\n",
              "      <td>2440</td>\n",
              "      <td>1167</td>\n",
              "      <td>3368</td>\n",
              "      <td>100</td>\n",
              "      <td>510</td>\n",
              "      <td>38</td>\n",
              "      <td>1055</td>\n",
              "      <td>342</td>\n",
              "      <td>230</td>\n",
              "      <td>3187</td>\n",
              "      <td>770</td>\n",
              "      <td>1339</td>\n",
              "      <td>45</td>\n",
              "      <td>1224</td>\n",
              "      <td>227</td>\n",
              "      <td>365</td>\n",
              "      <td>150</td>\n",
              "      <td>30</td>\n",
              "      <td>1773</td>\n",
              "      <td>1159</td>\n",
              "      <td>1156</td>\n",
              "      <td>501</td>\n",
              "      <td>440</td>\n",
              "      <td>176</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.147819</td>\n",
              "      <td>9.368338</td>\n",
              "      <td>0.762976</td>\n",
              "      <td>43.277736</td>\n",
              "      <td>885.142857</td>\n",
              "      <td>911.142857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 115 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  Open  Close  ...     rsi_14        MA_7       MA_14\n",
              "Date                            ...                                   \n",
              "2020-03-05   NICA   575    572  ...        NaN         NaN         NaN\n",
              "2020-03-10   NICA   526    549  ...        NaN         NaN         NaN\n",
              "2020-03-11   NICA   567    549  ...        NaN         NaN         NaN\n",
              "2020-03-12   NICA   545    564  ...        NaN         NaN         NaN\n",
              "2020-03-15   NICA   521    545  ...        NaN         NaN         NaN\n",
              "...           ...   ...    ...  ...        ...         ...         ...\n",
              "2021-03-01   NICA   892    887  ...  46.832016  924.857143  916.285714\n",
              "2021-03-02   NICA   880    890  ...  47.544567  913.142857  916.357143\n",
              "2021-03-03   NICA   855    862  ...  41.900337  901.142857  914.500000\n",
              "2021-03-04   NICA   893    878  ...  45.855843  895.428571  912.928571\n",
              "2021-03-17   NICA   863    865  ...  43.277736  885.142857  911.142857\n",
              "\n",
              "[180 rows x 115 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePcQyPxick0P"
      },
      "source": [
        "look_back_period=3\n",
        "for i in range(look_back_period):\n",
        "  final_df[f'previous_day_price_{i+1}']=final_df[\"Close\"].shift(i)\n",
        "final_df['target_price']=final_df[\"Close\"].shift(-look_back_period)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "IENaLdvZoBVX",
        "outputId": "326408d9-8e1c-42d3-f648-3a6edfb3b660"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>BB_16</th>\n",
              "      <th>SB_16</th>\n",
              "      <th>BB_3</th>\n",
              "      <th>SB_3</th>\n",
              "      <th>BB_43</th>\n",
              "      <th>SB_43</th>\n",
              "      <th>BB_34</th>\n",
              "      <th>SB_34</th>\n",
              "      <th>BB_39</th>\n",
              "      <th>SB_39</th>\n",
              "      <th>BB_42</th>\n",
              "      <th>SB_42</th>\n",
              "      <th>BB_58</th>\n",
              "      <th>SB_58</th>\n",
              "      <th>BB_55</th>\n",
              "      <th>SB_55</th>\n",
              "      <th>BB_56</th>\n",
              "      <th>SB_56</th>\n",
              "      <th>BB_25</th>\n",
              "      <th>SB_25</th>\n",
              "      <th>BB_44</th>\n",
              "      <th>SB_44</th>\n",
              "      <th>BB_50</th>\n",
              "      <th>SB_50</th>\n",
              "      <th>BB_22</th>\n",
              "      <th>SB_22</th>\n",
              "      <th>BB_10</th>\n",
              "      <th>SB_10</th>\n",
              "      <th>BB_49</th>\n",
              "      <th>SB_49</th>\n",
              "      <th>BB_41</th>\n",
              "      <th>SB_41</th>\n",
              "      <th>BB_52</th>\n",
              "      <th>SB_52</th>\n",
              "      <th>...</th>\n",
              "      <th>SB_36</th>\n",
              "      <th>BB_37</th>\n",
              "      <th>SB_37</th>\n",
              "      <th>BB_17</th>\n",
              "      <th>SB_17</th>\n",
              "      <th>BB_4</th>\n",
              "      <th>SB_4</th>\n",
              "      <th>BB_53</th>\n",
              "      <th>SB_53</th>\n",
              "      <th>BB_21</th>\n",
              "      <th>SB_21</th>\n",
              "      <th>BB_28</th>\n",
              "      <th>SB_28</th>\n",
              "      <th>BB_5</th>\n",
              "      <th>SB_5</th>\n",
              "      <th>BB_8</th>\n",
              "      <th>SB_8</th>\n",
              "      <th>BB_1</th>\n",
              "      <th>SB_1</th>\n",
              "      <th>BB_11</th>\n",
              "      <th>SB_11</th>\n",
              "      <th>BB_57</th>\n",
              "      <th>SB_57</th>\n",
              "      <th>BB_20</th>\n",
              "      <th>SB_20</th>\n",
              "      <th>BB_7</th>\n",
              "      <th>SB_7</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "      <th>MA_7</th>\n",
              "      <th>MA_14</th>\n",
              "      <th>previous_day_price_1</th>\n",
              "      <th>previous_day_price_2</th>\n",
              "      <th>previous_day_price_3</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>575</td>\n",
              "      <td>572</td>\n",
              "      <td>589</td>\n",
              "      <td>560</td>\n",
              "      <td>17</td>\n",
              "      <td>4650</td>\n",
              "      <td>4461</td>\n",
              "      <td>0</td>\n",
              "      <td>1555</td>\n",
              "      <td>1050</td>\n",
              "      <td>2591</td>\n",
              "      <td>45185</td>\n",
              "      <td>2508</td>\n",
              "      <td>2250</td>\n",
              "      <td>572</td>\n",
              "      <td>140</td>\n",
              "      <td>2034</td>\n",
              "      <td>8747</td>\n",
              "      <td>3116</td>\n",
              "      <td>2700</td>\n",
              "      <td>2342</td>\n",
              "      <td>6308</td>\n",
              "      <td>1058</td>\n",
              "      <td>1551</td>\n",
              "      <td>2732</td>\n",
              "      <td>2510</td>\n",
              "      <td>6505</td>\n",
              "      <td>10170</td>\n",
              "      <td>6522</td>\n",
              "      <td>2550</td>\n",
              "      <td>1774</td>\n",
              "      <td>400</td>\n",
              "      <td>4000</td>\n",
              "      <td>8129</td>\n",
              "      <td>3248</td>\n",
              "      <td>1494</td>\n",
              "      <td>1824</td>\n",
              "      <td>400</td>\n",
              "      <td>940</td>\n",
              "      <td>...</td>\n",
              "      <td>12268</td>\n",
              "      <td>2100</td>\n",
              "      <td>5796</td>\n",
              "      <td>100</td>\n",
              "      <td>6170</td>\n",
              "      <td>4800</td>\n",
              "      <td>4600</td>\n",
              "      <td>3510</td>\n",
              "      <td>647</td>\n",
              "      <td>400</td>\n",
              "      <td>3812</td>\n",
              "      <td>15550</td>\n",
              "      <td>9709</td>\n",
              "      <td>500</td>\n",
              "      <td>1254</td>\n",
              "      <td>200</td>\n",
              "      <td>600</td>\n",
              "      <td>2050</td>\n",
              "      <td>2278</td>\n",
              "      <td>270</td>\n",
              "      <td>507</td>\n",
              "      <td>2152</td>\n",
              "      <td>3336</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>580</td>\n",
              "      <td>196</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>572</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>564.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-10</th>\n",
              "      <td>NICA</td>\n",
              "      <td>526</td>\n",
              "      <td>549</td>\n",
              "      <td>564</td>\n",
              "      <td>526</td>\n",
              "      <td>19</td>\n",
              "      <td>1354</td>\n",
              "      <td>0</td>\n",
              "      <td>550</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>870</td>\n",
              "      <td>10732</td>\n",
              "      <td>3870</td>\n",
              "      <td>710</td>\n",
              "      <td>566</td>\n",
              "      <td>9295</td>\n",
              "      <td>2299</td>\n",
              "      <td>10678</td>\n",
              "      <td>20</td>\n",
              "      <td>200</td>\n",
              "      <td>1000</td>\n",
              "      <td>1150</td>\n",
              "      <td>13199</td>\n",
              "      <td>200</td>\n",
              "      <td>3605</td>\n",
              "      <td>15123</td>\n",
              "      <td>2689</td>\n",
              "      <td>8197</td>\n",
              "      <td>2622</td>\n",
              "      <td>1350</td>\n",
              "      <td>810</td>\n",
              "      <td>0</td>\n",
              "      <td>4150</td>\n",
              "      <td>4867</td>\n",
              "      <td>2350</td>\n",
              "      <td>1025</td>\n",
              "      <td>11729</td>\n",
              "      <td>210</td>\n",
              "      <td>16925</td>\n",
              "      <td>...</td>\n",
              "      <td>6970</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>2564</td>\n",
              "      <td>11000</td>\n",
              "      <td>1160</td>\n",
              "      <td>11086</td>\n",
              "      <td>5144</td>\n",
              "      <td>3500</td>\n",
              "      <td>1421</td>\n",
              "      <td>350</td>\n",
              "      <td>2026</td>\n",
              "      <td>901</td>\n",
              "      <td>121</td>\n",
              "      <td>1100</td>\n",
              "      <td>0</td>\n",
              "      <td>200</td>\n",
              "      <td>3070</td>\n",
              "      <td>1712</td>\n",
              "      <td>2015</td>\n",
              "      <td>420</td>\n",
              "      <td>1640</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1500</td>\n",
              "      <td>1235</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>549</td>\n",
              "      <td>572.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>545.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-11</th>\n",
              "      <td>NICA</td>\n",
              "      <td>567</td>\n",
              "      <td>549</td>\n",
              "      <td>569</td>\n",
              "      <td>520</td>\n",
              "      <td>29</td>\n",
              "      <td>2900</td>\n",
              "      <td>5450</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>550</td>\n",
              "      <td>2583</td>\n",
              "      <td>4490</td>\n",
              "      <td>1660</td>\n",
              "      <td>7811</td>\n",
              "      <td>3306</td>\n",
              "      <td>6470</td>\n",
              "      <td>4039</td>\n",
              "      <td>23698</td>\n",
              "      <td>17959</td>\n",
              "      <td>2010</td>\n",
              "      <td>3667</td>\n",
              "      <td>2710</td>\n",
              "      <td>3422</td>\n",
              "      <td>6550</td>\n",
              "      <td>15787</td>\n",
              "      <td>4259</td>\n",
              "      <td>11540</td>\n",
              "      <td>6108</td>\n",
              "      <td>4142</td>\n",
              "      <td>2040</td>\n",
              "      <td>3750</td>\n",
              "      <td>850</td>\n",
              "      <td>3740</td>\n",
              "      <td>6942</td>\n",
              "      <td>2565</td>\n",
              "      <td>3510</td>\n",
              "      <td>3903</td>\n",
              "      <td>3498</td>\n",
              "      <td>7615</td>\n",
              "      <td>...</td>\n",
              "      <td>2612</td>\n",
              "      <td>1400</td>\n",
              "      <td>1500</td>\n",
              "      <td>10485</td>\n",
              "      <td>50365</td>\n",
              "      <td>46250</td>\n",
              "      <td>5031</td>\n",
              "      <td>6050</td>\n",
              "      <td>3460</td>\n",
              "      <td>3688</td>\n",
              "      <td>1303</td>\n",
              "      <td>6612</td>\n",
              "      <td>34884</td>\n",
              "      <td>230</td>\n",
              "      <td>150</td>\n",
              "      <td>400</td>\n",
              "      <td>2200</td>\n",
              "      <td>2370</td>\n",
              "      <td>1410</td>\n",
              "      <td>450</td>\n",
              "      <td>648</td>\n",
              "      <td>4850</td>\n",
              "      <td>3240</td>\n",
              "      <td>2560</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>549</td>\n",
              "      <td>549.0</td>\n",
              "      <td>572.0</td>\n",
              "      <td>517.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-12</th>\n",
              "      <td>NICA</td>\n",
              "      <td>545</td>\n",
              "      <td>564</td>\n",
              "      <td>569</td>\n",
              "      <td>545</td>\n",
              "      <td>18</td>\n",
              "      <td>2380</td>\n",
              "      <td>1411</td>\n",
              "      <td>0</td>\n",
              "      <td>400</td>\n",
              "      <td>4600</td>\n",
              "      <td>588</td>\n",
              "      <td>4975</td>\n",
              "      <td>1687</td>\n",
              "      <td>3815</td>\n",
              "      <td>2298</td>\n",
              "      <td>4253</td>\n",
              "      <td>3564</td>\n",
              "      <td>12678</td>\n",
              "      <td>22444</td>\n",
              "      <td>3433</td>\n",
              "      <td>821</td>\n",
              "      <td>3637</td>\n",
              "      <td>3621</td>\n",
              "      <td>2710</td>\n",
              "      <td>9936</td>\n",
              "      <td>10666</td>\n",
              "      <td>10252</td>\n",
              "      <td>5550</td>\n",
              "      <td>15414</td>\n",
              "      <td>1330</td>\n",
              "      <td>1000</td>\n",
              "      <td>4748</td>\n",
              "      <td>1331</td>\n",
              "      <td>9487</td>\n",
              "      <td>8477</td>\n",
              "      <td>7146</td>\n",
              "      <td>743</td>\n",
              "      <td>3920</td>\n",
              "      <td>8799</td>\n",
              "      <td>...</td>\n",
              "      <td>1700</td>\n",
              "      <td>1600</td>\n",
              "      <td>1346</td>\n",
              "      <td>3410</td>\n",
              "      <td>2087</td>\n",
              "      <td>19550</td>\n",
              "      <td>8120</td>\n",
              "      <td>1915</td>\n",
              "      <td>10478</td>\n",
              "      <td>1270</td>\n",
              "      <td>500</td>\n",
              "      <td>1400</td>\n",
              "      <td>17466</td>\n",
              "      <td>1402</td>\n",
              "      <td>4162</td>\n",
              "      <td>950</td>\n",
              "      <td>0</td>\n",
              "      <td>880</td>\n",
              "      <td>547</td>\n",
              "      <td>700</td>\n",
              "      <td>250</td>\n",
              "      <td>4400</td>\n",
              "      <td>5112</td>\n",
              "      <td>1300</td>\n",
              "      <td>764</td>\n",
              "      <td>700</td>\n",
              "      <td>1075</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>564</td>\n",
              "      <td>549.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-15</th>\n",
              "      <td>NICA</td>\n",
              "      <td>521</td>\n",
              "      <td>545</td>\n",
              "      <td>547</td>\n",
              "      <td>518</td>\n",
              "      <td>16</td>\n",
              "      <td>5060</td>\n",
              "      <td>1500</td>\n",
              "      <td>200</td>\n",
              "      <td>250</td>\n",
              "      <td>125</td>\n",
              "      <td>1150</td>\n",
              "      <td>5875</td>\n",
              "      <td>1310</td>\n",
              "      <td>4317</td>\n",
              "      <td>6251</td>\n",
              "      <td>1955</td>\n",
              "      <td>880</td>\n",
              "      <td>2010</td>\n",
              "      <td>7031</td>\n",
              "      <td>1886</td>\n",
              "      <td>901</td>\n",
              "      <td>4368</td>\n",
              "      <td>4666</td>\n",
              "      <td>11018</td>\n",
              "      <td>1110</td>\n",
              "      <td>18412</td>\n",
              "      <td>14993</td>\n",
              "      <td>2100</td>\n",
              "      <td>3765</td>\n",
              "      <td>665</td>\n",
              "      <td>6186</td>\n",
              "      <td>2200</td>\n",
              "      <td>1236</td>\n",
              "      <td>6310</td>\n",
              "      <td>12230</td>\n",
              "      <td>4657</td>\n",
              "      <td>8784</td>\n",
              "      <td>5000</td>\n",
              "      <td>213</td>\n",
              "      <td>...</td>\n",
              "      <td>4808</td>\n",
              "      <td>1500</td>\n",
              "      <td>806</td>\n",
              "      <td>3278</td>\n",
              "      <td>14487</td>\n",
              "      <td>16540</td>\n",
              "      <td>35223</td>\n",
              "      <td>5200</td>\n",
              "      <td>2000</td>\n",
              "      <td>1364</td>\n",
              "      <td>386</td>\n",
              "      <td>2700</td>\n",
              "      <td>3300</td>\n",
              "      <td>1850</td>\n",
              "      <td>7417</td>\n",
              "      <td>700</td>\n",
              "      <td>6181</td>\n",
              "      <td>366</td>\n",
              "      <td>2529</td>\n",
              "      <td>450</td>\n",
              "      <td>1050</td>\n",
              "      <td>1870</td>\n",
              "      <td>2450</td>\n",
              "      <td>800</td>\n",
              "      <td>34</td>\n",
              "      <td>470</td>\n",
              "      <td>1962</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>545</td>\n",
              "      <td>564.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>508.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>892</td>\n",
              "      <td>887</td>\n",
              "      <td>927</td>\n",
              "      <td>861</td>\n",
              "      <td>14</td>\n",
              "      <td>3630</td>\n",
              "      <td>600</td>\n",
              "      <td>2380</td>\n",
              "      <td>3050</td>\n",
              "      <td>1270</td>\n",
              "      <td>5632</td>\n",
              "      <td>2719</td>\n",
              "      <td>12166</td>\n",
              "      <td>3053</td>\n",
              "      <td>2367</td>\n",
              "      <td>3450</td>\n",
              "      <td>1597</td>\n",
              "      <td>3756</td>\n",
              "      <td>13677</td>\n",
              "      <td>1110</td>\n",
              "      <td>652</td>\n",
              "      <td>10696</td>\n",
              "      <td>4604</td>\n",
              "      <td>600</td>\n",
              "      <td>9337</td>\n",
              "      <td>11136</td>\n",
              "      <td>20876</td>\n",
              "      <td>13575</td>\n",
              "      <td>2186</td>\n",
              "      <td>750</td>\n",
              "      <td>639</td>\n",
              "      <td>2355</td>\n",
              "      <td>0</td>\n",
              "      <td>28732</td>\n",
              "      <td>1422</td>\n",
              "      <td>2210</td>\n",
              "      <td>1644</td>\n",
              "      <td>1683</td>\n",
              "      <td>2372</td>\n",
              "      <td>...</td>\n",
              "      <td>500</td>\n",
              "      <td>886</td>\n",
              "      <td>3570</td>\n",
              "      <td>11932</td>\n",
              "      <td>2888</td>\n",
              "      <td>3080</td>\n",
              "      <td>11256</td>\n",
              "      <td>1110</td>\n",
              "      <td>4722</td>\n",
              "      <td>2279</td>\n",
              "      <td>1847</td>\n",
              "      <td>1970</td>\n",
              "      <td>18725</td>\n",
              "      <td>1000</td>\n",
              "      <td>78</td>\n",
              "      <td>2070</td>\n",
              "      <td>0</td>\n",
              "      <td>955</td>\n",
              "      <td>685</td>\n",
              "      <td>280</td>\n",
              "      <td>911</td>\n",
              "      <td>2940</td>\n",
              "      <td>5101</td>\n",
              "      <td>3270</td>\n",
              "      <td>5750</td>\n",
              "      <td>30</td>\n",
              "      <td>785</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.956010</td>\n",
              "      <td>9.032390</td>\n",
              "      <td>0.880831</td>\n",
              "      <td>46.832016</td>\n",
              "      <td>924.857143</td>\n",
              "      <td>916.285714</td>\n",
              "      <td>887</td>\n",
              "      <td>900.0</td>\n",
              "      <td>914.0</td>\n",
              "      <td>878.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>880</td>\n",
              "      <td>890</td>\n",
              "      <td>909</td>\n",
              "      <td>862</td>\n",
              "      <td>14</td>\n",
              "      <td>3383</td>\n",
              "      <td>1749</td>\n",
              "      <td>3970</td>\n",
              "      <td>1431</td>\n",
              "      <td>8972</td>\n",
              "      <td>0</td>\n",
              "      <td>3369</td>\n",
              "      <td>21202</td>\n",
              "      <td>2528</td>\n",
              "      <td>2351</td>\n",
              "      <td>8089</td>\n",
              "      <td>1824</td>\n",
              "      <td>4385</td>\n",
              "      <td>46906</td>\n",
              "      <td>2169</td>\n",
              "      <td>1834</td>\n",
              "      <td>1299</td>\n",
              "      <td>3803</td>\n",
              "      <td>3257</td>\n",
              "      <td>1616</td>\n",
              "      <td>4400</td>\n",
              "      <td>8473</td>\n",
              "      <td>2092</td>\n",
              "      <td>7157</td>\n",
              "      <td>2435</td>\n",
              "      <td>600</td>\n",
              "      <td>1958</td>\n",
              "      <td>219</td>\n",
              "      <td>19812</td>\n",
              "      <td>6547</td>\n",
              "      <td>5234</td>\n",
              "      <td>1569</td>\n",
              "      <td>4714</td>\n",
              "      <td>3581</td>\n",
              "      <td>...</td>\n",
              "      <td>13005</td>\n",
              "      <td>200</td>\n",
              "      <td>90</td>\n",
              "      <td>4926</td>\n",
              "      <td>3208</td>\n",
              "      <td>4616</td>\n",
              "      <td>3347</td>\n",
              "      <td>5901</td>\n",
              "      <td>2276</td>\n",
              "      <td>1882</td>\n",
              "      <td>1570</td>\n",
              "      <td>3561</td>\n",
              "      <td>6031</td>\n",
              "      <td>1150</td>\n",
              "      <td>182</td>\n",
              "      <td>950</td>\n",
              "      <td>0</td>\n",
              "      <td>5891</td>\n",
              "      <td>2065</td>\n",
              "      <td>930</td>\n",
              "      <td>883</td>\n",
              "      <td>7600</td>\n",
              "      <td>6819</td>\n",
              "      <td>7764</td>\n",
              "      <td>216</td>\n",
              "      <td>1350</td>\n",
              "      <td>930</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602009</td>\n",
              "      <td>8.387219</td>\n",
              "      <td>0.906380</td>\n",
              "      <td>47.544567</td>\n",
              "      <td>913.142857</td>\n",
              "      <td>916.357143</td>\n",
              "      <td>890</td>\n",
              "      <td>887.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>865.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>NICA</td>\n",
              "      <td>855</td>\n",
              "      <td>862</td>\n",
              "      <td>876</td>\n",
              "      <td>853</td>\n",
              "      <td>25</td>\n",
              "      <td>3365</td>\n",
              "      <td>2579</td>\n",
              "      <td>6377</td>\n",
              "      <td>310</td>\n",
              "      <td>475</td>\n",
              "      <td>6318</td>\n",
              "      <td>15183</td>\n",
              "      <td>7300</td>\n",
              "      <td>3026</td>\n",
              "      <td>4484</td>\n",
              "      <td>2000</td>\n",
              "      <td>7254</td>\n",
              "      <td>2946</td>\n",
              "      <td>12602</td>\n",
              "      <td>1688</td>\n",
              "      <td>1250</td>\n",
              "      <td>5859</td>\n",
              "      <td>1399</td>\n",
              "      <td>2883</td>\n",
              "      <td>10758</td>\n",
              "      <td>17491</td>\n",
              "      <td>8413</td>\n",
              "      <td>30492</td>\n",
              "      <td>4272</td>\n",
              "      <td>1830</td>\n",
              "      <td>3125</td>\n",
              "      <td>11643</td>\n",
              "      <td>402</td>\n",
              "      <td>17537</td>\n",
              "      <td>6836</td>\n",
              "      <td>2720</td>\n",
              "      <td>11679</td>\n",
              "      <td>11614</td>\n",
              "      <td>2447</td>\n",
              "      <td>...</td>\n",
              "      <td>1560</td>\n",
              "      <td>238</td>\n",
              "      <td>1590</td>\n",
              "      <td>4903</td>\n",
              "      <td>7256</td>\n",
              "      <td>2089</td>\n",
              "      <td>2682</td>\n",
              "      <td>1550</td>\n",
              "      <td>2217</td>\n",
              "      <td>1720</td>\n",
              "      <td>1209</td>\n",
              "      <td>4972</td>\n",
              "      <td>5115</td>\n",
              "      <td>1060</td>\n",
              "      <td>798</td>\n",
              "      <td>188</td>\n",
              "      <td>2000</td>\n",
              "      <td>947</td>\n",
              "      <td>8068</td>\n",
              "      <td>233</td>\n",
              "      <td>1697</td>\n",
              "      <td>6135</td>\n",
              "      <td>8193</td>\n",
              "      <td>3366</td>\n",
              "      <td>6900</td>\n",
              "      <td>2340</td>\n",
              "      <td>668</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.059009</td>\n",
              "      <td>9.788132</td>\n",
              "      <td>0.721180</td>\n",
              "      <td>41.900337</td>\n",
              "      <td>901.142857</td>\n",
              "      <td>914.500000</td>\n",
              "      <td>862</td>\n",
              "      <td>890.0</td>\n",
              "      <td>887.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>NICA</td>\n",
              "      <td>893</td>\n",
              "      <td>878</td>\n",
              "      <td>913</td>\n",
              "      <td>868</td>\n",
              "      <td>13</td>\n",
              "      <td>1229</td>\n",
              "      <td>3870</td>\n",
              "      <td>340</td>\n",
              "      <td>2220</td>\n",
              "      <td>1077</td>\n",
              "      <td>4739</td>\n",
              "      <td>611</td>\n",
              "      <td>2648</td>\n",
              "      <td>2265</td>\n",
              "      <td>889</td>\n",
              "      <td>1700</td>\n",
              "      <td>1888</td>\n",
              "      <td>2691</td>\n",
              "      <td>3198</td>\n",
              "      <td>1050</td>\n",
              "      <td>3475</td>\n",
              "      <td>5937</td>\n",
              "      <td>9769</td>\n",
              "      <td>2098</td>\n",
              "      <td>5547</td>\n",
              "      <td>10388</td>\n",
              "      <td>4386</td>\n",
              "      <td>28557</td>\n",
              "      <td>1420</td>\n",
              "      <td>1260</td>\n",
              "      <td>2327</td>\n",
              "      <td>1300</td>\n",
              "      <td>1995</td>\n",
              "      <td>3685</td>\n",
              "      <td>6724</td>\n",
              "      <td>2554</td>\n",
              "      <td>2771</td>\n",
              "      <td>650</td>\n",
              "      <td>11112</td>\n",
              "      <td>...</td>\n",
              "      <td>4125</td>\n",
              "      <td>88</td>\n",
              "      <td>473</td>\n",
              "      <td>611</td>\n",
              "      <td>2583</td>\n",
              "      <td>46735</td>\n",
              "      <td>1710</td>\n",
              "      <td>0</td>\n",
              "      <td>1295</td>\n",
              "      <td>1744</td>\n",
              "      <td>1903</td>\n",
              "      <td>2272</td>\n",
              "      <td>6358</td>\n",
              "      <td>1325</td>\n",
              "      <td>2749</td>\n",
              "      <td>1402</td>\n",
              "      <td>1885</td>\n",
              "      <td>1155</td>\n",
              "      <td>1382</td>\n",
              "      <td>590</td>\n",
              "      <td>2242</td>\n",
              "      <td>3717</td>\n",
              "      <td>1276</td>\n",
              "      <td>546</td>\n",
              "      <td>2490</td>\n",
              "      <td>3626</td>\n",
              "      <td>1591</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.697651</td>\n",
              "      <td>9.088980</td>\n",
              "      <td>0.846921</td>\n",
              "      <td>45.855843</td>\n",
              "      <td>895.428571</td>\n",
              "      <td>912.928571</td>\n",
              "      <td>878</td>\n",
              "      <td>862.0</td>\n",
              "      <td>890.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NICA</td>\n",
              "      <td>863</td>\n",
              "      <td>865</td>\n",
              "      <td>874</td>\n",
              "      <td>854</td>\n",
              "      <td>4</td>\n",
              "      <td>3019</td>\n",
              "      <td>760</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>814</td>\n",
              "      <td>723</td>\n",
              "      <td>2923</td>\n",
              "      <td>2964</td>\n",
              "      <td>130</td>\n",
              "      <td>1010</td>\n",
              "      <td>2910</td>\n",
              "      <td>702</td>\n",
              "      <td>876</td>\n",
              "      <td>2911</td>\n",
              "      <td>50</td>\n",
              "      <td>2200</td>\n",
              "      <td>1496</td>\n",
              "      <td>250</td>\n",
              "      <td>400</td>\n",
              "      <td>2826</td>\n",
              "      <td>3375</td>\n",
              "      <td>0</td>\n",
              "      <td>6800</td>\n",
              "      <td>1578</td>\n",
              "      <td>410</td>\n",
              "      <td>2664</td>\n",
              "      <td>1300</td>\n",
              "      <td>2278</td>\n",
              "      <td>6660</td>\n",
              "      <td>3552</td>\n",
              "      <td>920</td>\n",
              "      <td>591</td>\n",
              "      <td>350</td>\n",
              "      <td>210</td>\n",
              "      <td>...</td>\n",
              "      <td>648</td>\n",
              "      <td>1043</td>\n",
              "      <td>85</td>\n",
              "      <td>2440</td>\n",
              "      <td>1167</td>\n",
              "      <td>3368</td>\n",
              "      <td>100</td>\n",
              "      <td>510</td>\n",
              "      <td>38</td>\n",
              "      <td>1055</td>\n",
              "      <td>342</td>\n",
              "      <td>230</td>\n",
              "      <td>3187</td>\n",
              "      <td>770</td>\n",
              "      <td>1339</td>\n",
              "      <td>45</td>\n",
              "      <td>1224</td>\n",
              "      <td>227</td>\n",
              "      <td>365</td>\n",
              "      <td>150</td>\n",
              "      <td>30</td>\n",
              "      <td>1773</td>\n",
              "      <td>1159</td>\n",
              "      <td>1156</td>\n",
              "      <td>501</td>\n",
              "      <td>440</td>\n",
              "      <td>176</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.147819</td>\n",
              "      <td>9.368338</td>\n",
              "      <td>0.762976</td>\n",
              "      <td>43.277736</td>\n",
              "      <td>885.142857</td>\n",
              "      <td>911.142857</td>\n",
              "      <td>865</td>\n",
              "      <td>878.0</td>\n",
              "      <td>862.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  Open  ...  previous_day_price_3  target_price\n",
              "Date                     ...                                    \n",
              "2020-03-05   NICA   575  ...                   NaN         564.0\n",
              "2020-03-10   NICA   526  ...                   NaN         545.0\n",
              "2020-03-11   NICA   567  ...                 572.0         517.0\n",
              "2020-03-12   NICA   545  ...                 549.0         515.0\n",
              "2020-03-15   NICA   521  ...                 549.0         508.0\n",
              "...           ...   ...  ...                   ...           ...\n",
              "2021-03-01   NICA   892  ...                 914.0         878.0\n",
              "2021-03-02   NICA   880  ...                 900.0         865.0\n",
              "2021-03-03   NICA   855  ...                 887.0           NaN\n",
              "2021-03-04   NICA   893  ...                 890.0           NaN\n",
              "2021-03-17   NICA   863  ...                 862.0           NaN\n",
              "\n",
              "[180 rows x 119 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bKnkZ5ppF73"
      },
      "source": [
        "final_df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyCWqlqR58gu"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(final_df.iloc[:,1:])\n",
        "final_df.iloc[:,1:] =scaler.transform(final_df.iloc[:,1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "gBSWyiDKqzh7",
        "outputId": "ad400b43-275e-49e0-8016-d65948bb28bb"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>BB_16</th>\n",
              "      <th>SB_16</th>\n",
              "      <th>BB_3</th>\n",
              "      <th>SB_3</th>\n",
              "      <th>BB_43</th>\n",
              "      <th>SB_43</th>\n",
              "      <th>BB_34</th>\n",
              "      <th>SB_34</th>\n",
              "      <th>BB_39</th>\n",
              "      <th>SB_39</th>\n",
              "      <th>BB_42</th>\n",
              "      <th>SB_42</th>\n",
              "      <th>BB_58</th>\n",
              "      <th>SB_58</th>\n",
              "      <th>BB_55</th>\n",
              "      <th>SB_55</th>\n",
              "      <th>BB_56</th>\n",
              "      <th>SB_56</th>\n",
              "      <th>BB_25</th>\n",
              "      <th>SB_25</th>\n",
              "      <th>BB_44</th>\n",
              "      <th>SB_44</th>\n",
              "      <th>BB_50</th>\n",
              "      <th>SB_50</th>\n",
              "      <th>BB_22</th>\n",
              "      <th>SB_22</th>\n",
              "      <th>BB_10</th>\n",
              "      <th>SB_10</th>\n",
              "      <th>BB_49</th>\n",
              "      <th>SB_49</th>\n",
              "      <th>BB_41</th>\n",
              "      <th>SB_41</th>\n",
              "      <th>BB_52</th>\n",
              "      <th>SB_52</th>\n",
              "      <th>...</th>\n",
              "      <th>SB_36</th>\n",
              "      <th>BB_37</th>\n",
              "      <th>SB_37</th>\n",
              "      <th>BB_17</th>\n",
              "      <th>SB_17</th>\n",
              "      <th>BB_4</th>\n",
              "      <th>SB_4</th>\n",
              "      <th>BB_53</th>\n",
              "      <th>SB_53</th>\n",
              "      <th>BB_21</th>\n",
              "      <th>SB_21</th>\n",
              "      <th>BB_28</th>\n",
              "      <th>SB_28</th>\n",
              "      <th>BB_5</th>\n",
              "      <th>SB_5</th>\n",
              "      <th>BB_8</th>\n",
              "      <th>SB_8</th>\n",
              "      <th>BB_1</th>\n",
              "      <th>SB_1</th>\n",
              "      <th>BB_11</th>\n",
              "      <th>SB_11</th>\n",
              "      <th>BB_57</th>\n",
              "      <th>SB_57</th>\n",
              "      <th>BB_20</th>\n",
              "      <th>SB_20</th>\n",
              "      <th>BB_7</th>\n",
              "      <th>SB_7</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "      <th>MA_7</th>\n",
              "      <th>MA_14</th>\n",
              "      <th>previous_day_price_1</th>\n",
              "      <th>previous_day_price_2</th>\n",
              "      <th>previous_day_price_3</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-07-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.052783</td>\n",
              "      <td>-1.085382</td>\n",
              "      <td>-1.108542</td>\n",
              "      <td>-1.140584</td>\n",
              "      <td>-1.107547</td>\n",
              "      <td>-0.772388</td>\n",
              "      <td>-0.828397</td>\n",
              "      <td>-0.206789</td>\n",
              "      <td>-0.475103</td>\n",
              "      <td>-0.620874</td>\n",
              "      <td>-0.645458</td>\n",
              "      <td>-0.729339</td>\n",
              "      <td>-0.720127</td>\n",
              "      <td>0.481024</td>\n",
              "      <td>-0.920176</td>\n",
              "      <td>-0.329842</td>\n",
              "      <td>-0.698201</td>\n",
              "      <td>-0.661405</td>\n",
              "      <td>-0.734627</td>\n",
              "      <td>-0.471587</td>\n",
              "      <td>-0.829103</td>\n",
              "      <td>-0.449318</td>\n",
              "      <td>-0.590094</td>\n",
              "      <td>-0.637881</td>\n",
              "      <td>-0.584058</td>\n",
              "      <td>-0.135282</td>\n",
              "      <td>-0.441189</td>\n",
              "      <td>-0.504848</td>\n",
              "      <td>-0.676990</td>\n",
              "      <td>-0.547788</td>\n",
              "      <td>-0.074236</td>\n",
              "      <td>-0.462974</td>\n",
              "      <td>-0.631639</td>\n",
              "      <td>-0.496350</td>\n",
              "      <td>-0.942065</td>\n",
              "      <td>-0.549434</td>\n",
              "      <td>-0.897226</td>\n",
              "      <td>-0.689941</td>\n",
              "      <td>-0.673642</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.729175</td>\n",
              "      <td>-0.500114</td>\n",
              "      <td>-0.711854</td>\n",
              "      <td>-0.807829</td>\n",
              "      <td>-0.195416</td>\n",
              "      <td>-0.425001</td>\n",
              "      <td>-0.403936</td>\n",
              "      <td>-0.586680</td>\n",
              "      <td>-0.333546</td>\n",
              "      <td>-0.763407</td>\n",
              "      <td>-0.790941</td>\n",
              "      <td>-0.560875</td>\n",
              "      <td>-0.249909</td>\n",
              "      <td>-0.421130</td>\n",
              "      <td>-0.775754</td>\n",
              "      <td>-0.721435</td>\n",
              "      <td>-0.560971</td>\n",
              "      <td>-0.723570</td>\n",
              "      <td>-0.792328</td>\n",
              "      <td>-0.767891</td>\n",
              "      <td>-0.736898</td>\n",
              "      <td>-0.615531</td>\n",
              "      <td>-0.931920</td>\n",
              "      <td>-0.446191</td>\n",
              "      <td>-0.477359</td>\n",
              "      <td>-0.705638</td>\n",
              "      <td>-0.852342</td>\n",
              "      <td>2.922840</td>\n",
              "      <td>3.615779</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>0.165952</td>\n",
              "      <td>2.128917</td>\n",
              "      <td>-1.022598</td>\n",
              "      <td>-1.560250</td>\n",
              "      <td>-1.393057</td>\n",
              "      <td>-1.167805</td>\n",
              "      <td>-1.085382</td>\n",
              "      <td>-1.527632</td>\n",
              "      <td>-1.750096</td>\n",
              "      <td>-1.392426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.394220</td>\n",
              "      <td>-1.078486</td>\n",
              "      <td>-1.135386</td>\n",
              "      <td>-1.433049</td>\n",
              "      <td>-0.730972</td>\n",
              "      <td>-0.749063</td>\n",
              "      <td>-0.828397</td>\n",
              "      <td>-0.349601</td>\n",
              "      <td>-0.475103</td>\n",
              "      <td>-0.759669</td>\n",
              "      <td>-0.645458</td>\n",
              "      <td>-0.722317</td>\n",
              "      <td>-0.713724</td>\n",
              "      <td>-0.733682</td>\n",
              "      <td>-0.507120</td>\n",
              "      <td>-0.423976</td>\n",
              "      <td>-0.700139</td>\n",
              "      <td>-0.647061</td>\n",
              "      <td>-0.721348</td>\n",
              "      <td>-0.661975</td>\n",
              "      <td>-0.829103</td>\n",
              "      <td>-0.650453</td>\n",
              "      <td>-0.582813</td>\n",
              "      <td>-0.637881</td>\n",
              "      <td>-0.358076</td>\n",
              "      <td>-0.282974</td>\n",
              "      <td>-0.760401</td>\n",
              "      <td>-0.388978</td>\n",
              "      <td>-0.779726</td>\n",
              "      <td>-0.530432</td>\n",
              "      <td>0.187615</td>\n",
              "      <td>-0.462974</td>\n",
              "      <td>-0.631639</td>\n",
              "      <td>-0.764552</td>\n",
              "      <td>-0.934388</td>\n",
              "      <td>-0.940714</td>\n",
              "      <td>-0.663710</td>\n",
              "      <td>-0.211999</td>\n",
              "      <td>-0.673642</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.639627</td>\n",
              "      <td>-0.622583</td>\n",
              "      <td>-0.711854</td>\n",
              "      <td>-0.807829</td>\n",
              "      <td>-0.642985</td>\n",
              "      <td>-0.425001</td>\n",
              "      <td>-0.626931</td>\n",
              "      <td>-0.679762</td>\n",
              "      <td>-0.493367</td>\n",
              "      <td>-0.019885</td>\n",
              "      <td>-0.829806</td>\n",
              "      <td>-0.077175</td>\n",
              "      <td>-0.542556</td>\n",
              "      <td>-0.645231</td>\n",
              "      <td>-0.475360</td>\n",
              "      <td>-0.721435</td>\n",
              "      <td>-0.651019</td>\n",
              "      <td>-0.712726</td>\n",
              "      <td>-0.792328</td>\n",
              "      <td>0.107923</td>\n",
              "      <td>-0.680355</td>\n",
              "      <td>-0.810009</td>\n",
              "      <td>0.497660</td>\n",
              "      <td>-0.446191</td>\n",
              "      <td>-0.526370</td>\n",
              "      <td>-0.705638</td>\n",
              "      <td>-0.852342</td>\n",
              "      <td>-0.073816</td>\n",
              "      <td>-0.471062</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>0.040198</td>\n",
              "      <td>1.811555</td>\n",
              "      <td>-1.012681</td>\n",
              "      <td>-1.536300</td>\n",
              "      <td>-1.345201</td>\n",
              "      <td>-1.177574</td>\n",
              "      <td>-1.078486</td>\n",
              "      <td>-1.066067</td>\n",
              "      <td>-1.502946</td>\n",
              "      <td>-1.337257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.380284</td>\n",
              "      <td>-1.423327</td>\n",
              "      <td>-1.323292</td>\n",
              "      <td>-1.440182</td>\n",
              "      <td>-0.806287</td>\n",
              "      <td>-0.746731</td>\n",
              "      <td>-0.644360</td>\n",
              "      <td>-0.379353</td>\n",
              "      <td>-0.475103</td>\n",
              "      <td>-0.503851</td>\n",
              "      <td>-0.635628</td>\n",
              "      <td>-0.714292</td>\n",
              "      <td>-0.662929</td>\n",
              "      <td>-0.569809</td>\n",
              "      <td>1.629344</td>\n",
              "      <td>-0.249257</td>\n",
              "      <td>-0.586511</td>\n",
              "      <td>-0.595423</td>\n",
              "      <td>-0.625603</td>\n",
              "      <td>-0.456356</td>\n",
              "      <td>-0.829103</td>\n",
              "      <td>-0.717498</td>\n",
              "      <td>-0.715441</td>\n",
              "      <td>-0.549768</td>\n",
              "      <td>-0.177242</td>\n",
              "      <td>-0.184512</td>\n",
              "      <td>-0.697325</td>\n",
              "      <td>-0.453054</td>\n",
              "      <td>-0.574255</td>\n",
              "      <td>-0.555502</td>\n",
              "      <td>-0.008774</td>\n",
              "      <td>-0.462974</td>\n",
              "      <td>-0.440925</td>\n",
              "      <td>-0.111816</td>\n",
              "      <td>-0.742615</td>\n",
              "      <td>-0.839358</td>\n",
              "      <td>-0.087219</td>\n",
              "      <td>-0.037014</td>\n",
              "      <td>-0.350096</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.711265</td>\n",
              "      <td>-0.438879</td>\n",
              "      <td>-0.711854</td>\n",
              "      <td>1.075230</td>\n",
              "      <td>-0.274325</td>\n",
              "      <td>-0.425001</td>\n",
              "      <td>-0.592664</td>\n",
              "      <td>-0.400515</td>\n",
              "      <td>-0.561862</td>\n",
              "      <td>-0.679298</td>\n",
              "      <td>-0.855716</td>\n",
              "      <td>-0.100933</td>\n",
              "      <td>0.227605</td>\n",
              "      <td>-0.645231</td>\n",
              "      <td>-0.775754</td>\n",
              "      <td>-0.721435</td>\n",
              "      <td>-0.651019</td>\n",
              "      <td>-0.798444</td>\n",
              "      <td>-0.066509</td>\n",
              "      <td>-0.773073</td>\n",
              "      <td>-0.715309</td>\n",
              "      <td>-0.784726</td>\n",
              "      <td>-0.498365</td>\n",
              "      <td>-0.417141</td>\n",
              "      <td>-0.526370</td>\n",
              "      <td>-0.357053</td>\n",
              "      <td>-0.678741</td>\n",
              "      <td>-2.389414</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>3.996616</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>2.816005</td>\n",
              "      <td>-1.313694</td>\n",
              "      <td>-2.349172</td>\n",
              "      <td>-1.357165</td>\n",
              "      <td>-1.213049</td>\n",
              "      <td>-1.423327</td>\n",
              "      <td>-1.059178</td>\n",
              "      <td>-1.042973</td>\n",
              "      <td>-1.309672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-06</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.289698</td>\n",
              "      <td>-1.347462</td>\n",
              "      <td>-1.336714</td>\n",
              "      <td>-1.383116</td>\n",
              "      <td>-1.107547</td>\n",
              "      <td>-0.725739</td>\n",
              "      <td>-0.799045</td>\n",
              "      <td>-0.437108</td>\n",
              "      <td>-0.326243</td>\n",
              "      <td>-0.735176</td>\n",
              "      <td>-0.486900</td>\n",
              "      <td>-0.528715</td>\n",
              "      <td>-0.717993</td>\n",
              "      <td>-0.692714</td>\n",
              "      <td>-0.907934</td>\n",
              "      <td>-0.432534</td>\n",
              "      <td>-0.700139</td>\n",
              "      <td>-0.624685</td>\n",
              "      <td>-0.520213</td>\n",
              "      <td>-0.323085</td>\n",
              "      <td>-0.824268</td>\n",
              "      <td>-0.784543</td>\n",
              "      <td>-0.715441</td>\n",
              "      <td>-0.432284</td>\n",
              "      <td>-0.411304</td>\n",
              "      <td>-0.564933</td>\n",
              "      <td>-0.760401</td>\n",
              "      <td>-0.493261</td>\n",
              "      <td>-0.787123</td>\n",
              "      <td>-0.451364</td>\n",
              "      <td>-0.434282</td>\n",
              "      <td>-0.462974</td>\n",
              "      <td>-0.388691</td>\n",
              "      <td>-0.704207</td>\n",
              "      <td>-0.911818</td>\n",
              "      <td>-0.908186</td>\n",
              "      <td>-0.856725</td>\n",
              "      <td>-0.689941</td>\n",
              "      <td>-0.421506</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.279642</td>\n",
              "      <td>-0.622583</td>\n",
              "      <td>-0.664541</td>\n",
              "      <td>-0.267578</td>\n",
              "      <td>-0.553283</td>\n",
              "      <td>-0.372249</td>\n",
              "      <td>-0.623214</td>\n",
              "      <td>-0.586680</td>\n",
              "      <td>-0.455695</td>\n",
              "      <td>-0.746585</td>\n",
              "      <td>-0.596614</td>\n",
              "      <td>-0.573764</td>\n",
              "      <td>-0.070856</td>\n",
              "      <td>-0.645231</td>\n",
              "      <td>-0.260878</td>\n",
              "      <td>-0.689652</td>\n",
              "      <td>-0.515947</td>\n",
              "      <td>-0.470030</td>\n",
              "      <td>-0.750590</td>\n",
              "      <td>-0.798985</td>\n",
              "      <td>-0.736898</td>\n",
              "      <td>-0.790561</td>\n",
              "      <td>-0.824935</td>\n",
              "      <td>-0.446191</td>\n",
              "      <td>-0.526370</td>\n",
              "      <td>-0.635921</td>\n",
              "      <td>1.760243</td>\n",
              "      <td>0.380223</td>\n",
              "      <td>0.148157</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>-0.044909</td>\n",
              "      <td>2.449565</td>\n",
              "      <td>-1.219214</td>\n",
              "      <td>-2.073424</td>\n",
              "      <td>-1.353177</td>\n",
              "      <td>-1.250581</td>\n",
              "      <td>-1.347462</td>\n",
              "      <td>-1.403630</td>\n",
              "      <td>-1.036108</td>\n",
              "      <td>-1.316568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-07</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.226985</td>\n",
              "      <td>-1.292287</td>\n",
              "      <td>-1.222628</td>\n",
              "      <td>-1.247583</td>\n",
              "      <td>-1.107547</td>\n",
              "      <td>-0.772388</td>\n",
              "      <td>-0.804916</td>\n",
              "      <td>-0.235492</td>\n",
              "      <td>-0.401418</td>\n",
              "      <td>-0.631760</td>\n",
              "      <td>-0.613747</td>\n",
              "      <td>-0.729339</td>\n",
              "      <td>-0.657167</td>\n",
              "      <td>-0.590293</td>\n",
              "      <td>1.136488</td>\n",
              "      <td>-0.392241</td>\n",
              "      <td>-0.653912</td>\n",
              "      <td>-0.147724</td>\n",
              "      <td>-0.529858</td>\n",
              "      <td>-0.661975</td>\n",
              "      <td>-0.659878</td>\n",
              "      <td>-0.784543</td>\n",
              "      <td>-0.672532</td>\n",
              "      <td>-0.598440</td>\n",
              "      <td>-0.458116</td>\n",
              "      <td>-0.421716</td>\n",
              "      <td>-0.601306</td>\n",
              "      <td>-0.329884</td>\n",
              "      <td>-0.628704</td>\n",
              "      <td>-0.532360</td>\n",
              "      <td>-0.313437</td>\n",
              "      <td>-0.355661</td>\n",
              "      <td>-0.186235</td>\n",
              "      <td>-0.704989</td>\n",
              "      <td>-0.583700</td>\n",
              "      <td>2.935312</td>\n",
              "      <td>-0.468141</td>\n",
              "      <td>-0.307326</td>\n",
              "      <td>-0.189179</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.609896</td>\n",
              "      <td>-0.303551</td>\n",
              "      <td>-0.711854</td>\n",
              "      <td>-0.604393</td>\n",
              "      <td>0.030412</td>\n",
              "      <td>-0.399881</td>\n",
              "      <td>-0.434858</td>\n",
              "      <td>-0.386366</td>\n",
              "      <td>-0.327077</td>\n",
              "      <td>-0.443793</td>\n",
              "      <td>-0.629002</td>\n",
              "      <td>-0.576403</td>\n",
              "      <td>0.559201</td>\n",
              "      <td>-0.645231</td>\n",
              "      <td>0.426423</td>\n",
              "      <td>-0.721435</td>\n",
              "      <td>-0.651019</td>\n",
              "      <td>0.016910</td>\n",
              "      <td>-0.393315</td>\n",
              "      <td>-0.420675</td>\n",
              "      <td>-0.438249</td>\n",
              "      <td>-0.693322</td>\n",
              "      <td>0.478670</td>\n",
              "      <td>-0.446191</td>\n",
              "      <td>-0.526370</td>\n",
              "      <td>-0.705638</td>\n",
              "      <td>0.034349</td>\n",
              "      <td>0.244011</td>\n",
              "      <td>-0.037609</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>-0.047497</td>\n",
              "      <td>2.109300</td>\n",
              "      <td>-1.145215</td>\n",
              "      <td>-1.871284</td>\n",
              "      <td>-1.327255</td>\n",
              "      <td>-1.274231</td>\n",
              "      <td>-1.292287</td>\n",
              "      <td>-1.327850</td>\n",
              "      <td>-1.379371</td>\n",
              "      <td>-1.254503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-24</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.650843</td>\n",
              "      <td>1.728522</td>\n",
              "      <td>1.716768</td>\n",
              "      <td>1.719869</td>\n",
              "      <td>0.700014</td>\n",
              "      <td>2.358928</td>\n",
              "      <td>-0.211123</td>\n",
              "      <td>1.263329</td>\n",
              "      <td>-0.356015</td>\n",
              "      <td>0.788844</td>\n",
              "      <td>1.361882</td>\n",
              "      <td>2.575632</td>\n",
              "      <td>-0.609146</td>\n",
              "      <td>0.433910</td>\n",
              "      <td>-0.738812</td>\n",
              "      <td>-0.385538</td>\n",
              "      <td>-0.616633</td>\n",
              "      <td>-0.619636</td>\n",
              "      <td>0.447095</td>\n",
              "      <td>-0.250737</td>\n",
              "      <td>0.294307</td>\n",
              "      <td>-0.432557</td>\n",
              "      <td>-0.112113</td>\n",
              "      <td>0.239890</td>\n",
              "      <td>4.181543</td>\n",
              "      <td>-0.388150</td>\n",
              "      <td>-0.029277</td>\n",
              "      <td>0.162450</td>\n",
              "      <td>1.487856</td>\n",
              "      <td>-0.250801</td>\n",
              "      <td>-0.053288</td>\n",
              "      <td>-0.383797</td>\n",
              "      <td>5.496308</td>\n",
              "      <td>0.091459</td>\n",
              "      <td>1.807092</td>\n",
              "      <td>0.039842</td>\n",
              "      <td>-0.744711</td>\n",
              "      <td>-0.489362</td>\n",
              "      <td>-0.609080</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.349311</td>\n",
              "      <td>-0.438879</td>\n",
              "      <td>-0.033704</td>\n",
              "      <td>-0.339925</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>-0.343360</td>\n",
              "      <td>-0.613923</td>\n",
              "      <td>-0.231105</td>\n",
              "      <td>0.790908</td>\n",
              "      <td>1.129378</td>\n",
              "      <td>0.490319</td>\n",
              "      <td>-0.161648</td>\n",
              "      <td>2.311827</td>\n",
              "      <td>-0.458481</td>\n",
              "      <td>-0.488577</td>\n",
              "      <td>1.157108</td>\n",
              "      <td>-0.371871</td>\n",
              "      <td>0.363397</td>\n",
              "      <td>0.194352</td>\n",
              "      <td>0.823084</td>\n",
              "      <td>-0.340584</td>\n",
              "      <td>0.358802</td>\n",
              "      <td>0.174032</td>\n",
              "      <td>-0.422951</td>\n",
              "      <td>0.987237</td>\n",
              "      <td>0.688701</td>\n",
              "      <td>-0.165721</td>\n",
              "      <td>0.743454</td>\n",
              "      <td>0.643531</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>0.337622</td>\n",
              "      <td>0.302184</td>\n",
              "      <td>-0.025752</td>\n",
              "      <td>0.213581</td>\n",
              "      <td>1.806310</td>\n",
              "      <td>1.756056</td>\n",
              "      <td>1.728522</td>\n",
              "      <td>1.613766</td>\n",
              "      <td>1.819843</td>\n",
              "      <td>1.338441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-25</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.546321</td>\n",
              "      <td>1.569895</td>\n",
              "      <td>1.790589</td>\n",
              "      <td>1.541536</td>\n",
              "      <td>0.022179</td>\n",
              "      <td>-0.130031</td>\n",
              "      <td>1.725236</td>\n",
              "      <td>2.298362</td>\n",
              "      <td>1.931591</td>\n",
              "      <td>1.085211</td>\n",
              "      <td>0.804710</td>\n",
              "      <td>-0.218251</td>\n",
              "      <td>-0.401484</td>\n",
              "      <td>0.587951</td>\n",
              "      <td>-0.392407</td>\n",
              "      <td>-0.042875</td>\n",
              "      <td>-0.635272</td>\n",
              "      <td>-0.433109</td>\n",
              "      <td>0.260427</td>\n",
              "      <td>-0.489483</td>\n",
              "      <td>3.092557</td>\n",
              "      <td>0.336448</td>\n",
              "      <td>-0.195590</td>\n",
              "      <td>-0.565712</td>\n",
              "      <td>2.554514</td>\n",
              "      <td>-0.150677</td>\n",
              "      <td>-0.537974</td>\n",
              "      <td>-0.180990</td>\n",
              "      <td>4.648827</td>\n",
              "      <td>-0.185811</td>\n",
              "      <td>-0.350882</td>\n",
              "      <td>0.425652</td>\n",
              "      <td>-0.554705</td>\n",
              "      <td>0.028208</td>\n",
              "      <td>-0.379029</td>\n",
              "      <td>2.375735</td>\n",
              "      <td>-0.308329</td>\n",
              "      <td>0.238521</td>\n",
              "      <td>2.650348</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.723802</td>\n",
              "      <td>0.908282</td>\n",
              "      <td>0.407093</td>\n",
              "      <td>-0.078891</td>\n",
              "      <td>-0.287948</td>\n",
              "      <td>-0.324687</td>\n",
              "      <td>-0.626931</td>\n",
              "      <td>-0.214350</td>\n",
              "      <td>0.416470</td>\n",
              "      <td>1.866845</td>\n",
              "      <td>2.822237</td>\n",
              "      <td>0.548608</td>\n",
              "      <td>0.267950</td>\n",
              "      <td>-0.271730</td>\n",
              "      <td>-0.627960</td>\n",
              "      <td>0.051803</td>\n",
              "      <td>-0.602844</td>\n",
              "      <td>0.109341</td>\n",
              "      <td>-0.576544</td>\n",
              "      <td>1.662621</td>\n",
              "      <td>-0.024459</td>\n",
              "      <td>3.345588</td>\n",
              "      <td>-0.441931</td>\n",
              "      <td>1.809781</td>\n",
              "      <td>-0.526370</td>\n",
              "      <td>0.020812</td>\n",
              "      <td>-0.544063</td>\n",
              "      <td>-1.163509</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>1.545173</td>\n",
              "      <td>0.184163</td>\n",
              "      <td>0.712909</td>\n",
              "      <td>-0.455260</td>\n",
              "      <td>-0.420132</td>\n",
              "      <td>1.793349</td>\n",
              "      <td>1.774565</td>\n",
              "      <td>1.569895</td>\n",
              "      <td>1.744658</td>\n",
              "      <td>1.627616</td>\n",
              "      <td>1.359130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-28</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.379087</td>\n",
              "      <td>1.473339</td>\n",
              "      <td>1.347666</td>\n",
              "      <td>1.455937</td>\n",
              "      <td>0.549384</td>\n",
              "      <td>0.555943</td>\n",
              "      <td>0.132003</td>\n",
              "      <td>4.507039</td>\n",
              "      <td>-0.393975</td>\n",
              "      <td>2.173524</td>\n",
              "      <td>0.500914</td>\n",
              "      <td>1.039257</td>\n",
              "      <td>-0.245470</td>\n",
              "      <td>5.358281</td>\n",
              "      <td>4.332568</td>\n",
              "      <td>0.284028</td>\n",
              "      <td>1.031577</td>\n",
              "      <td>-0.018401</td>\n",
              "      <td>-0.110674</td>\n",
              "      <td>-0.079388</td>\n",
              "      <td>1.118431</td>\n",
              "      <td>-0.124150</td>\n",
              "      <td>-0.154502</td>\n",
              "      <td>-0.352731</td>\n",
              "      <td>0.633303</td>\n",
              "      <td>-0.464055</td>\n",
              "      <td>0.858260</td>\n",
              "      <td>0.175311</td>\n",
              "      <td>-0.248377</td>\n",
              "      <td>-0.261408</td>\n",
              "      <td>-0.301130</td>\n",
              "      <td>0.220987</td>\n",
              "      <td>0.257954</td>\n",
              "      <td>1.268418</td>\n",
              "      <td>-0.274928</td>\n",
              "      <td>-0.412251</td>\n",
              "      <td>1.119034</td>\n",
              "      <td>3.116361</td>\n",
              "      <td>-0.396317</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.455157</td>\n",
              "      <td>0.191837</td>\n",
              "      <td>-0.346757</td>\n",
              "      <td>0.346927</td>\n",
              "      <td>0.102509</td>\n",
              "      <td>-0.104297</td>\n",
              "      <td>-0.505176</td>\n",
              "      <td>0.087237</td>\n",
              "      <td>0.569061</td>\n",
              "      <td>-0.013156</td>\n",
              "      <td>-0.078410</td>\n",
              "      <td>0.542397</td>\n",
              "      <td>2.535411</td>\n",
              "      <td>1.343289</td>\n",
              "      <td>0.582628</td>\n",
              "      <td>1.971143</td>\n",
              "      <td>0.657378</td>\n",
              "      <td>0.226558</td>\n",
              "      <td>0.512811</td>\n",
              "      <td>0.028892</td>\n",
              "      <td>-0.501474</td>\n",
              "      <td>1.393228</td>\n",
              "      <td>0.378640</td>\n",
              "      <td>1.320590</td>\n",
              "      <td>0.129320</td>\n",
              "      <td>-0.333350</td>\n",
              "      <td>0.542698</td>\n",
              "      <td>-0.754874</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>0.728025</td>\n",
              "      <td>0.041666</td>\n",
              "      <td>0.860450</td>\n",
              "      <td>-0.655335</td>\n",
              "      <td>-0.774883</td>\n",
              "      <td>1.763439</td>\n",
              "      <td>1.784848</td>\n",
              "      <td>1.473339</td>\n",
              "      <td>1.586210</td>\n",
              "      <td>1.758056</td>\n",
              "      <td>1.166038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.441800</td>\n",
              "      <td>1.383681</td>\n",
              "      <td>1.528861</td>\n",
              "      <td>1.334670</td>\n",
              "      <td>-0.128451</td>\n",
              "      <td>0.074292</td>\n",
              "      <td>-0.652285</td>\n",
              "      <td>0.255948</td>\n",
              "      <td>0.659955</td>\n",
              "      <td>-0.416765</td>\n",
              "      <td>1.140535</td>\n",
              "      <td>-0.456591</td>\n",
              "      <td>0.578137</td>\n",
              "      <td>0.517076</td>\n",
              "      <td>0.153044</td>\n",
              "      <td>-0.197911</td>\n",
              "      <td>-0.461996</td>\n",
              "      <td>-0.454510</td>\n",
              "      <td>0.220521</td>\n",
              "      <td>-0.450644</td>\n",
              "      <td>-0.671482</td>\n",
              "      <td>2.767497</td>\n",
              "      <td>0.481854</td>\n",
              "      <td>-0.537180</td>\n",
              "      <td>1.438857</td>\n",
              "      <td>0.431859</td>\n",
              "      <td>1.905149</td>\n",
              "      <td>1.068093</td>\n",
              "      <td>-0.371660</td>\n",
              "      <td>-0.412794</td>\n",
              "      <td>-0.416083</td>\n",
              "      <td>0.123385</td>\n",
              "      <td>-0.631639</td>\n",
              "      <td>2.412746</td>\n",
              "      <td>-0.723730</td>\n",
              "      <td>0.020985</td>\n",
              "      <td>-0.297383</td>\n",
              "      <td>-0.250391</td>\n",
              "      <td>-0.093558</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.639627</td>\n",
              "      <td>-0.080045</td>\n",
              "      <td>2.103256</td>\n",
              "      <td>0.709298</td>\n",
              "      <td>-0.348937</td>\n",
              "      <td>-0.167099</td>\n",
              "      <td>0.209745</td>\n",
              "      <td>-0.359558</td>\n",
              "      <td>1.044719</td>\n",
              "      <td>0.719600</td>\n",
              "      <td>0.340687</td>\n",
              "      <td>-0.332612</td>\n",
              "      <td>1.332268</td>\n",
              "      <td>0.101771</td>\n",
              "      <td>-0.728892</td>\n",
              "      <td>0.260530</td>\n",
              "      <td>-0.651019</td>\n",
              "      <td>-0.359526</td>\n",
              "      <td>-0.506424</td>\n",
              "      <td>-0.436222</td>\n",
              "      <td>-0.268621</td>\n",
              "      <td>-0.238245</td>\n",
              "      <td>0.432399</td>\n",
              "      <td>0.503723</td>\n",
              "      <td>2.012499</td>\n",
              "      <td>-0.684723</td>\n",
              "      <td>-0.241234</td>\n",
              "      <td>-0.709470</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>0.637230</td>\n",
              "      <td>-0.090654</td>\n",
              "      <td>0.971470</td>\n",
              "      <td>-0.814272</td>\n",
              "      <td>-1.090853</td>\n",
              "      <td>1.720568</td>\n",
              "      <td>1.778164</td>\n",
              "      <td>1.383681</td>\n",
              "      <td>1.489764</td>\n",
              "      <td>1.600155</td>\n",
              "      <td>1.276376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.358182</td>\n",
              "      <td>1.404371</td>\n",
              "      <td>1.408064</td>\n",
              "      <td>1.341804</td>\n",
              "      <td>-0.128451</td>\n",
              "      <td>0.016681</td>\n",
              "      <td>-0.315029</td>\n",
              "      <td>0.812492</td>\n",
              "      <td>0.057444</td>\n",
              "      <td>1.679307</td>\n",
              "      <td>-0.645458</td>\n",
              "      <td>-0.391389</td>\n",
              "      <td>1.542391</td>\n",
              "      <td>0.301993</td>\n",
              "      <td>0.145789</td>\n",
              "      <td>0.132914</td>\n",
              "      <td>-0.428147</td>\n",
              "      <td>-0.418421</td>\n",
              "      <td>2.542802</td>\n",
              "      <td>-0.249024</td>\n",
              "      <td>-0.385734</td>\n",
              "      <td>-0.382609</td>\n",
              "      <td>0.273549</td>\n",
              "      <td>-0.091246</td>\n",
              "      <td>-0.395858</td>\n",
              "      <td>-0.171086</td>\n",
              "      <td>0.321473</td>\n",
              "      <td>-0.262447</td>\n",
              "      <td>0.649738</td>\n",
              "      <td>-0.087844</td>\n",
              "      <td>-0.421189</td>\n",
              "      <td>0.024538</td>\n",
              "      <td>-0.542963</td>\n",
              "      <td>1.415929</td>\n",
              "      <td>0.063170</td>\n",
              "      <td>1.446563</td>\n",
              "      <td>-0.324748</td>\n",
              "      <td>0.541218</td>\n",
              "      <td>0.202109</td>\n",
              "      <td>...</td>\n",
              "      <td>1.599977</td>\n",
              "      <td>-0.500114</td>\n",
              "      <td>-0.640885</td>\n",
              "      <td>-0.181499</td>\n",
              "      <td>-0.315404</td>\n",
              "      <td>-0.038482</td>\n",
              "      <td>-0.378143</td>\n",
              "      <td>1.424273</td>\n",
              "      <td>0.113952</td>\n",
              "      <td>0.452470</td>\n",
              "      <td>0.161259</td>\n",
              "      <td>-0.085560</td>\n",
              "      <td>-0.143640</td>\n",
              "      <td>0.213821</td>\n",
              "      <td>-0.666410</td>\n",
              "      <td>-0.270775</td>\n",
              "      <td>-0.651019</td>\n",
              "      <td>2.189296</td>\n",
              "      <td>0.069556</td>\n",
              "      <td>0.405907</td>\n",
              "      <td>-0.283014</td>\n",
              "      <td>0.668021</td>\n",
              "      <td>0.891897</td>\n",
              "      <td>1.809200</td>\n",
              "      <td>-0.430997</td>\n",
              "      <td>0.235541</td>\n",
              "      <td>-0.128354</td>\n",
              "      <td>0.016992</td>\n",
              "      <td>-0.347218</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>-0.167191</td>\n",
              "      <td>0.736783</td>\n",
              "      <td>-0.774039</td>\n",
              "      <td>-1.007706</td>\n",
              "      <td>1.638814</td>\n",
              "      <td>1.778678</td>\n",
              "      <td>1.404371</td>\n",
              "      <td>1.400206</td>\n",
              "      <td>1.504041</td>\n",
              "      <td>1.186727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>163 rows × 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol      Open  ...  previous_day_price_3  target_price\n",
              "Date                         ...                                    \n",
              "2020-07-01   NICA -1.052783  ...             -1.750096     -1.392426\n",
              "2020-07-02   NICA -1.394220  ...             -1.502946     -1.337257\n",
              "2020-07-05   NICA -1.380284  ...             -1.042973     -1.309672\n",
              "2020-07-06   NICA -1.289698  ...             -1.036108     -1.316568\n",
              "2020-07-07   NICA -1.226985  ...             -1.379371     -1.254503\n",
              "...           ...       ...  ...                   ...           ...\n",
              "2021-02-24   NICA  1.650843  ...              1.819843      1.338441\n",
              "2021-02-25   NICA  1.546321  ...              1.627616      1.359130\n",
              "2021-02-28   NICA  1.379087  ...              1.758056      1.166038\n",
              "2021-03-01   NICA  1.441800  ...              1.600155      1.276376\n",
              "2021-03-02   NICA  1.358182  ...              1.504041      1.186727\n",
              "\n",
              "[163 rows x 119 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NKIVdOoq10j"
      },
      "source": [
        "total_entry= len(final_df)\n",
        "train_df_x=final_df.iloc[:int(.8*total_entry),1:-1]\n",
        "train_df_y=final_df.iloc[:int(.8*total_entry),-1]\n",
        "test_df_x=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "test_df_y=final_df.iloc[int(.8*total_entry):,-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-oAcJJ6GK63"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IrBCsF7rxKf",
        "outputId": "13103a32-eabd-46e2-9f3a-49dc26d5ad99"
      },
      "source": [
        "test_df_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2021-01-17    1.462572\n",
              "2021-01-18    1.359130\n",
              "2021-01-19    1.455676\n",
              "2021-01-20    1.628079\n",
              "2021-01-21    1.483260\n",
              "2021-01-24    1.434987\n",
              "2021-01-25    1.324649\n",
              "2021-01-26    1.255688\n",
              "2021-01-27    1.159142\n",
              "2021-01-28    1.179830\n",
              "2021-01-31    1.359130\n",
              "2021-02-01    1.303961\n",
              "2021-02-02    1.276376\n",
              "2021-02-03    1.290168\n",
              "2021-02-04    1.428091\n",
              "2021-02-07    1.352234\n",
              "2021-02-08    1.345338\n",
              "2021-02-09    1.428091\n",
              "2021-02-10    1.359130\n",
              "2021-02-11    1.614287\n",
              "2021-02-14    1.634975\n",
              "2021-02-15    1.634975\n",
              "2021-02-16    1.924612\n",
              "2021-02-17    1.745313\n",
              "2021-02-18    1.552221\n",
              "2021-02-21    1.683248\n",
              "2021-02-22    1.524637\n",
              "2021-02-23    1.428091\n",
              "2021-02-24    1.338441\n",
              "2021-02-25    1.359130\n",
              "2021-02-28    1.166038\n",
              "2021-03-01    1.276376\n",
              "2021-03-02    1.186727\n",
              "Name: target_price, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecc-4tGOrzBO"
      },
      "source": [
        "train_df_x=np.array(train_df_x)\n",
        "test_df_x=np.array(test_df_x)\n",
        "train_df_y=np.array(train_df_y)\n",
        "test_df_y=np.array(test_df_y)\n",
        "train_df_x =train_df_x.reshape(train_df_x.shape[0],train_df_x.shape[1] , 1)\n",
        "test_df_x = test_df_x.reshape(test_df_x.shape[0],test_df_x.shape[1] , 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9g60ztHzAGX",
        "outputId": "73d48462-8273-4fa7-de53-0c20454c3dcc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard , ModelCheckpoint\n",
        "import time\n",
        "\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "# NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128,input_shape = (train_df_x.shape[1:]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Flatten()) \n",
        "\n",
        "# model.add(Dense(32, activation=\"swish\"))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation=\"relu\"))\n",
        "\n",
        "opt=tf.keras.optimizers.Adam()\n",
        "model.compile(loss=tf.keras.losses.log_cosh,\n",
        "              optimizer=opt,\n",
        "              metrics=[tf.keras.metrics.LogCoshError()]\n",
        "              )\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=f'logs/{Company_symbol}')\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True,monitor='val_loss', verbose=1,)\n",
        "\n",
        "history = model.fit(\n",
        "    train_df_x, train_df_y,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data= (test_df_x,test_df_y),\n",
        "    callbacks = [tensorboard,checkpoint_cb]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 7s 1s/step - loss: 0.9906 - logcosh: 0.9906 - val_loss: 0.7606 - val_logcosh: 0.7606\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.76058, saving model to my_keras_model.h5\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.5730 - logcosh: 0.5730 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.76058\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.4992 - logcosh: 0.4992 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.76058\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3383 - logcosh: 0.3383 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.76058\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3612 - logcosh: 0.3612 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.76058\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3742 - logcosh: 0.3742 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.76058\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3619 - logcosh: 0.3619 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.76058\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3002 - logcosh: 0.3002 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.76058\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3230 - logcosh: 0.3230 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.76058\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3243 - logcosh: 0.3243 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.76058\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3391 - logcosh: 0.3391 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.76058\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3075 - logcosh: 0.3075 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.76058\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2984 - logcosh: 0.2984 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.76058\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3082 - logcosh: 0.3082 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.76058\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3314 - logcosh: 0.3314 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.76058\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3211 - logcosh: 0.3211 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.76058\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3018 - logcosh: 0.3018 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.76058\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2854 - logcosh: 0.2854 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.76058\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2616 - logcosh: 0.2616 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.76058\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2594 - logcosh: 0.2594 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.76058\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2828 - logcosh: 0.2828 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.76058\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2668 - logcosh: 0.2668 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.76058\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2884 - logcosh: 0.2884 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.76058\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2929 - logcosh: 0.2929 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.76058\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2657 - logcosh: 0.2657 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.76058\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2943 - logcosh: 0.2943 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.76058\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2682 - logcosh: 0.2682 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.76058\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2789 - logcosh: 0.2789 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.76058\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2744 - logcosh: 0.2744 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.76058\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2571 - logcosh: 0.2571 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.76058\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2966 - logcosh: 0.2966 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.76058\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2512 - logcosh: 0.2512 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.76058\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2664 - logcosh: 0.2664 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.76058\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2536 - logcosh: 0.2536 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.76058\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2658 - logcosh: 0.2658 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.76058\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2460 - logcosh: 0.2460 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.76058\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2491 - logcosh: 0.2491 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.76058\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2632 - logcosh: 0.2632 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.76058\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2440 - logcosh: 0.2440 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.76058\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2714 - logcosh: 0.2714 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.76058\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2596 - logcosh: 0.2596 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.76058\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2459 - logcosh: 0.2459 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.76058\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2776 - logcosh: 0.2776 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.76058\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2816 - logcosh: 0.2816 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.76058\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2759 - logcosh: 0.2759 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.76058\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2519 - logcosh: 0.2519 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.76058\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2768 - logcosh: 0.2768 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.76058\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2974 - logcosh: 0.2974 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.76058\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2711 - logcosh: 0.2711 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.76058\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2444 - logcosh: 0.2444 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.76058\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2634 - logcosh: 0.2634 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.76058\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2532 - logcosh: 0.2532 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.76058\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2667 - logcosh: 0.2667 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.76058\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2736 - logcosh: 0.2736 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.76058\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2499 - logcosh: 0.2499 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.76058\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2572 - logcosh: 0.2572 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.76058\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2576 - logcosh: 0.2576 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.76058\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2706 - logcosh: 0.2706 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.76058\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2567 - logcosh: 0.2567 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.76058\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2534 - logcosh: 0.2534 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.76058\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2385 - logcosh: 0.2385 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.76058\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2472 - logcosh: 0.2472 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.76058\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2475 - logcosh: 0.2475 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.76058\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2625 - logcosh: 0.2625 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.76058\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2498 - logcosh: 0.2498 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.76058\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2394 - logcosh: 0.2394 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.76058\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2554 - logcosh: 0.2554 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.76058\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2602 - logcosh: 0.2602 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.76058\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2755 - logcosh: 0.2755 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.76058\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2602 - logcosh: 0.2602 - val_loss: 0.7882 - val_logcosh: 0.7882\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.76058\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2465 - logcosh: 0.2465 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.76058\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2415 - logcosh: 0.2415 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.76058\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2435 - logcosh: 0.2435 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.76058\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2467 - logcosh: 0.2467 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.76058\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2470 - logcosh: 0.2470 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.76058\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2444 - logcosh: 0.2444 - val_loss: 0.7890 - val_logcosh: 0.7890\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.76058\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2615 - logcosh: 0.2615 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.76058\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2571 - logcosh: 0.2571 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.76058\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2527 - logcosh: 0.2527 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.76058\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2431 - logcosh: 0.2431 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.76058\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2443 - logcosh: 0.2443 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.76058\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2647 - logcosh: 0.2647 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.76058\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2760 - logcosh: 0.2760 - val_loss: 0.7892 - val_logcosh: 0.7892\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.76058\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2564 - logcosh: 0.2564 - val_loss: 0.7870 - val_logcosh: 0.7870\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.76058\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2739 - logcosh: 0.2739 - val_loss: 0.7865 - val_logcosh: 0.7865\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.76058\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2794 - logcosh: 0.2794 - val_loss: 0.7800 - val_logcosh: 0.7800\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.76058\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2708 - logcosh: 0.2708 - val_loss: 0.7763 - val_logcosh: 0.7763\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.76058\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2704 - logcosh: 0.2704 - val_loss: 0.7645 - val_logcosh: 0.7645\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.76058\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2621 - logcosh: 0.2621 - val_loss: 0.7455 - val_logcosh: 0.7455\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.76058 to 0.74547, saving model to my_keras_model.h5\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2372 - logcosh: 0.2372 - val_loss: 0.6964 - val_logcosh: 0.6964\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.74547 to 0.69642, saving model to my_keras_model.h5\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2451 - logcosh: 0.2451 - val_loss: 0.7192 - val_logcosh: 0.7192\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.69642\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2565 - logcosh: 0.2565 - val_loss: 0.7467 - val_logcosh: 0.7467\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.69642\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2405 - logcosh: 0.2405 - val_loss: 0.7471 - val_logcosh: 0.7471\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.69642\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2410 - logcosh: 0.2410 - val_loss: 0.7479 - val_logcosh: 0.7479\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.69642\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2395 - logcosh: 0.2395 - val_loss: 0.7285 - val_logcosh: 0.7285\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.69642\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2589 - logcosh: 0.2589 - val_loss: 0.6702 - val_logcosh: 0.6702\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.69642 to 0.67015, saving model to my_keras_model.h5\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2314 - logcosh: 0.2314 - val_loss: 0.6615 - val_logcosh: 0.6615\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.67015 to 0.66152, saving model to my_keras_model.h5\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2470 - logcosh: 0.2470 - val_loss: 0.7094 - val_logcosh: 0.7094\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.66152\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2481 - logcosh: 0.2481 - val_loss: 0.7674 - val_logcosh: 0.7674\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.66152\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2498 - logcosh: 0.2498 - val_loss: 0.7838 - val_logcosh: 0.7838\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.66152\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2591 - logcosh: 0.2591 - val_loss: 0.7836 - val_logcosh: 0.7836\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.66152\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2516 - logcosh: 0.2516 - val_loss: 0.7655 - val_logcosh: 0.7655\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.66152\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2433 - logcosh: 0.2433 - val_loss: 0.7089 - val_logcosh: 0.7089\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.66152\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2356 - logcosh: 0.2356 - val_loss: 0.6186 - val_logcosh: 0.6186\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.66152 to 0.61864, saving model to my_keras_model.h5\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2600 - logcosh: 0.2600 - val_loss: 0.4831 - val_logcosh: 0.4831\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.61864 to 0.48315, saving model to my_keras_model.h5\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2546 - logcosh: 0.2546 - val_loss: 0.4631 - val_logcosh: 0.4631\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.48315 to 0.46306, saving model to my_keras_model.h5\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2390 - logcosh: 0.2390 - val_loss: 0.4470 - val_logcosh: 0.4470\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.46306 to 0.44704, saving model to my_keras_model.h5\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2426 - logcosh: 0.2426 - val_loss: 0.4214 - val_logcosh: 0.4214\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.44704 to 0.42140, saving model to my_keras_model.h5\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2547 - logcosh: 0.2547 - val_loss: 0.4423 - val_logcosh: 0.4423\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.42140\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2389 - logcosh: 0.2389 - val_loss: 0.4966 - val_logcosh: 0.4966\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.42140\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2466 - logcosh: 0.2466 - val_loss: 0.5564 - val_logcosh: 0.5564\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.42140\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2485 - logcosh: 0.2485 - val_loss: 0.5795 - val_logcosh: 0.5795\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.42140\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2431 - logcosh: 0.2431 - val_loss: 0.6492 - val_logcosh: 0.6492\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.42140\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2620 - logcosh: 0.2620 - val_loss: 0.7154 - val_logcosh: 0.7154\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.42140\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2762 - logcosh: 0.2762 - val_loss: 0.7620 - val_logcosh: 0.7620\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.42140\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2414 - logcosh: 0.2414 - val_loss: 0.7807 - val_logcosh: 0.7807\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.42140\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2429 - logcosh: 0.2429 - val_loss: 0.7858 - val_logcosh: 0.7858\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.42140\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2390 - logcosh: 0.2390 - val_loss: 0.7700 - val_logcosh: 0.7700\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.42140\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2479 - logcosh: 0.2479 - val_loss: 0.7487 - val_logcosh: 0.7487\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.42140\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2413 - logcosh: 0.2413 - val_loss: 0.7246 - val_logcosh: 0.7246\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.42140\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2407 - logcosh: 0.2407 - val_loss: 0.7380 - val_logcosh: 0.7380\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.42140\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2458 - logcosh: 0.2458 - val_loss: 0.7378 - val_logcosh: 0.7378\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.42140\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2357 - logcosh: 0.2357 - val_loss: 0.7394 - val_logcosh: 0.7394\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.42140\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2567 - logcosh: 0.2567 - val_loss: 0.7222 - val_logcosh: 0.7222\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.42140\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2541 - logcosh: 0.2541 - val_loss: 0.7440 - val_logcosh: 0.7440\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.42140\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2451 - logcosh: 0.2451 - val_loss: 0.7422 - val_logcosh: 0.7422\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.42140\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2557 - logcosh: 0.2557 - val_loss: 0.7136 - val_logcosh: 0.7136\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.42140\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2400 - logcosh: 0.2400 - val_loss: 0.7047 - val_logcosh: 0.7047\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.42140\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2429 - logcosh: 0.2429 - val_loss: 0.6934 - val_logcosh: 0.6934\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.42140\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2627 - logcosh: 0.2627 - val_loss: 0.6998 - val_logcosh: 0.6998\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.42140\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2372 - logcosh: 0.2372 - val_loss: 0.6871 - val_logcosh: 0.6871\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.42140\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2492 - logcosh: 0.2492 - val_loss: 0.6789 - val_logcosh: 0.6789\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.42140\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2565 - logcosh: 0.2565 - val_loss: 0.5833 - val_logcosh: 0.5833\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.42140\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2493 - logcosh: 0.2493 - val_loss: 0.5653 - val_logcosh: 0.5653\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.42140\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2430 - logcosh: 0.2430 - val_loss: 0.6043 - val_logcosh: 0.6043\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.42140\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2546 - logcosh: 0.2546 - val_loss: 0.5190 - val_logcosh: 0.5190\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.42140\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2351 - logcosh: 0.2351 - val_loss: 0.3228 - val_logcosh: 0.3228\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.42140 to 0.32278, saving model to my_keras_model.h5\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2530 - logcosh: 0.2530 - val_loss: 0.3105 - val_logcosh: 0.3105\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.32278 to 0.31046, saving model to my_keras_model.h5\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2435 - logcosh: 0.2435 - val_loss: 0.3941 - val_logcosh: 0.3941\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.31046\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2396 - logcosh: 0.2396 - val_loss: 0.4417 - val_logcosh: 0.4417\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.31046\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2487 - logcosh: 0.2487 - val_loss: 0.4916 - val_logcosh: 0.4916\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.31046\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2243 - logcosh: 0.2243 - val_loss: 0.5827 - val_logcosh: 0.5827\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.31046\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2541 - logcosh: 0.2541 - val_loss: 0.6563 - val_logcosh: 0.6563\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.31046\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2449 - logcosh: 0.2449 - val_loss: 0.6561 - val_logcosh: 0.6561\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.31046\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2378 - logcosh: 0.2378 - val_loss: 0.6584 - val_logcosh: 0.6584\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.31046\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2408 - logcosh: 0.2408 - val_loss: 0.6667 - val_logcosh: 0.6667\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.31046\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2399 - logcosh: 0.2399 - val_loss: 0.6424 - val_logcosh: 0.6424\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.31046\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2296 - logcosh: 0.2296 - val_loss: 0.6665 - val_logcosh: 0.6665\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.31046\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2361 - logcosh: 0.2361 - val_loss: 0.6710 - val_logcosh: 0.6710\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.31046\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2459 - logcosh: 0.2459 - val_loss: 0.6686 - val_logcosh: 0.6686\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.31046\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2469 - logcosh: 0.2469 - val_loss: 0.6201 - val_logcosh: 0.6201\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.31046\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2555 - logcosh: 0.2555 - val_loss: 0.5789 - val_logcosh: 0.5789\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.31046\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2524 - logcosh: 0.2524 - val_loss: 0.5060 - val_logcosh: 0.5060\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.31046\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2419 - logcosh: 0.2419 - val_loss: 0.4575 - val_logcosh: 0.4575\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.31046\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2328 - logcosh: 0.2328 - val_loss: 0.4082 - val_logcosh: 0.4082\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.31046\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.2376 - logcosh: 0.2376 - val_loss: 0.3515 - val_logcosh: 0.3515\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.31046\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2532 - logcosh: 0.2532 - val_loss: 0.2826 - val_logcosh: 0.2826\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.31046 to 0.28260, saving model to my_keras_model.h5\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2290 - logcosh: 0.2290 - val_loss: 0.2994 - val_logcosh: 0.2994\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.28260\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2319 - logcosh: 0.2319 - val_loss: 0.3267 - val_logcosh: 0.3267\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.28260\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.2308 - logcosh: 0.2308 - val_loss: 0.3877 - val_logcosh: 0.3877\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.28260\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2227 - logcosh: 0.2227 - val_loss: 0.4093 - val_logcosh: 0.4093\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.28260\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2364 - logcosh: 0.2364 - val_loss: 0.3286 - val_logcosh: 0.3286\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.28260\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2588 - logcosh: 0.2588 - val_loss: 0.2732 - val_logcosh: 0.2732\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.28260 to 0.27323, saving model to my_keras_model.h5\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2267 - logcosh: 0.2267 - val_loss: 0.2720 - val_logcosh: 0.2720\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.27323 to 0.27198, saving model to my_keras_model.h5\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2456 - logcosh: 0.2456 - val_loss: 0.3311 - val_logcosh: 0.3311\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.27198\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2420 - logcosh: 0.2420 - val_loss: 0.3965 - val_logcosh: 0.3965\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.27198\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2333 - logcosh: 0.2333 - val_loss: 0.3436 - val_logcosh: 0.3436\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.27198\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2617 - logcosh: 0.2617 - val_loss: 0.2799 - val_logcosh: 0.2799\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.27198\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2329 - logcosh: 0.2329 - val_loss: 0.2648 - val_logcosh: 0.2648\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.27198 to 0.26477, saving model to my_keras_model.h5\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2515 - logcosh: 0.2515 - val_loss: 0.2084 - val_logcosh: 0.2084\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.26477 to 0.20842, saving model to my_keras_model.h5\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2249 - logcosh: 0.2249 - val_loss: 0.1785 - val_logcosh: 0.1785\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.20842 to 0.17851, saving model to my_keras_model.h5\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2302 - logcosh: 0.2302 - val_loss: 0.1769 - val_logcosh: 0.1769\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.17851 to 0.17692, saving model to my_keras_model.h5\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2288 - logcosh: 0.2288 - val_loss: 0.1625 - val_logcosh: 0.1625\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.17692 to 0.16252, saving model to my_keras_model.h5\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2335 - logcosh: 0.2335 - val_loss: 0.1182 - val_logcosh: 0.1182\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.16252 to 0.11819, saving model to my_keras_model.h5\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2339 - logcosh: 0.2339 - val_loss: 0.0981 - val_logcosh: 0.0981\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.11819 to 0.09805, saving model to my_keras_model.h5\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2303 - logcosh: 0.2303 - val_loss: 0.0812 - val_logcosh: 0.0812\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.09805 to 0.08118, saving model to my_keras_model.h5\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2356 - logcosh: 0.2356 - val_loss: 0.0800 - val_logcosh: 0.0800\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.08118 to 0.08003, saving model to my_keras_model.h5\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2281 - logcosh: 0.2281 - val_loss: 0.0901 - val_logcosh: 0.0901\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.08003\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2344 - logcosh: 0.2344 - val_loss: 0.1113 - val_logcosh: 0.1113\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.08003\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2296 - logcosh: 0.2296 - val_loss: 0.1338 - val_logcosh: 0.1338\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.08003\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2291 - logcosh: 0.2291 - val_loss: 0.1430 - val_logcosh: 0.1430\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.08003\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2418 - logcosh: 0.2418 - val_loss: 0.1477 - val_logcosh: 0.1477\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.08003\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2240 - logcosh: 0.2240 - val_loss: 0.1136 - val_logcosh: 0.1136\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.08003\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2311 - logcosh: 0.2311 - val_loss: 0.0646 - val_logcosh: 0.0646\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.08003 to 0.06461, saving model to my_keras_model.h5\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2268 - logcosh: 0.2268 - val_loss: 0.0486 - val_logcosh: 0.0486\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.06461 to 0.04860, saving model to my_keras_model.h5\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2385 - logcosh: 0.2385 - val_loss: 0.0396 - val_logcosh: 0.0396\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.04860 to 0.03960, saving model to my_keras_model.h5\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2348 - logcosh: 0.2348 - val_loss: 0.0376 - val_logcosh: 0.0376\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.03960 to 0.03761, saving model to my_keras_model.h5\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2257 - logcosh: 0.2257 - val_loss: 0.0446 - val_logcosh: 0.0446\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.03761\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2357 - logcosh: 0.2357 - val_loss: 0.0593 - val_logcosh: 0.0593\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.03761\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2378 - logcosh: 0.2378 - val_loss: 0.0635 - val_logcosh: 0.0635\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.03761\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2185 - logcosh: 0.2185 - val_loss: 0.0480 - val_logcosh: 0.0480\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.03761\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2388 - logcosh: 0.2388 - val_loss: 0.0370 - val_logcosh: 0.0370\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.03761 to 0.03701, saving model to my_keras_model.h5\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2295 - logcosh: 0.2295 - val_loss: 0.0402 - val_logcosh: 0.0402\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.03701\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2291 - logcosh: 0.2291 - val_loss: 0.0474 - val_logcosh: 0.0474\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.03701\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2202 - logcosh: 0.2202 - val_loss: 0.0594 - val_logcosh: 0.0594\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.03701\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2502 - logcosh: 0.2502 - val_loss: 0.0531 - val_logcosh: 0.0531\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.03701\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2316 - logcosh: 0.2316 - val_loss: 0.0525 - val_logcosh: 0.0525\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.03701\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2233 - logcosh: 0.2233 - val_loss: 0.0579 - val_logcosh: 0.0579\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.03701\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2211 - logcosh: 0.2211 - val_loss: 0.0572 - val_logcosh: 0.0572\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.03701\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2219 - logcosh: 0.2219 - val_loss: 0.0540 - val_logcosh: 0.0540\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.03701\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2448 - logcosh: 0.2448 - val_loss: 0.0660 - val_logcosh: 0.0660\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.03701\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2267 - logcosh: 0.2267 - val_loss: 0.0549 - val_logcosh: 0.0549\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.03701\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2262 - logcosh: 0.2262 - val_loss: 0.0482 - val_logcosh: 0.0482\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.03701\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2343 - logcosh: 0.2343 - val_loss: 0.0427 - val_logcosh: 0.0427\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.03701\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2296 - logcosh: 0.2296 - val_loss: 0.0466 - val_logcosh: 0.0466\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.03701\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.2406 - logcosh: 0.2406 - val_loss: 0.0562 - val_logcosh: 0.0562\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.03701\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2441 - logcosh: 0.2441 - val_loss: 0.0616 - val_logcosh: 0.0616\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.03701\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2352 - logcosh: 0.2352 - val_loss: 0.0539 - val_logcosh: 0.0539\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.03701\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2338 - logcosh: 0.2338 - val_loss: 0.0731 - val_logcosh: 0.0731\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.03701\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2338 - logcosh: 0.2338 - val_loss: 0.0648 - val_logcosh: 0.0648\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.03701\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.2276 - logcosh: 0.2276 - val_loss: 0.0500 - val_logcosh: 0.0500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.03701\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2292 - logcosh: 0.2292 - val_loss: 0.0489 - val_logcosh: 0.0489\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.03701\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.2283 - logcosh: 0.2283 - val_loss: 0.0657 - val_logcosh: 0.0657\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.03701\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2434 - logcosh: 0.2434 - val_loss: 0.0692 - val_logcosh: 0.0692\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.03701\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.2318 - logcosh: 0.2318 - val_loss: 0.0645 - val_logcosh: 0.0645\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.03701\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2276 - logcosh: 0.2276 - val_loss: 0.0558 - val_logcosh: 0.0558\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.03701\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2286 - logcosh: 0.2286 - val_loss: 0.0535 - val_logcosh: 0.0535\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.03701\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2147 - logcosh: 0.2147 - val_loss: 0.0659 - val_logcosh: 0.0659\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.03701\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2307 - logcosh: 0.2307 - val_loss: 0.0674 - val_logcosh: 0.0674\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.03701\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2325 - logcosh: 0.2325 - val_loss: 0.1092 - val_logcosh: 0.1092\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.03701\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.2295 - logcosh: 0.2295 - val_loss: 0.1446 - val_logcosh: 0.1446\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.03701\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2396 - logcosh: 0.2396 - val_loss: 0.1952 - val_logcosh: 0.1952\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.03701\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2462 - logcosh: 0.2462 - val_loss: 0.1977 - val_logcosh: 0.1977\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.03701\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.2676 - logcosh: 0.2676 - val_loss: 0.1750 - val_logcosh: 0.1750\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.03701\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2443 - logcosh: 0.2443 - val_loss: 0.1601 - val_logcosh: 0.1601\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.03701\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2619 - logcosh: 0.2619 - val_loss: 0.1244 - val_logcosh: 0.1244\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.03701\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2443 - logcosh: 0.2443 - val_loss: 0.0932 - val_logcosh: 0.0932\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.03701\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2332 - logcosh: 0.2332 - val_loss: 0.0835 - val_logcosh: 0.0835\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.03701\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2260 - logcosh: 0.2260 - val_loss: 0.0896 - val_logcosh: 0.0896\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.03701\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2286 - logcosh: 0.2286 - val_loss: 0.0822 - val_logcosh: 0.0822\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.03701\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2247 - logcosh: 0.2247 - val_loss: 0.0617 - val_logcosh: 0.0617\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.03701\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2232 - logcosh: 0.2232 - val_loss: 0.0568 - val_logcosh: 0.0568\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.03701\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2180 - logcosh: 0.2180 - val_loss: 0.0561 - val_logcosh: 0.0561\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.03701\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2213 - logcosh: 0.2213 - val_loss: 0.0606 - val_logcosh: 0.0606\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.03701\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2381 - logcosh: 0.2381 - val_loss: 0.0583 - val_logcosh: 0.0583\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.03701\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2281 - logcosh: 0.2281 - val_loss: 0.0555 - val_logcosh: 0.0555\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.03701\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2360 - logcosh: 0.2360 - val_loss: 0.0574 - val_logcosh: 0.0574\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.03701\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2307 - logcosh: 0.2307 - val_loss: 0.0738 - val_logcosh: 0.0738\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.03701\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2463 - logcosh: 0.2463 - val_loss: 0.0926 - val_logcosh: 0.0926\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.03701\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2405 - logcosh: 0.2405 - val_loss: 0.0800 - val_logcosh: 0.0800\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.03701\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2255 - logcosh: 0.2255 - val_loss: 0.0502 - val_logcosh: 0.0502\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.03701\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2326 - logcosh: 0.2326 - val_loss: 0.0436 - val_logcosh: 0.0436\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.03701\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2185 - logcosh: 0.2185 - val_loss: 0.0485 - val_logcosh: 0.0485\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.03701\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2322 - logcosh: 0.2322 - val_loss: 0.0490 - val_logcosh: 0.0490\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.03701\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2287 - logcosh: 0.2287 - val_loss: 0.0691 - val_logcosh: 0.0691\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.03701\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2205 - logcosh: 0.2205 - val_loss: 0.0922 - val_logcosh: 0.0922\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.03701\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2301 - logcosh: 0.2301 - val_loss: 0.0769 - val_logcosh: 0.0769\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.03701\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2238 - logcosh: 0.2238 - val_loss: 0.0544 - val_logcosh: 0.0544\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.03701\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2336 - logcosh: 0.2336 - val_loss: 0.0600 - val_logcosh: 0.0600\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.03701\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2278 - logcosh: 0.2278 - val_loss: 0.0663 - val_logcosh: 0.0663\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.03701\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2105 - logcosh: 0.2105 - val_loss: 0.0975 - val_logcosh: 0.0975\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.03701\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2193 - logcosh: 0.2193 - val_loss: 0.1205 - val_logcosh: 0.1205\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.03701\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2312 - logcosh: 0.2312 - val_loss: 0.1346 - val_logcosh: 0.1346\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.03701\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2396 - logcosh: 0.2396 - val_loss: 0.1534 - val_logcosh: 0.1534\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.03701\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2177 - logcosh: 0.2177 - val_loss: 0.1437 - val_logcosh: 0.1437\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.03701\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2446 - logcosh: 0.2446 - val_loss: 0.0907 - val_logcosh: 0.0907\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.03701\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2383 - logcosh: 0.2383 - val_loss: 0.0526 - val_logcosh: 0.0526\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.03701\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2406 - logcosh: 0.2406 - val_loss: 0.0606 - val_logcosh: 0.0606\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.03701\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2411 - logcosh: 0.2411 - val_loss: 0.0415 - val_logcosh: 0.0415\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.03701\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2306 - logcosh: 0.2306 - val_loss: 0.0382 - val_logcosh: 0.0382\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.03701\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2316 - logcosh: 0.2316 - val_loss: 0.0397 - val_logcosh: 0.0397\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.03701\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2270 - logcosh: 0.2270 - val_loss: 0.0471 - val_logcosh: 0.0471\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.03701\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2402 - logcosh: 0.2402 - val_loss: 0.0830 - val_logcosh: 0.0830\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.03701\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2184 - logcosh: 0.2184 - val_loss: 0.1003 - val_logcosh: 0.1003\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.03701\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2289 - logcosh: 0.2289 - val_loss: 0.0895 - val_logcosh: 0.0895\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.03701\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2430 - logcosh: 0.2430 - val_loss: 0.0841 - val_logcosh: 0.0841\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.03701\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2414 - logcosh: 0.2414 - val_loss: 0.0924 - val_logcosh: 0.0924\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.03701\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.2236 - logcosh: 0.2236 - val_loss: 0.1224 - val_logcosh: 0.1224\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.03701\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2344 - logcosh: 0.2344 - val_loss: 0.1626 - val_logcosh: 0.1626\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.03701\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2385 - logcosh: 0.2385 - val_loss: 0.1372 - val_logcosh: 0.1372\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.03701\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2399 - logcosh: 0.2399 - val_loss: 0.1111 - val_logcosh: 0.1111\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.03701\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2389 - logcosh: 0.2389 - val_loss: 0.1101 - val_logcosh: 0.1101\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.03701\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2430 - logcosh: 0.2430 - val_loss: 0.1164 - val_logcosh: 0.1164\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.03701\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2283 - logcosh: 0.2283 - val_loss: 0.1467 - val_logcosh: 0.1467\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.03701\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2225 - logcosh: 0.2225 - val_loss: 0.1604 - val_logcosh: 0.1604\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.03701\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2240 - logcosh: 0.2240 - val_loss: 0.1510 - val_logcosh: 0.1510\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.03701\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2359 - logcosh: 0.2359 - val_loss: 0.1381 - val_logcosh: 0.1381\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.03701\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2373 - logcosh: 0.2373 - val_loss: 0.1506 - val_logcosh: 0.1506\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.03701\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2375 - logcosh: 0.2375 - val_loss: 0.1717 - val_logcosh: 0.1717\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.03701\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2576 - logcosh: 0.2576 - val_loss: 0.1466 - val_logcosh: 0.1466\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.03701\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2348 - logcosh: 0.2348 - val_loss: 0.0837 - val_logcosh: 0.0837\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.03701\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2506 - logcosh: 0.2506 - val_loss: 0.0618 - val_logcosh: 0.0618\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.03701\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2390 - logcosh: 0.2390 - val_loss: 0.0690 - val_logcosh: 0.0690\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.03701\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2397 - logcosh: 0.2397 - val_loss: 0.1241 - val_logcosh: 0.1241\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.03701\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2239 - logcosh: 0.2239 - val_loss: 0.1610 - val_logcosh: 0.1610\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.03701\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2166 - logcosh: 0.2166 - val_loss: 0.2169 - val_logcosh: 0.2169\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.03701\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2245 - logcosh: 0.2245 - val_loss: 0.2227 - val_logcosh: 0.2227\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.03701\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2249 - logcosh: 0.2249 - val_loss: 0.1491 - val_logcosh: 0.1491\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.03701\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2349 - logcosh: 0.2349 - val_loss: 0.0878 - val_logcosh: 0.0878\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.03701\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2133 - logcosh: 0.2133 - val_loss: 0.0656 - val_logcosh: 0.0656\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.03701\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2277 - logcosh: 0.2277 - val_loss: 0.0670 - val_logcosh: 0.0670\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.03701\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2292 - logcosh: 0.2292 - val_loss: 0.0962 - val_logcosh: 0.0962\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.03701\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2238 - logcosh: 0.2238 - val_loss: 0.1420 - val_logcosh: 0.1420\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.03701\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2294 - logcosh: 0.2294 - val_loss: 0.1903 - val_logcosh: 0.1903\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.03701\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2305 - logcosh: 0.2305 - val_loss: 0.1877 - val_logcosh: 0.1877\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.03701\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2312 - logcosh: 0.2312 - val_loss: 0.1519 - val_logcosh: 0.1519\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.03701\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2207 - logcosh: 0.2207 - val_loss: 0.1405 - val_logcosh: 0.1405\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.03701\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2227 - logcosh: 0.2227 - val_loss: 0.0851 - val_logcosh: 0.0851\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.03701\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2166 - logcosh: 0.2166 - val_loss: 0.0602 - val_logcosh: 0.0602\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.03701\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2252 - logcosh: 0.2252 - val_loss: 0.1040 - val_logcosh: 0.1040\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.03701\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2345 - logcosh: 0.2345 - val_loss: 0.1533 - val_logcosh: 0.1533\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.03701\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2251 - logcosh: 0.2251 - val_loss: 0.1737 - val_logcosh: 0.1737\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.03701\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2275 - logcosh: 0.2275 - val_loss: 0.1847 - val_logcosh: 0.1847\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.03701\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2326 - logcosh: 0.2326 - val_loss: 0.2024 - val_logcosh: 0.2024\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.03701\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2203 - logcosh: 0.2203 - val_loss: 0.2177 - val_logcosh: 0.2177\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.03701\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2189 - logcosh: 0.2189 - val_loss: 0.2306 - val_logcosh: 0.2306\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.03701\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2162 - logcosh: 0.2162 - val_loss: 0.2329 - val_logcosh: 0.2329\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.03701\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2274 - logcosh: 0.2274 - val_loss: 0.2388 - val_logcosh: 0.2388\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.03701\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2184 - logcosh: 0.2184 - val_loss: 0.2286 - val_logcosh: 0.2286\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.03701\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2140 - logcosh: 0.2140 - val_loss: 0.2230 - val_logcosh: 0.2230\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.03701\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2218 - logcosh: 0.2218 - val_loss: 0.1935 - val_logcosh: 0.1935\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.03701\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2217 - logcosh: 0.2217 - val_loss: 0.1644 - val_logcosh: 0.1644\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.03701\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2264 - logcosh: 0.2264 - val_loss: 0.1491 - val_logcosh: 0.1491\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.03701\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2290 - logcosh: 0.2290 - val_loss: 0.1468 - val_logcosh: 0.1468\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.03701\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2194 - logcosh: 0.2194 - val_loss: 0.1507 - val_logcosh: 0.1507\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.03701\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2212 - logcosh: 0.2212 - val_loss: 0.1510 - val_logcosh: 0.1510\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.03701\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2160 - logcosh: 0.2160 - val_loss: 0.1347 - val_logcosh: 0.1347\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.03701\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2214 - logcosh: 0.2214 - val_loss: 0.1365 - val_logcosh: 0.1365\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.03701\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2189 - logcosh: 0.2189 - val_loss: 0.1312 - val_logcosh: 0.1312\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.03701\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2158 - logcosh: 0.2158 - val_loss: 0.1274 - val_logcosh: 0.1274\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.03701\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2188 - logcosh: 0.2188 - val_loss: 0.1290 - val_logcosh: 0.1290\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.03701\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2189 - logcosh: 0.2189 - val_loss: 0.1385 - val_logcosh: 0.1385\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.03701\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2212 - logcosh: 0.2212 - val_loss: 0.1426 - val_logcosh: 0.1426\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.03701\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2156 - logcosh: 0.2156 - val_loss: 0.1456 - val_logcosh: 0.1456\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.03701\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2203 - logcosh: 0.2203 - val_loss: 0.1267 - val_logcosh: 0.1267\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.03701\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2182 - logcosh: 0.2182 - val_loss: 0.1068 - val_logcosh: 0.1068\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.03701\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2199 - logcosh: 0.2199 - val_loss: 0.0950 - val_logcosh: 0.0950\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.03701\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2180 - logcosh: 0.2180 - val_loss: 0.0975 - val_logcosh: 0.0975\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.03701\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2276 - logcosh: 0.2276 - val_loss: 0.1100 - val_logcosh: 0.1100\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.03701\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2300 - logcosh: 0.2300 - val_loss: 0.1182 - val_logcosh: 0.1182\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.03701\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2320 - logcosh: 0.2320 - val_loss: 0.1186 - val_logcosh: 0.1186\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.03701\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2329 - logcosh: 0.2329 - val_loss: 0.1079 - val_logcosh: 0.1079\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.03701\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2307 - logcosh: 0.2307 - val_loss: 0.0911 - val_logcosh: 0.0911\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.03701\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2247 - logcosh: 0.2247 - val_loss: 0.0914 - val_logcosh: 0.0914\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.03701\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2278 - logcosh: 0.2278 - val_loss: 0.0947 - val_logcosh: 0.0947\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.03701\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2389 - logcosh: 0.2389 - val_loss: 0.0875 - val_logcosh: 0.0875\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.03701\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2336 - logcosh: 0.2336 - val_loss: 0.0951 - val_logcosh: 0.0951\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.03701\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2183 - logcosh: 0.2183 - val_loss: 0.1154 - val_logcosh: 0.1154\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.03701\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2207 - logcosh: 0.2207 - val_loss: 0.1322 - val_logcosh: 0.1322\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.03701\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2166 - logcosh: 0.2166 - val_loss: 0.1332 - val_logcosh: 0.1332\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.03701\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2295 - logcosh: 0.2295 - val_loss: 0.1213 - val_logcosh: 0.1213\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.03701\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2256 - logcosh: 0.2256 - val_loss: 0.1150 - val_logcosh: 0.1150\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.03701\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2185 - logcosh: 0.2185 - val_loss: 0.1118 - val_logcosh: 0.1118\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.03701\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2135 - logcosh: 0.2135 - val_loss: 0.1082 - val_logcosh: 0.1082\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.03701\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2217 - logcosh: 0.2217 - val_loss: 0.1045 - val_logcosh: 0.1045\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.03701\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2157 - logcosh: 0.2157 - val_loss: 0.1029 - val_logcosh: 0.1029\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.03701\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2291 - logcosh: 0.2291 - val_loss: 0.1092 - val_logcosh: 0.1092\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.03701\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2233 - logcosh: 0.2233 - val_loss: 0.1087 - val_logcosh: 0.1087\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.03701\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2154 - logcosh: 0.2154 - val_loss: 0.1058 - val_logcosh: 0.1058\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.03701\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2167 - logcosh: 0.2167 - val_loss: 0.1119 - val_logcosh: 0.1119\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.03701\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2170 - logcosh: 0.2170 - val_loss: 0.1163 - val_logcosh: 0.1163\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.03701\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2119 - logcosh: 0.2119 - val_loss: 0.1102 - val_logcosh: 0.1102\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.03701\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2129 - logcosh: 0.2129 - val_loss: 0.1117 - val_logcosh: 0.1117\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.03701\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2173 - logcosh: 0.2173 - val_loss: 0.1298 - val_logcosh: 0.1298\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.03701\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2144 - logcosh: 0.2144 - val_loss: 0.1457 - val_logcosh: 0.1457\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.03701\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2192 - logcosh: 0.2192 - val_loss: 0.1291 - val_logcosh: 0.1291\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.03701\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2250 - logcosh: 0.2250 - val_loss: 0.1050 - val_logcosh: 0.1050\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.03701\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2197 - logcosh: 0.2197 - val_loss: 0.0907 - val_logcosh: 0.0907\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.03701\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2205 - logcosh: 0.2205 - val_loss: 0.0820 - val_logcosh: 0.0820\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.03701\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2161 - logcosh: 0.2161 - val_loss: 0.0969 - val_logcosh: 0.0969\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.03701\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2164 - logcosh: 0.2164 - val_loss: 0.1123 - val_logcosh: 0.1123\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.03701\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2171 - logcosh: 0.2171 - val_loss: 0.1333 - val_logcosh: 0.1333\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.03701\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2127 - logcosh: 0.2127 - val_loss: 0.1439 - val_logcosh: 0.1439\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.03701\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2110 - logcosh: 0.2110 - val_loss: 0.1725 - val_logcosh: 0.1725\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.03701\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2185 - logcosh: 0.2185 - val_loss: 0.1707 - val_logcosh: 0.1707\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.03701\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2119 - logcosh: 0.2119 - val_loss: 0.1611 - val_logcosh: 0.1611\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.03701\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2161 - logcosh: 0.2161 - val_loss: 0.1349 - val_logcosh: 0.1349\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.03701\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2085 - logcosh: 0.2085 - val_loss: 0.1129 - val_logcosh: 0.1129\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.03701\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2235 - logcosh: 0.2235 - val_loss: 0.1201 - val_logcosh: 0.1201\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.03701\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.2158 - logcosh: 0.2158 - val_loss: 0.1399 - val_logcosh: 0.1399\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.03701\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2211 - logcosh: 0.2211 - val_loss: 0.1781 - val_logcosh: 0.1781\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.03701\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2172 - logcosh: 0.2172 - val_loss: 0.1940 - val_logcosh: 0.1940\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.03701\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2220 - logcosh: 0.2220 - val_loss: 0.1597 - val_logcosh: 0.1597\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.03701\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2359 - logcosh: 0.2359 - val_loss: 0.1022 - val_logcosh: 0.1022\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.03701\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2079 - logcosh: 0.2079 - val_loss: 0.0751 - val_logcosh: 0.0751\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.03701\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2146 - logcosh: 0.2146 - val_loss: 0.0630 - val_logcosh: 0.0630\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.03701\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2171 - logcosh: 0.2171 - val_loss: 0.0596 - val_logcosh: 0.0596\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.03701\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2188 - logcosh: 0.2188 - val_loss: 0.0623 - val_logcosh: 0.0623\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.03701\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2114 - logcosh: 0.2114 - val_loss: 0.0700 - val_logcosh: 0.0700\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.03701\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2143 - logcosh: 0.2143 - val_loss: 0.0893 - val_logcosh: 0.0893\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.03701\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2195 - logcosh: 0.2195 - val_loss: 0.0975 - val_logcosh: 0.0975\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.03701\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2110 - logcosh: 0.2110 - val_loss: 0.1047 - val_logcosh: 0.1047\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.03701\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2219 - logcosh: 0.2219 - val_loss: 0.1154 - val_logcosh: 0.1154\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.03701\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2134 - logcosh: 0.2134 - val_loss: 0.1124 - val_logcosh: 0.1124\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.03701\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2208 - logcosh: 0.2208 - val_loss: 0.1041 - val_logcosh: 0.1041\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.03701\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2172 - logcosh: 0.2172 - val_loss: 0.0999 - val_logcosh: 0.0999\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.03701\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2162 - logcosh: 0.2162 - val_loss: 0.0887 - val_logcosh: 0.0887\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.03701\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2140 - logcosh: 0.2140 - val_loss: 0.0978 - val_logcosh: 0.0978\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.03701\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2146 - logcosh: 0.2146 - val_loss: 0.1051 - val_logcosh: 0.1051\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.03701\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2167 - logcosh: 0.2167 - val_loss: 0.1106 - val_logcosh: 0.1106\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.03701\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2157 - logcosh: 0.2157 - val_loss: 0.1143 - val_logcosh: 0.1143\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.03701\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2282 - logcosh: 0.2282 - val_loss: 0.1657 - val_logcosh: 0.1657\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.03701\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2282 - logcosh: 0.2282 - val_loss: 0.1684 - val_logcosh: 0.1684\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.03701\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2147 - logcosh: 0.2147 - val_loss: 0.1934 - val_logcosh: 0.1934\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.03701\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2206 - logcosh: 0.2206 - val_loss: 0.1910 - val_logcosh: 0.1910\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.03701\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2163 - logcosh: 0.2163 - val_loss: 0.1692 - val_logcosh: 0.1692\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.03701\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2111 - logcosh: 0.2111 - val_loss: 0.1891 - val_logcosh: 0.1891\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.03701\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2139 - logcosh: 0.2139 - val_loss: 0.1984 - val_logcosh: 0.1984\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.03701\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2215 - logcosh: 0.2215 - val_loss: 0.2235 - val_logcosh: 0.2235\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.03701\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2118 - logcosh: 0.2118 - val_loss: 0.2830 - val_logcosh: 0.2830\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.03701\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2175 - logcosh: 0.2175 - val_loss: 0.2692 - val_logcosh: 0.2692\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.03701\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2137 - logcosh: 0.2137 - val_loss: 0.2099 - val_logcosh: 0.2099\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.03701\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2182 - logcosh: 0.2182 - val_loss: 0.1376 - val_logcosh: 0.1376\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.03701\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2226 - logcosh: 0.2226 - val_loss: 0.0843 - val_logcosh: 0.0843\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.03701\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2221 - logcosh: 0.2221 - val_loss: 0.0774 - val_logcosh: 0.0774\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.03701\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2182 - logcosh: 0.2182 - val_loss: 0.0767 - val_logcosh: 0.0767\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.03701\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2392 - logcosh: 0.2392 - val_loss: 0.1040 - val_logcosh: 0.1040\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.03701\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2263 - logcosh: 0.2263 - val_loss: 0.1758 - val_logcosh: 0.1758\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.03701\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2358 - logcosh: 0.2358 - val_loss: 0.2624 - val_logcosh: 0.2624\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.03701\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2302 - logcosh: 0.2302 - val_loss: 0.2407 - val_logcosh: 0.2407\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.03701\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.2585 - logcosh: 0.2585 - val_loss: 0.1886 - val_logcosh: 0.1886\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.03701\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.2303 - logcosh: 0.2303 - val_loss: 0.1835 - val_logcosh: 0.1835\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.03701\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2412 - logcosh: 0.2412 - val_loss: 0.1160 - val_logcosh: 0.1160\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.03701\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2373 - logcosh: 0.2373 - val_loss: 0.0550 - val_logcosh: 0.0550\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.03701\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2345 - logcosh: 0.2345 - val_loss: 0.0384 - val_logcosh: 0.0384\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.03701\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2406 - logcosh: 0.2406 - val_loss: 0.0362 - val_logcosh: 0.0362\n",
            "\n",
            "Epoch 00416: val_loss improved from 0.03701 to 0.03619, saving model to my_keras_model.h5\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2216 - logcosh: 0.2216 - val_loss: 0.0387 - val_logcosh: 0.0387\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.03619\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2194 - logcosh: 0.2194 - val_loss: 0.0464 - val_logcosh: 0.0464\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.03619\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2277 - logcosh: 0.2277 - val_loss: 0.0613 - val_logcosh: 0.0613\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.03619\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2283 - logcosh: 0.2283 - val_loss: 0.1064 - val_logcosh: 0.1064\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.03619\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2425 - logcosh: 0.2425 - val_loss: 0.0787 - val_logcosh: 0.0787\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.03619\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2216 - logcosh: 0.2216 - val_loss: 0.0556 - val_logcosh: 0.0556\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.03619\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2241 - logcosh: 0.2241 - val_loss: 0.0651 - val_logcosh: 0.0651\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.03619\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2181 - logcosh: 0.2181 - val_loss: 0.0907 - val_logcosh: 0.0907\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.03619\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2266 - logcosh: 0.2266 - val_loss: 0.1142 - val_logcosh: 0.1142\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.03619\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2242 - logcosh: 0.2242 - val_loss: 0.1279 - val_logcosh: 0.1279\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.03619\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2246 - logcosh: 0.2246 - val_loss: 0.1378 - val_logcosh: 0.1378\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.03619\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2394 - logcosh: 0.2394 - val_loss: 0.1429 - val_logcosh: 0.1429\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.03619\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2319 - logcosh: 0.2319 - val_loss: 0.1580 - val_logcosh: 0.1580\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.03619\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2411 - logcosh: 0.2411 - val_loss: 0.1566 - val_logcosh: 0.1566\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.03619\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2322 - logcosh: 0.2322 - val_loss: 0.1275 - val_logcosh: 0.1275\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.03619\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2307 - logcosh: 0.2307 - val_loss: 0.1122 - val_logcosh: 0.1122\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.03619\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2268 - logcosh: 0.2268 - val_loss: 0.1060 - val_logcosh: 0.1060\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.03619\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2304 - logcosh: 0.2304 - val_loss: 0.1023 - val_logcosh: 0.1023\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.03619\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2206 - logcosh: 0.2206 - val_loss: 0.1051 - val_logcosh: 0.1051\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.03619\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2160 - logcosh: 0.2160 - val_loss: 0.2065 - val_logcosh: 0.2065\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.03619\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2273 - logcosh: 0.2273 - val_loss: 0.2733 - val_logcosh: 0.2733\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.03619\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2379 - logcosh: 0.2379 - val_loss: 0.2070 - val_logcosh: 0.2070\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.03619\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2300 - logcosh: 0.2300 - val_loss: 0.1561 - val_logcosh: 0.1561\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.03619\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2273 - logcosh: 0.2273 - val_loss: 0.1068 - val_logcosh: 0.1068\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.03619\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2276 - logcosh: 0.2276 - val_loss: 0.1174 - val_logcosh: 0.1174\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.03619\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2264 - logcosh: 0.2264 - val_loss: 0.0966 - val_logcosh: 0.0966\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.03619\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2295 - logcosh: 0.2295 - val_loss: 0.0578 - val_logcosh: 0.0578\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.03619\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2204 - logcosh: 0.2204 - val_loss: 0.0370 - val_logcosh: 0.0370\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.03619\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2298 - logcosh: 0.2298 - val_loss: 0.0294 - val_logcosh: 0.0294\n",
            "\n",
            "Epoch 00445: val_loss improved from 0.03619 to 0.02941, saving model to my_keras_model.h5\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2226 - logcosh: 0.2226 - val_loss: 0.0286 - val_logcosh: 0.0286\n",
            "\n",
            "Epoch 00446: val_loss improved from 0.02941 to 0.02856, saving model to my_keras_model.h5\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2268 - logcosh: 0.2268 - val_loss: 0.0370 - val_logcosh: 0.0370\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.02856\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2172 - logcosh: 0.2172 - val_loss: 0.0449 - val_logcosh: 0.0449\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.02856\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2280 - logcosh: 0.2280 - val_loss: 0.0331 - val_logcosh: 0.0331\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.02856\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2191 - logcosh: 0.2191 - val_loss: 0.0218 - val_logcosh: 0.0218\n",
            "\n",
            "Epoch 00450: val_loss improved from 0.02856 to 0.02178, saving model to my_keras_model.h5\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2153 - logcosh: 0.2153 - val_loss: 0.0241 - val_logcosh: 0.0241\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.02178\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2352 - logcosh: 0.2352 - val_loss: 0.0220 - val_logcosh: 0.0220\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.02178\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2404 - logcosh: 0.2404 - val_loss: 0.0472 - val_logcosh: 0.0472\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.02178\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2268 - logcosh: 0.2268 - val_loss: 0.1853 - val_logcosh: 0.1853\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.02178\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2302 - logcosh: 0.2302 - val_loss: 0.3445 - val_logcosh: 0.3445\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.02178\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2467 - logcosh: 0.2467 - val_loss: 0.3871 - val_logcosh: 0.3871\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.02178\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2324 - logcosh: 0.2324 - val_loss: 0.3228 - val_logcosh: 0.3228\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.02178\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2444 - logcosh: 0.2444 - val_loss: 0.1987 - val_logcosh: 0.1987\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.02178\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2150 - logcosh: 0.2150 - val_loss: 0.1221 - val_logcosh: 0.1221\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.02178\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2279 - logcosh: 0.2279 - val_loss: 0.1023 - val_logcosh: 0.1023\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.02178\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2246 - logcosh: 0.2246 - val_loss: 0.1124 - val_logcosh: 0.1124\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.02178\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2166 - logcosh: 0.2166 - val_loss: 0.1692 - val_logcosh: 0.1692\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.02178\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2124 - logcosh: 0.2124 - val_loss: 0.2139 - val_logcosh: 0.2139\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.02178\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2133 - logcosh: 0.2133 - val_loss: 0.1977 - val_logcosh: 0.1977\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.02178\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2191 - logcosh: 0.2191 - val_loss: 0.1483 - val_logcosh: 0.1483\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.02178\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2229 - logcosh: 0.2229 - val_loss: 0.1464 - val_logcosh: 0.1464\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.02178\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2253 - logcosh: 0.2253 - val_loss: 0.1005 - val_logcosh: 0.1005\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.02178\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2327 - logcosh: 0.2327 - val_loss: 0.0782 - val_logcosh: 0.0782\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.02178\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2138 - logcosh: 0.2138 - val_loss: 0.0616 - val_logcosh: 0.0616\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.02178\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2180 - logcosh: 0.2180 - val_loss: 0.0521 - val_logcosh: 0.0521\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.02178\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2132 - logcosh: 0.2132 - val_loss: 0.0472 - val_logcosh: 0.0472\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.02178\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2226 - logcosh: 0.2226 - val_loss: 0.0574 - val_logcosh: 0.0574\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.02178\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2122 - logcosh: 0.2122 - val_loss: 0.0623 - val_logcosh: 0.0623\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.02178\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.2216 - logcosh: 0.2216 - val_loss: 0.0761 - val_logcosh: 0.0761\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.02178\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2162 - logcosh: 0.2162 - val_loss: 0.0982 - val_logcosh: 0.0982\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.02178\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2219 - logcosh: 0.2219 - val_loss: 0.1005 - val_logcosh: 0.1005\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.02178\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2196 - logcosh: 0.2196 - val_loss: 0.1224 - val_logcosh: 0.1224\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.02178\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2213 - logcosh: 0.2213 - val_loss: 0.0978 - val_logcosh: 0.0978\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.02178\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2169 - logcosh: 0.2169 - val_loss: 0.0763 - val_logcosh: 0.0763\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.02178\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2198 - logcosh: 0.2198 - val_loss: 0.0595 - val_logcosh: 0.0595\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.02178\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2146 - logcosh: 0.2146 - val_loss: 0.0486 - val_logcosh: 0.0486\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.02178\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2142 - logcosh: 0.2142 - val_loss: 0.0440 - val_logcosh: 0.0440\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.02178\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2146 - logcosh: 0.2146 - val_loss: 0.0410 - val_logcosh: 0.0410\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.02178\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2157 - logcosh: 0.2157 - val_loss: 0.0474 - val_logcosh: 0.0474\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.02178\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2155 - logcosh: 0.2155 - val_loss: 0.1020 - val_logcosh: 0.1020\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.02178\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2130 - logcosh: 0.2130 - val_loss: 0.1362 - val_logcosh: 0.1362\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.02178\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2126 - logcosh: 0.2126 - val_loss: 0.1434 - val_logcosh: 0.1434\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.02178\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2139 - logcosh: 0.2139 - val_loss: 0.1240 - val_logcosh: 0.1240\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.02178\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2137 - logcosh: 0.2137 - val_loss: 0.0881 - val_logcosh: 0.0881\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.02178\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2136 - logcosh: 0.2136 - val_loss: 0.0580 - val_logcosh: 0.0580\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.02178\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2118 - logcosh: 0.2118 - val_loss: 0.0484 - val_logcosh: 0.0484\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.02178\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2106 - logcosh: 0.2106 - val_loss: 0.0505 - val_logcosh: 0.0505\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.02178\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2084 - logcosh: 0.2084 - val_loss: 0.0586 - val_logcosh: 0.0586\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.02178\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2291 - logcosh: 0.2291 - val_loss: 0.0887 - val_logcosh: 0.0887\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.02178\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2274 - logcosh: 0.2274 - val_loss: 0.1592 - val_logcosh: 0.1592\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.02178\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2254 - logcosh: 0.2254 - val_loss: 0.1674 - val_logcosh: 0.1674\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.02178\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2135 - logcosh: 0.2135 - val_loss: 0.1539 - val_logcosh: 0.1539\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.02178\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2217 - logcosh: 0.2217 - val_loss: 0.1407 - val_logcosh: 0.1407\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.02178\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2209 - logcosh: 0.2209 - val_loss: 0.1367 - val_logcosh: 0.1367\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.02178\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2216 - logcosh: 0.2216 - val_loss: 0.1391 - val_logcosh: 0.1391\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.02178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BWZmTdbPV0_"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/my_keras_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Psf8AJkFIU3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "203185b8-1517-45c1-94a9-ef5d2a83da9a"
      },
      "source": [
        "predict_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "predict_df_inverse['target_price']=np.array(model.predict(test_df_x))\n",
        "\n",
        "actual_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "actual_df_inverse['target_price']=np.array(test_df_y)\n",
        "\n",
        "predicted_value_lstm=scaler.inverse_transform(predict_df_inverse)[:,-1]\n",
        "actual_value=scaler.inverse_transform(actual_df_inverse)[:,-1]\n",
        "plt.plot(predicted_value_lstm , color=\"red\")\n",
        "plt.plot(actual_value, color=\"green\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f66e877e4d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTQOl33b6n6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618b435f-a3b2-4349-8047-51f1e42862e9"
      },
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "# NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(128,input_shape = (train_df_x.shape[1:]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(128,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Flatten()) \n",
        "\n",
        "# model.add(Dense(32, activation=\"swish\"))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation=\"relu\"))\n",
        "\n",
        "opt=tf.keras.optimizers.Adam()\n",
        "model.compile(loss='huber_loss',\n",
        "              optimizer=opt,\n",
        "              # metrics=[tf.keras.metrics.huber()]\n",
        "              )\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=f'logs/{Company_symbol}')\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True,monitor='val_loss', verbose=1,)\n",
        "\n",
        "history = model.fit(\n",
        "    train_df_x, train_df_y,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data= (test_df_x,test_df_y),\n",
        "    callbacks = [tensorboard,checkpoint_cb]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 6s 850ms/step - loss: 0.5622 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92287, saving model to my_keras_model.h5\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.4877 - val_loss: 0.9216\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92287 to 0.92162, saving model to my_keras_model.h5\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.4728 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.92162\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.5880 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.92162\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4930 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.92162\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4665 - val_loss: 0.9205\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.92162 to 0.92050, saving model to my_keras_model.h5\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.5108 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.92050\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4292 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.92050\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3961 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.92050\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3589 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.92050\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3418 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.92050\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3975 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.92050\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3649 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.92050\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3799 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.92050\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3358 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.92050\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3671 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.92050\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3686 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.92050\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3689 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.92050\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3178 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.92050\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3485 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.92050\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3134 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.92050\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3217 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.92050\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3060 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.92050\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3166 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.92050\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3364 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.92050\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2869 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.92050\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3040 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.92050\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3032 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.92050\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3187 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.92050\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2916 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.92050\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3048 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.92050\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3141 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.92050\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3009 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.92050\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3075 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.92050\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3015 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.92050\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3220 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.92050\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2792 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.92050\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2890 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.92050\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2784 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.92050\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2819 - val_loss: 0.8641\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.92050 to 0.86412, saving model to my_keras_model.h5\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3434 - val_loss: 0.7877\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.86412 to 0.78767, saving model to my_keras_model.h5\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3156 - val_loss: 0.7902\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.78767\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3077 - val_loss: 0.8709\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.78767\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3084 - val_loss: 0.9134\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.78767\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2947 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.78767\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2937 - val_loss: 0.9229\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.78767\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2997 - val_loss: 0.9214\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.78767\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2805 - val_loss: 0.8969\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.78767\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3161 - val_loss: 0.8847\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.78767\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3104 - val_loss: 0.8565\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.78767\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3032 - val_loss: 0.7935\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.78767\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3040 - val_loss: 0.7497\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.78767 to 0.74974, saving model to my_keras_model.h5\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2845 - val_loss: 0.6723\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.74974 to 0.67226, saving model to my_keras_model.h5\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2933 - val_loss: 0.6212\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.67226 to 0.62119, saving model to my_keras_model.h5\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2997 - val_loss: 0.6366\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.62119\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2965 - val_loss: 0.7026\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.62119\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3092 - val_loss: 0.6269\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.62119\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2960 - val_loss: 0.4504\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.62119 to 0.45041, saving model to my_keras_model.h5\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2764 - val_loss: 0.3226\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.45041 to 0.32262, saving model to my_keras_model.h5\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2958 - val_loss: 0.2383\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.32262 to 0.23826, saving model to my_keras_model.h5\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3144 - val_loss: 0.1796\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.23826 to 0.17962, saving model to my_keras_model.h5\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2991 - val_loss: 0.1399\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.17962 to 0.13992, saving model to my_keras_model.h5\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3069 - val_loss: 0.1546\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.13992\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3058 - val_loss: 0.2160\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.13992\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3137 - val_loss: 0.2930\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.13992\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2929 - val_loss: 0.3113\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.13992\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2909 - val_loss: 0.1605\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.13992\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2912 - val_loss: 0.1034\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.13992 to 0.10340, saving model to my_keras_model.h5\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3126 - val_loss: 0.1957\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.10340\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2879 - val_loss: 0.3005\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.10340\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3009 - val_loss: 0.3967\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.10340\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2967 - val_loss: 0.4271\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.10340\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2668 - val_loss: 0.4033\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.10340\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2937 - val_loss: 0.2869\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.10340\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2777 - val_loss: 0.1264\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.10340\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3140 - val_loss: 0.0676\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.10340 to 0.06758, saving model to my_keras_model.h5\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2944 - val_loss: 0.0498\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.06758 to 0.04979, saving model to my_keras_model.h5\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2936 - val_loss: 0.0462\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.04979 to 0.04619, saving model to my_keras_model.h5\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2938 - val_loss: 0.0519\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.04619\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2885 - val_loss: 0.1324\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.04619\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2948 - val_loss: 0.2345\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.04619\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2923 - val_loss: 0.2117\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.04619\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2994 - val_loss: 0.2076\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.04619\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2824 - val_loss: 0.1709\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.04619\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2823 - val_loss: 0.0769\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.04619\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2680 - val_loss: 0.0637\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.04619\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2875 - val_loss: 0.0696\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.04619\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2913 - val_loss: 0.0633\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.04619\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2879 - val_loss: 0.0579\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.04619\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2992 - val_loss: 0.0761\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.04619\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3038 - val_loss: 0.1253\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.04619\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2967 - val_loss: 0.2050\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.04619\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2976 - val_loss: 0.3129\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.04619\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3087 - val_loss: 0.2701\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.04619\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2757 - val_loss: 0.1464\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.04619\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2802 - val_loss: 0.0779\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.04619\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3148 - val_loss: 0.0296\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.04619 to 0.02958, saving model to my_keras_model.h5\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3085 - val_loss: 0.0695\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.02958\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3505 - val_loss: 0.0867\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.02958\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3742 - val_loss: 0.0206\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.02958 to 0.02057, saving model to my_keras_model.h5\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3241 - val_loss: 0.0637\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.02057\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3001 - val_loss: 0.1084\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.02057\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2886 - val_loss: 0.1979\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.02057\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2985 - val_loss: 0.2923\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.02057\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2727 - val_loss: 0.3111\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.02057\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2919 - val_loss: 0.2952\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.02057\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2858 - val_loss: 0.2508\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.02057\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3029 - val_loss: 0.3221\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.02057\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2779 - val_loss: 0.3886\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.02057\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3078 - val_loss: 0.5004\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.02057\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3123 - val_loss: 0.6225\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.02057\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2765 - val_loss: 0.5349\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.02057\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2838 - val_loss: 0.3801\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.02057\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2896 - val_loss: 0.1820\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.02057\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3012 - val_loss: 0.0698\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.02057\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3241 - val_loss: 0.1004\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.02057\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2831 - val_loss: 0.1594\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.02057\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2812 - val_loss: 0.2743\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.02057\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2856 - val_loss: 0.3237\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.02057\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2998 - val_loss: 0.3126\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.02057\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3075 - val_loss: 0.2920\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.02057\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.3012 - val_loss: 0.1448\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.02057\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2926 - val_loss: 0.0419\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.02057\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3563 - val_loss: 0.0714\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.02057\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3145 - val_loss: 0.3488\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.02057\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3100 - val_loss: 0.6161\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.02057\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3073 - val_loss: 0.7523\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.02057\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3126 - val_loss: 0.7422\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.02057\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2986 - val_loss: 0.4642\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.02057\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2890 - val_loss: 0.0983\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.02057\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2737 - val_loss: 0.0288\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.02057\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2936 - val_loss: 0.0276\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.02057\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2692 - val_loss: 0.0390\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.02057\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2762 - val_loss: 0.0744\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.02057\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2648 - val_loss: 0.1405\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.02057\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2901 - val_loss: 0.1892\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.02057\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2703 - val_loss: 0.1763\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.02057\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2975 - val_loss: 0.3088\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.02057\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3014 - val_loss: 0.5747\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.02057\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2753 - val_loss: 0.6119\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.02057\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2922 - val_loss: 0.5180\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.02057\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.3073 - val_loss: 0.3689\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.02057\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3103 - val_loss: 0.2545\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.02057\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2721 - val_loss: 0.1267\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.02057\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2875 - val_loss: 0.0434\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.02057\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2881 - val_loss: 0.0281\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.02057\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2783 - val_loss: 0.0470\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.02057\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2915 - val_loss: 0.0617\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.02057\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2877 - val_loss: 0.0697\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.02057\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2619 - val_loss: 0.1764\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.02057\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2730 - val_loss: 0.2037\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.02057\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3043 - val_loss: 0.1525\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.02057\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2774 - val_loss: 0.1927\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.02057\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2833 - val_loss: 0.1887\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.02057\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3163 - val_loss: 0.0859\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.02057\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2797 - val_loss: 0.0304\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.02057\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2685 - val_loss: 0.0311\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.02057\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2774 - val_loss: 0.0256\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.02057\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2751 - val_loss: 0.0197\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.02057 to 0.01969, saving model to my_keras_model.h5\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3004 - val_loss: 0.0333\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.01969\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2739 - val_loss: 0.0574\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.01969\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2811 - val_loss: 0.0632\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.01969\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2717 - val_loss: 0.0580\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.01969\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2829 - val_loss: 0.0474\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.01969\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2772 - val_loss: 0.0294\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.01969\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2694 - val_loss: 0.0404\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.01969\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2817 - val_loss: 0.0506\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.01969\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2629 - val_loss: 0.0342\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.01969\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2616 - val_loss: 0.0322\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.01969\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2803 - val_loss: 0.0417\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.01969\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2964 - val_loss: 0.0530\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.01969\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2870 - val_loss: 0.0552\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.01969\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2859 - val_loss: 0.0412\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.01969\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3028 - val_loss: 0.0329\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.01969\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2683 - val_loss: 0.0378\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.01969\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2822 - val_loss: 0.0804\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.01969\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2897 - val_loss: 0.0803\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.01969\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2947 - val_loss: 0.0659\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.01969\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2802 - val_loss: 0.0199\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.01969\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2805 - val_loss: 0.0230\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.01969\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2962 - val_loss: 0.0160\n",
            "\n",
            "Epoch 00181: val_loss improved from 0.01969 to 0.01596, saving model to my_keras_model.h5\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3144 - val_loss: 0.0195\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.01596\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2999 - val_loss: 0.0709\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.01596\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2844 - val_loss: 0.1308\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.01596\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2977 - val_loss: 0.2068\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.01596\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2879 - val_loss: 0.2550\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.01596\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2802 - val_loss: 0.2487\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.01596\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2775 - val_loss: 0.2236\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.01596\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2782 - val_loss: 0.2040\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.01596\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2749 - val_loss: 0.2713\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.01596\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2905 - val_loss: 0.4463\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.01596\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2866 - val_loss: 0.3859\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.01596\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2880 - val_loss: 0.2891\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.01596\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2609 - val_loss: 0.2475\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.01596\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2710 - val_loss: 0.2672\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.01596\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2603 - val_loss: 0.2994\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.01596\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2675 - val_loss: 0.3490\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.01596\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2797 - val_loss: 0.1718\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.01596\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2883 - val_loss: 0.0256\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.01596\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3088 - val_loss: 0.0140\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.01596 to 0.01399, saving model to my_keras_model.h5\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2995 - val_loss: 0.0272\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.01399\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2927 - val_loss: 0.0918\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.01399\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3070 - val_loss: 0.1683\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.01399\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2725 - val_loss: 0.1541\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.01399\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2910 - val_loss: 0.1424\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.01399\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2788 - val_loss: 0.1593\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.01399\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2924 - val_loss: 0.1837\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.01399\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3049 - val_loss: 0.1902\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.01399\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2934 - val_loss: 0.1264\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.01399\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2734 - val_loss: 0.0931\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.01399\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2798 - val_loss: 0.0892\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.01399\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2997 - val_loss: 0.1244\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.01399\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2760 - val_loss: 0.2140\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.01399\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2674 - val_loss: 0.3238\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.01399\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2780 - val_loss: 0.3607\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.01399\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2975 - val_loss: 0.2829\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.01399\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2890 - val_loss: 0.2346\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.01399\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2743 - val_loss: 0.3339\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.01399\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2743 - val_loss: 0.2239\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.01399\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2819 - val_loss: 0.0891\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.01399\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2634 - val_loss: 0.0258\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.01399\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2774 - val_loss: 0.0243\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.01399\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3025 - val_loss: 0.0360\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.01399\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2901 - val_loss: 0.0873\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.01399\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2693 - val_loss: 0.1928\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.01399\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2727 - val_loss: 0.2583\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.01399\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2768 - val_loss: 0.2650\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.01399\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2832 - val_loss: 0.2225\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.01399\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2731 - val_loss: 0.1667\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.01399\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2892 - val_loss: 0.1050\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.01399\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2674 - val_loss: 0.0632\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.01399\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2989 - val_loss: 0.0740\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.01399\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2840 - val_loss: 0.1233\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.01399\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2933 - val_loss: 0.2729\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.01399\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2783 - val_loss: 0.3051\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.01399\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2868 - val_loss: 0.3403\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.01399\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2637 - val_loss: 0.3012\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.01399\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2838 - val_loss: 0.2677\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.01399\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2785 - val_loss: 0.2403\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.01399\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2893 - val_loss: 0.2963\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.01399\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2696 - val_loss: 0.4088\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.01399\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2986 - val_loss: 0.3543\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.01399\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2788 - val_loss: 0.2693\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.01399\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2927 - val_loss: 0.3210\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.01399\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2570 - val_loss: 0.3111\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.01399\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.3029 - val_loss: 0.2638\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.01399\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2983 - val_loss: 0.2019\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.01399\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.3007 - val_loss: 0.1492\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.01399\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2704 - val_loss: 0.1170\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.01399\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3161 - val_loss: 0.0928\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.01399\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2820 - val_loss: 0.0871\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.01399\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2932 - val_loss: 0.0902\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.01399\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2734 - val_loss: 0.0950\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.01399\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2862 - val_loss: 0.1251\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.01399\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2805 - val_loss: 0.1480\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.01399\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2771 - val_loss: 0.1722\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.01399\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3005 - val_loss: 0.1589\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.01399\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2557 - val_loss: 0.1274\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.01399\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2619 - val_loss: 0.1056\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.01399\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2823 - val_loss: 0.1702\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.01399\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2770 - val_loss: 0.1651\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.01399\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2759 - val_loss: 0.1911\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.01399\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2714 - val_loss: 0.2488\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.01399\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2896 - val_loss: 0.3458\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.01399\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2618 - val_loss: 0.3721\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.01399\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2734 - val_loss: 0.3975\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.01399\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2738 - val_loss: 0.3131\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.01399\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2830 - val_loss: 0.1443\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.01399\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2814 - val_loss: 0.1024\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.01399\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2820 - val_loss: 0.1074\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.01399\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2877 - val_loss: 0.1305\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.01399\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2897 - val_loss: 0.2426\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.01399\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2738 - val_loss: 0.3416\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.01399\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2695 - val_loss: 0.3981\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.01399\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2745 - val_loss: 0.3459\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.01399\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2706 - val_loss: 0.2557\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.01399\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2844 - val_loss: 0.1357\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.01399\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2677 - val_loss: 0.0845\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.01399\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2582 - val_loss: 0.0788\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.01399\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2844 - val_loss: 0.1091\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.01399\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2756 - val_loss: 0.1549\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.01399\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2765 - val_loss: 0.1705\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.01399\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2703 - val_loss: 0.2366\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.01399\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2657 - val_loss: 0.2969\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.01399\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2816 - val_loss: 0.3608\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.01399\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2595 - val_loss: 0.3391\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.01399\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2705 - val_loss: 0.2387\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.01399\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2606 - val_loss: 0.1450\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.01399\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2751 - val_loss: 0.1758\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.01399\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2721 - val_loss: 0.1934\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.01399\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2590 - val_loss: 0.2417\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.01399\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2529 - val_loss: 0.3094\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.01399\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2605 - val_loss: 0.3365\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.01399\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2800 - val_loss: 0.2798\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.01399\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2789 - val_loss: 0.1817\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.01399\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2602 - val_loss: 0.1203\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.01399\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2558 - val_loss: 0.1480\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.01399\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2646 - val_loss: 0.1684\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.01399\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2599 - val_loss: 0.1434\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.01399\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2447 - val_loss: 0.1045\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.01399\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2572 - val_loss: 0.1257\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.01399\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2572 - val_loss: 0.1505\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.01399\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2552 - val_loss: 0.1568\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.01399\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2640 - val_loss: 0.0474\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.01399\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2609 - val_loss: 0.0669\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.01399\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2986 - val_loss: 0.1262\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.01399\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3031 - val_loss: 0.0657\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.01399\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.3068 - val_loss: 0.0206\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.01399\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2743 - val_loss: 0.0419\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.01399\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.2653 - val_loss: 0.1213\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.01399\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2803 - val_loss: 0.3104\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.01399\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2963 - val_loss: 0.5328\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.01399\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2779 - val_loss: 0.5015\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.01399\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2982 - val_loss: 0.2008\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.01399\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2829 - val_loss: 0.0613\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.01399\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2584 - val_loss: 0.0353\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.01399\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2584 - val_loss: 0.0265\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.01399\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2778 - val_loss: 0.0457\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.01399\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2843 - val_loss: 0.0690\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.01399\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2733 - val_loss: 0.0973\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.01399\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2654 - val_loss: 0.1663\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.01399\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2833 - val_loss: 0.2646\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.01399\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2899 - val_loss: 0.3958\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.01399\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2634 - val_loss: 0.5045\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.01399\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2566 - val_loss: 0.2473\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.01399\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2475 - val_loss: 0.1137\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.01399\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2734 - val_loss: 0.0742\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.01399\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2577 - val_loss: 0.0594\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.01399\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2736 - val_loss: 0.0721\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.01399\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2693 - val_loss: 0.1080\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.01399\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2605 - val_loss: 0.1868\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.01399\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2598 - val_loss: 0.4103\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.01399\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2835 - val_loss: 0.6932\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.01399\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3064 - val_loss: 0.4605\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.01399\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2875 - val_loss: 0.2121\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.01399\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2590 - val_loss: 0.2605\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.01399\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2658 - val_loss: 0.5266\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.01399\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2978 - val_loss: 0.8357\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.01399\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.2947 - val_loss: 0.8584\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.01399\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2944 - val_loss: 0.5542\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.01399\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2762 - val_loss: 0.4779\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.01399\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2851 - val_loss: 0.5228\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.01399\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2882 - val_loss: 0.6004\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.01399\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2870 - val_loss: 0.6699\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.01399\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2788 - val_loss: 0.5714\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.01399\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2810 - val_loss: 0.4224\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.01399\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2742 - val_loss: 0.2927\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.01399\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2850 - val_loss: 0.2035\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.01399\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2825 - val_loss: 0.2120\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.01399\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2845 - val_loss: 0.3823\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.01399\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2594 - val_loss: 0.6852\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.01399\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2770 - val_loss: 0.7723\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.01399\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2838 - val_loss: 0.7365\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.01399\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3013 - val_loss: 0.4549\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.01399\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2703 - val_loss: 0.2022\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.01399\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2723 - val_loss: 0.1322\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.01399\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2870 - val_loss: 0.1870\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.01399\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2835 - val_loss: 0.2829\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.01399\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3042 - val_loss: 0.5872\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.01399\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3019 - val_loss: 0.6454\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.01399\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2950 - val_loss: 0.4384\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.01399\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2874 - val_loss: 0.1964\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.01399\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2812 - val_loss: 0.0957\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.01399\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2984 - val_loss: 0.1078\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.01399\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2796 - val_loss: 0.2490\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.01399\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2914 - val_loss: 0.5075\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.01399\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3119 - val_loss: 0.8488\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.01399\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3330 - val_loss: 0.9279\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.01399\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3271 - val_loss: 0.5602\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.01399\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2722 - val_loss: 0.1988\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.01399\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2725 - val_loss: 0.1055\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.01399\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2931 - val_loss: 0.1199\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.01399\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2870 - val_loss: 0.1628\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.01399\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2910 - val_loss: 0.2102\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.01399\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2905 - val_loss: 0.2321\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.01399\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2698 - val_loss: 0.2997\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.01399\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2727 - val_loss: 0.4551\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.01399\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2820 - val_loss: 0.7665\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.01399\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2859 - val_loss: 0.7475\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.01399\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2864 - val_loss: 0.4971\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.01399\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2852 - val_loss: 0.2979\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.01399\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2851 - val_loss: 0.2224\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.01399\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2776 - val_loss: 0.2318\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.01399\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2658 - val_loss: 0.3186\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.01399\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2705 - val_loss: 0.3658\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.01399\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2698 - val_loss: 0.3980\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.01399\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2726 - val_loss: 0.4885\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.01399\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2720 - val_loss: 0.4832\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.01399\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2929 - val_loss: 0.4057\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.01399\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2682 - val_loss: 0.2734\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.01399\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2784 - val_loss: 0.2018\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.01399\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2785 - val_loss: 0.1650\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.01399\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2630 - val_loss: 0.1598\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.01399\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2709 - val_loss: 0.2268\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.01399\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2666 - val_loss: 0.3182\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.01399\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2618 - val_loss: 0.4089\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.01399\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2701 - val_loss: 0.4651\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.01399\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2718 - val_loss: 0.4375\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.01399\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2732 - val_loss: 0.3412\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.01399\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2674 - val_loss: 0.1690\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.01399\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2722 - val_loss: 0.1214\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.01399\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2756 - val_loss: 0.1064\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.01399\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2853 - val_loss: 0.1136\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.01399\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2886 - val_loss: 0.1281\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.01399\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2594 - val_loss: 0.1715\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.01399\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2703 - val_loss: 0.2663\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.01399\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2695 - val_loss: 0.3091\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.01399\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2611 - val_loss: 0.3478\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.01399\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2668 - val_loss: 0.4094\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.01399\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2557 - val_loss: 0.2488\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.01399\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2858 - val_loss: 0.0788\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.01399\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2789 - val_loss: 0.0368\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.01399\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2978 - val_loss: 0.0562\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.01399\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2745 - val_loss: 0.1843\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.01399\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2680 - val_loss: 0.3411\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.01399\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2781 - val_loss: 0.5278\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.01399\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2733 - val_loss: 0.8557\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.01399\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2752 - val_loss: 1.0625\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.01399\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2763 - val_loss: 1.3023\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.01399\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2860 - val_loss: 1.3094\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.01399\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.2719 - val_loss: 1.2344\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.01399\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2855 - val_loss: 1.0724\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.01399\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2945 - val_loss: 0.8858\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.01399\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2856 - val_loss: 0.7160\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.01399\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2885 - val_loss: 0.6273\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.01399\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2887 - val_loss: 0.5888\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.01399\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2702 - val_loss: 0.6407\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.01399\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2651 - val_loss: 0.7328\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.01399\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2665 - val_loss: 0.7808\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.01399\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2649 - val_loss: 0.7464\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.01399\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2710 - val_loss: 0.7316\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.01399\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2794 - val_loss: 0.7803\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.01399\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2715 - val_loss: 0.7822\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.01399\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2612 - val_loss: 0.7136\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.01399\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2817 - val_loss: 0.6863\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.01399\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2587 - val_loss: 0.6271\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.01399\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2703 - val_loss: 0.5256\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.01399\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2702 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.01399\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2736 - val_loss: 0.7001\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.01399\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3092 - val_loss: 0.6741\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.01399\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3071 - val_loss: 0.6508\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.01399\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2733 - val_loss: 0.6388\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.01399\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2897 - val_loss: 0.6658\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.01399\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2718 - val_loss: 0.6809\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.01399\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2796 - val_loss: 0.6674\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.01399\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2830 - val_loss: 0.6553\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.01399\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2923 - val_loss: 0.6481\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.01399\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2975 - val_loss: 0.6364\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.01399\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2871 - val_loss: 0.6478\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.01399\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2984 - val_loss: 0.6615\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.01399\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2935 - val_loss: 0.6505\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.01399\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2871 - val_loss: 0.6371\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.01399\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.2685 - val_loss: 0.6379\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.01399\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2865 - val_loss: 0.6471\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.01399\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3192 - val_loss: 0.6521\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.01399\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2939 - val_loss: 0.6611\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.01399\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2833 - val_loss: 0.6624\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.01399\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2824 - val_loss: 0.6613\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.01399\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2975 - val_loss: 0.6553\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.01399\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2869 - val_loss: 0.6378\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.01399\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3415 - val_loss: 0.6359\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.01399\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.2719 - val_loss: 0.6372\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.01399\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3408 - val_loss: 0.6441\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.01399\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2748 - val_loss: 0.6755\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.01399\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3020 - val_loss: 0.7092\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.01399\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2923 - val_loss: 0.7381\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.01399\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2714 - val_loss: 0.7494\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.01399\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2714 - val_loss: 0.7674\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.01399\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2995 - val_loss: 0.7716\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.01399\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.3013 - val_loss: 0.7635\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.01399\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2698 - val_loss: 0.7618\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.01399\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2853 - val_loss: 0.7454\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.01399\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3018 - val_loss: 0.7386\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.01399\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2913 - val_loss: 0.7107\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.01399\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2980 - val_loss: 0.7021\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.01399\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2977 - val_loss: 0.7157\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.01399\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2923 - val_loss: 0.7468\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.01399\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2938 - val_loss: 0.7677\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.01399\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2653 - val_loss: 0.7784\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.01399\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2722 - val_loss: 0.7997\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.01399\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2790 - val_loss: 0.7858\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.01399\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2761 - val_loss: 0.7816\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.01399\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.2868 - val_loss: 0.7589\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.01399\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2681 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.01399\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2590 - val_loss: 0.6747\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.01399\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2773 - val_loss: 0.6577\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.01399\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2722 - val_loss: 0.6530\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.01399\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.2959 - val_loss: 0.6532\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.01399\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2878 - val_loss: 0.6700\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.01399\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3060 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.01399\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2835 - val_loss: 0.6838\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.01399\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2801 - val_loss: 0.6709\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.01399\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2832 - val_loss: 0.6485\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.01399\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2591 - val_loss: 0.6429\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.01399\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.2821 - val_loss: 0.6404\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.01399\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2922 - val_loss: 0.6388\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.01399\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2665 - val_loss: 0.6292\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.01399\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.2780 - val_loss: 0.6362\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.01399\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.2779 - val_loss: 0.6360\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.01399\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2759 - val_loss: 0.6370\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.01399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNobzWKgMdZl"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/my_keras_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "MfzxJhOiNOoL",
        "outputId": "6da74a75-9dc0-44bb-9385-ad74fbb2d12b"
      },
      "source": [
        "predict_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "predict_df_inverse['target_price']=np.array(model.predict(test_df_x))\n",
        "\n",
        "actual_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "actual_df_inverse['target_price']=np.array(test_df_y)\n",
        "\n",
        "predicted_value_GRU=scaler.inverse_transform(predict_df_inverse)[:,-1]\n",
        "actual_value=scaler.inverse_transform(actual_df_inverse)[:,-1]\n",
        "plt.plot(predicted_value_GRU , color=\"red\")\n",
        "plt.plot(actual_value, color=\"green\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f66e618d490>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34C1ZvW3NRgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf2cb97-22c4-4ab6-e3d1-66667de495ca"
      },
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "# NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(128,input_shape = (train_df_x.shape[1:]),return_sequences=True, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(128,return_sequences=True,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(128,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Flatten()) \n",
        "\n",
        "# model.add(Dense(32, activation=\"swish\"))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation=\"relu\"))\n",
        "\n",
        "opt=tf.keras.optimizers.Adam()\n",
        "model.compile(loss='huber_loss',\n",
        "              optimizer=opt,\n",
        "              # metrics=[tf.keras.metrics.huber()]\n",
        "              )\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=f'logs/{Company_symbol}')\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True,monitor='val_loss', verbose=1,)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5,min_lr=0.0000001,verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_df_x, train_df_y,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data= (test_df_x,test_df_y),\n",
        "    callbacks = [tensorboard,checkpoint_cb,reduce_lr]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 6s 860ms/step - loss: 0.9752 - val_loss: 1.0149\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.01491, saving model to my_keras_model.h5\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.6326 - val_loss: 1.0148\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.01491 to 1.01485, saving model to my_keras_model.h5\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.5318 - val_loss: 1.0148\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.01485 to 1.01480, saving model to my_keras_model.h5\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.4902 - val_loss: 1.0148\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.01480 to 1.01477, saving model to my_keras_model.h5\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.5118 - val_loss: 1.0148\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.01477 to 1.01476, saving model to my_keras_model.h5\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.4474 - val_loss: 1.0141\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.01476 to 1.01406, saving model to my_keras_model.h5\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.4692 - val_loss: 1.0145\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.01406\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4360 - val_loss: 1.0147\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.01406\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4440 - val_loss: 1.0135\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.01406 to 1.01353, saving model to my_keras_model.h5\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4576 - val_loss: 1.0083\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.01353 to 1.00829, saving model to my_keras_model.h5\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4689 - val_loss: 1.0108\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.00829\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4536 - val_loss: 1.0135\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.00829\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4443 - val_loss: 1.0141\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.00829\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4236 - val_loss: 1.0092\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.00829\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4190 - val_loss: 0.9811\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.00829 to 0.98114, saving model to my_keras_model.h5\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4003 - val_loss: 0.9914\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.98114\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4112 - val_loss: 0.9798\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.98114 to 0.97976, saving model to my_keras_model.h5\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4045 - val_loss: 0.9600\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.97976 to 0.95999, saving model to my_keras_model.h5\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4053 - val_loss: 0.8965\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.95999 to 0.89645, saving model to my_keras_model.h5\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3798 - val_loss: 0.8654\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.89645 to 0.86542, saving model to my_keras_model.h5\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4173 - val_loss: 0.8476\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.86542 to 0.84763, saving model to my_keras_model.h5\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4090 - val_loss: 0.9177\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.84763\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4140 - val_loss: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.84763\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3916 - val_loss: 1.0146\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.84763\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4002 - val_loss: 1.0006\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.84763\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4200 - val_loss: 0.9022\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.84763\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3875 - val_loss: 0.8400\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.84763 to 0.83997, saving model to my_keras_model.h5\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4134 - val_loss: 0.7965\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.83997 to 0.79646, saving model to my_keras_model.h5\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3947 - val_loss: 0.7632\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.79646 to 0.76324, saving model to my_keras_model.h5\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4140 - val_loss: 0.7228\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.76324 to 0.72277, saving model to my_keras_model.h5\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4003 - val_loss: 0.7070\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.72277 to 0.70700, saving model to my_keras_model.h5\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4105 - val_loss: 0.6767\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.70700 to 0.67675, saving model to my_keras_model.h5\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4336 - val_loss: 0.6465\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.67675 to 0.64653, saving model to my_keras_model.h5\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4128 - val_loss: 0.6310\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.64653 to 0.63096, saving model to my_keras_model.h5\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4086 - val_loss: 0.6260\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.63096 to 0.62604, saving model to my_keras_model.h5\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4043 - val_loss: 0.6277\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.62604\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3656 - val_loss: 0.6291\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.62604\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4115 - val_loss: 0.6199\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.62604 to 0.61994, saving model to my_keras_model.h5\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3919 - val_loss: 0.5980\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.61994 to 0.59795, saving model to my_keras_model.h5\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4054 - val_loss: 0.5893\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.59795 to 0.58927, saving model to my_keras_model.h5\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3888 - val_loss: 0.5847\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.58927 to 0.58470, saving model to my_keras_model.h5\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3852 - val_loss: 0.5898\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.58470\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3851 - val_loss: 0.5541\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.58470 to 0.55412, saving model to my_keras_model.h5\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3922 - val_loss: 0.5303\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.55412 to 0.53031, saving model to my_keras_model.h5\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4021 - val_loss: 0.4875\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.53031 to 0.48746, saving model to my_keras_model.h5\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3819 - val_loss: 0.4911\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.48746\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3858 - val_loss: 0.4879\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.48746\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3883 - val_loss: 0.4739\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.48746 to 0.47389, saving model to my_keras_model.h5\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3634 - val_loss: 0.4357\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.47389 to 0.43572, saving model to my_keras_model.h5\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3859 - val_loss: 0.3746\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.43572 to 0.37458, saving model to my_keras_model.h5\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3898 - val_loss: 0.3265\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.37458 to 0.32648, saving model to my_keras_model.h5\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3669 - val_loss: 0.2809\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.32648 to 0.28092, saving model to my_keras_model.h5\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3820 - val_loss: 0.2457\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.28092 to 0.24568, saving model to my_keras_model.h5\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3759 - val_loss: 0.2323\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.24568 to 0.23231, saving model to my_keras_model.h5\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4136 - val_loss: 0.2267\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.23231 to 0.22675, saving model to my_keras_model.h5\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3888 - val_loss: 0.2031\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.22675 to 0.20306, saving model to my_keras_model.h5\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4068 - val_loss: 0.1981\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.20306 to 0.19806, saving model to my_keras_model.h5\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3755 - val_loss: 0.1885\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.19806 to 0.18845, saving model to my_keras_model.h5\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3922 - val_loss: 0.1919\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.18845\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.4103 - val_loss: 0.2012\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.18845\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3800 - val_loss: 0.2257\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.18845\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3806 - val_loss: 0.2599\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.18845\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3984 - val_loss: 0.2722\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.18845\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3842 - val_loss: 0.2681\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.18845\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3672 - val_loss: 0.2589\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.18845\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4037 - val_loss: 0.2505\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.18845\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3927 - val_loss: 0.2401\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.18845\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3841 - val_loss: 0.2287\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.18845\n",
            "\n",
            "Epoch 00068: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3878 - val_loss: 0.2191\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.18845\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3642 - val_loss: 0.2178\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.18845\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3812 - val_loss: 0.2100\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.18845\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3868 - val_loss: 0.1996\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.18845\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3665 - val_loss: 0.1886\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.18845\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3955 - val_loss: 0.1824\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.18845 to 0.18245, saving model to my_keras_model.h5\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4089 - val_loss: 0.1842\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.18245\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3901 - val_loss: 0.1868\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.18245\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3766 - val_loss: 0.1736\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.18245 to 0.17365, saving model to my_keras_model.h5\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3878 - val_loss: 0.1638\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.17365 to 0.16379, saving model to my_keras_model.h5\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3904 - val_loss: 0.1586\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.16379 to 0.15858, saving model to my_keras_model.h5\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4071 - val_loss: 0.1610\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.15858\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3672 - val_loss: 0.1640\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.15858\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3784 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.15858 to 0.15531, saving model to my_keras_model.h5\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3646 - val_loss: 0.1505\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.15531 to 0.15047, saving model to my_keras_model.h5\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3699 - val_loss: 0.1496\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.15047 to 0.14962, saving model to my_keras_model.h5\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3798 - val_loss: 0.1487\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.14962 to 0.14869, saving model to my_keras_model.h5\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3985 - val_loss: 0.1450\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.14869 to 0.14500, saving model to my_keras_model.h5\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3683 - val_loss: 0.1386\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.14500 to 0.13862, saving model to my_keras_model.h5\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3899 - val_loss: 0.1314\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.13862 to 0.13138, saving model to my_keras_model.h5\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3659 - val_loss: 0.1304\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.13138 to 0.13037, saving model to my_keras_model.h5\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3768 - val_loss: 0.1294\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.13037 to 0.12942, saving model to my_keras_model.h5\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3619 - val_loss: 0.1273\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.12942 to 0.12726, saving model to my_keras_model.h5\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3963 - val_loss: 0.1264\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.12726 to 0.12640, saving model to my_keras_model.h5\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3942 - val_loss: 0.1236\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.12640 to 0.12365, saving model to my_keras_model.h5\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.4052 - val_loss: 0.1222\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.12365 to 0.12218, saving model to my_keras_model.h5\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3778 - val_loss: 0.1185\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.12218 to 0.11846, saving model to my_keras_model.h5\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3994 - val_loss: 0.1184\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.11846 to 0.11844, saving model to my_keras_model.h5\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3728 - val_loss: 0.1149\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.11844 to 0.11486, saving model to my_keras_model.h5\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.3825 - val_loss: 0.1133\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.11486 to 0.11326, saving model to my_keras_model.h5\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3726 - val_loss: 0.1137\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.11326\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3883 - val_loss: 0.1114\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.11326 to 0.11143, saving model to my_keras_model.h5\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3832 - val_loss: 0.1101\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.11143 to 0.11011, saving model to my_keras_model.h5\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3705 - val_loss: 0.1104\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.11011\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.3956 - val_loss: 0.1077\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.11011 to 0.10767, saving model to my_keras_model.h5\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3853 - val_loss: 0.1069\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.10767 to 0.10689, saving model to my_keras_model.h5\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3892 - val_loss: 0.1068\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.10689 to 0.10679, saving model to my_keras_model.h5\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3714 - val_loss: 0.1073\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.10679\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3778 - val_loss: 0.1079\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.10679\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3899 - val_loss: 0.1079\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.10679\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3789 - val_loss: 0.1090\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.10679\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3922 - val_loss: 0.1114\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.10679\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3917 - val_loss: 0.1139\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.10679\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3764 - val_loss: 0.1150\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.10679\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4068 - val_loss: 0.1138\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.10679\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4036 - val_loss: 0.1145\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.10679\n",
            "\n",
            "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3932 - val_loss: 0.1167\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.10679\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.4066 - val_loss: 0.1126\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.10679\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3834 - val_loss: 0.1127\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.10679\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3780 - val_loss: 0.1150\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.10679\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3657 - val_loss: 0.1177\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.10679\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4052 - val_loss: 0.1207\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.10679\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4070 - val_loss: 0.1245\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.10679\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3867 - val_loss: 0.1223\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.10679\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3836 - val_loss: 0.1263\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.10679\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3919 - val_loss: 0.1305\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.10679\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3993 - val_loss: 0.1350\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.10679\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3702 - val_loss: 0.1378\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.10679\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3918 - val_loss: 0.1386\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.10679\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4106 - val_loss: 0.1393\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.10679\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3994 - val_loss: 0.1454\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.10679\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3769 - val_loss: 0.1478\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.10679\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3942 - val_loss: 0.1514\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.10679\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4012 - val_loss: 0.1552\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.10679\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3987 - val_loss: 0.1609\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.10679\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3933 - val_loss: 0.1655\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.10679\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3851 - val_loss: 0.1679\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.10679\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4025 - val_loss: 0.1596\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.10679\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3882 - val_loss: 0.1681\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.10679\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3793 - val_loss: 0.1721\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.10679\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3798 - val_loss: 0.1742\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.10679\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3731 - val_loss: 0.1733\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.10679\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4000 - val_loss: 0.1702\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.10679\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3849 - val_loss: 0.1762\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.10679\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3911 - val_loss: 0.1860\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.10679\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3790 - val_loss: 0.1868\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.10679\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3795 - val_loss: 0.1929\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.10679\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3963 - val_loss: 0.1943\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.10679\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3856 - val_loss: 0.1875\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.10679\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3746 - val_loss: 0.1907\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.10679\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3588 - val_loss: 0.1919\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.10679\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4064 - val_loss: 0.1942\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.10679\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3691 - val_loss: 0.1951\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.10679\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4110 - val_loss: 0.2009\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.10679\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3779 - val_loss: 0.1969\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.10679\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3820 - val_loss: 0.1866\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.10679\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3797 - val_loss: 0.1923\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.10679\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3850 - val_loss: 0.1852\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.10679\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3999 - val_loss: 0.1828\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.10679\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3940 - val_loss: 0.1823\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.10679\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4025 - val_loss: 0.1891\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.10679\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3922 - val_loss: 0.1939\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.10679\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.4086 - val_loss: 0.1957\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.10679\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3721 - val_loss: 0.2042\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.10679\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3494 - val_loss: 0.2073\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.10679\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3854 - val_loss: 0.2068\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.10679\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4106 - val_loss: 0.2087\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.10679\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3983 - val_loss: 0.2089\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.10679\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3806 - val_loss: 0.2073\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.10679\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3739 - val_loss: 0.2034\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.10679\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3930 - val_loss: 0.2004\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.10679\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3959 - val_loss: 0.2010\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.10679\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.3808 - val_loss: 0.2068\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.10679\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3729 - val_loss: 0.2127\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.10679\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3804 - val_loss: 0.2163\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.10679\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3853 - val_loss: 0.2167\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.10679\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3940 - val_loss: 0.2158\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.10679\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3858 - val_loss: 0.2158\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.10679\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3848 - val_loss: 0.2253\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.10679\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3884 - val_loss: 0.2275\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.10679\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3920 - val_loss: 0.2285\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.10679\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3952 - val_loss: 0.2280\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.10679\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3791 - val_loss: 0.2119\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.10679\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3849 - val_loss: 0.2028\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.10679\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4081 - val_loss: 0.2013\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.10679\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3983 - val_loss: 0.2040\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.10679\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4090 - val_loss: 0.2007\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.10679\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3812 - val_loss: 0.2027\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.10679\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3831 - val_loss: 0.2045\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.10679\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4196 - val_loss: 0.2116\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.10679\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3769 - val_loss: 0.2039\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.10679\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3735 - val_loss: 0.1900\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.10679\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3785 - val_loss: 0.1946\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.10679\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3797 - val_loss: 0.2040\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.10679\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3917 - val_loss: 0.2063\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.10679\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3756 - val_loss: 0.1984\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.10679\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4014 - val_loss: 0.2045\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.10679\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3753 - val_loss: 0.2008\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.10679\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3761 - val_loss: 0.2016\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.10679\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3998 - val_loss: 0.2097\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.10679\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3751 - val_loss: 0.2137\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.10679\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3906 - val_loss: 0.2109\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.10679\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3971 - val_loss: 0.2152\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.10679\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3893 - val_loss: 0.2197\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.10679\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3714 - val_loss: 0.2172\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.10679\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3923 - val_loss: 0.2120\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.10679\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3781 - val_loss: 0.1986\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.10679\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3715 - val_loss: 0.1995\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.10679\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3743 - val_loss: 0.2065\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.10679\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3865 - val_loss: 0.2073\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.10679\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3970 - val_loss: 0.2071\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.10679\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.4027 - val_loss: 0.2070\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.10679\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3902 - val_loss: 0.2052\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.10679\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3925 - val_loss: 0.2031\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.10679\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3743 - val_loss: 0.2101\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.10679\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3937 - val_loss: 0.2013\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.10679\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3560 - val_loss: 0.2045\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.10679\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3662 - val_loss: 0.2003\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.10679\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3626 - val_loss: 0.2006\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.10679\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3810 - val_loss: 0.2048\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.10679\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.4164 - val_loss: 0.2032\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.10679\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4010 - val_loss: 0.1994\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.10679\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.3839 - val_loss: 0.1945\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.10679\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4012 - val_loss: 0.2031\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.10679\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.4332 - val_loss: 0.1874\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.10679\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3818 - val_loss: 0.1909\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.10679\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3910 - val_loss: 0.1739\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.10679\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4004 - val_loss: 0.1789\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.10679\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3751 - val_loss: 0.1797\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.10679\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3631 - val_loss: 0.1792\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.10679\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.4260 - val_loss: 0.1835\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.10679\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4000 - val_loss: 0.1891\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.10679\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4001 - val_loss: 0.1952\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.10679\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3722 - val_loss: 0.2104\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.10679\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3850 - val_loss: 0.2125\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.10679\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4060 - val_loss: 0.2180\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.10679\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3643 - val_loss: 0.2209\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.10679\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3924 - val_loss: 0.2128\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.10679\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3745 - val_loss: 0.2128\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.10679\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3874 - val_loss: 0.2087\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.10679\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.3629 - val_loss: 0.2083\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.10679\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4022 - val_loss: 0.2014\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.10679\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3797 - val_loss: 0.2112\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.10679\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3732 - val_loss: 0.2161\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.10679\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3697 - val_loss: 0.2155\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.10679\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3776 - val_loss: 0.2060\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.10679\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3849 - val_loss: 0.2080\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.10679\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3778 - val_loss: 0.2060\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.10679\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3789 - val_loss: 0.2004\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.10679\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3947 - val_loss: 0.2016\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.10679\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3598 - val_loss: 0.2016\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.10679\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3778 - val_loss: 0.2010\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.10679\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4021 - val_loss: 0.1997\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.10679\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3712 - val_loss: 0.1862\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.10679\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3769 - val_loss: 0.1900\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.10679\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3938 - val_loss: 0.1891\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.10679\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3728 - val_loss: 0.1894\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.10679\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3652 - val_loss: 0.1923\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.10679\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4158 - val_loss: 0.1931\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.10679\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3837 - val_loss: 0.1886\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.10679\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3642 - val_loss: 0.1746\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.10679\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3814 - val_loss: 0.1779\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.10679\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4053 - val_loss: 0.1622\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.10679\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3985 - val_loss: 0.1663\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.10679\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3876 - val_loss: 0.1604\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.10679\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3764 - val_loss: 0.1642\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.10679\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3766 - val_loss: 0.1694\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.10679\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3754 - val_loss: 0.1774\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.10679\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3593 - val_loss: 0.1799\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.10679\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.3780 - val_loss: 0.1864\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.10679\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3856 - val_loss: 0.1696\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.10679\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3987 - val_loss: 0.1620\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.10679\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3760 - val_loss: 0.1657\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.10679\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3907 - val_loss: 0.1721\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.10679\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3766 - val_loss: 0.1777\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.10679\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3748 - val_loss: 0.1717\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.10679\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3971 - val_loss: 0.1725\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.10679\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3774 - val_loss: 0.1736\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.10679\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4003 - val_loss: 0.1719\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.10679\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4168 - val_loss: 0.1655\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.10679\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4261 - val_loss: 0.1618\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.10679\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3953 - val_loss: 0.1636\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.10679\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4035 - val_loss: 0.1686\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.10679\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3767 - val_loss: 0.1650\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.10679\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4026 - val_loss: 0.1655\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.10679\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3773 - val_loss: 0.1671\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.10679\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3670 - val_loss: 0.1738\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.10679\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3890 - val_loss: 0.1792\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.10679\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3720 - val_loss: 0.1854\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.10679\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3904 - val_loss: 0.1814\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.10679\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3785 - val_loss: 0.1681\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.10679\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.3786 - val_loss: 0.1695\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.10679\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3817 - val_loss: 0.1729\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.10679\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3914 - val_loss: 0.1803\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.10679\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3588 - val_loss: 0.1823\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.10679\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3825 - val_loss: 0.1807\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.10679\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3712 - val_loss: 0.1831\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.10679\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.3869 - val_loss: 0.1861\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.10679\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3614 - val_loss: 0.1796\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.10679\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3812 - val_loss: 0.1733\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.10679\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3773 - val_loss: 0.1701\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.10679\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3856 - val_loss: 0.1666\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.10679\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3746 - val_loss: 0.1568\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.10679\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3758 - val_loss: 0.1568\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.10679\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4037 - val_loss: 0.1626\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.10679\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3713 - val_loss: 0.1700\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.10679\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3723 - val_loss: 0.1721\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.10679\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3782 - val_loss: 0.1803\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.10679\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3963 - val_loss: 0.1842\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.10679\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3833 - val_loss: 0.1843\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.10679\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3655 - val_loss: 0.1714\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.10679\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3758 - val_loss: 0.1780\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.10679\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3770 - val_loss: 0.1797\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.10679\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4098 - val_loss: 0.1861\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.10679\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3928 - val_loss: 0.1862\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.10679\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3974 - val_loss: 0.1873\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.10679\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4428 - val_loss: 0.1824\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.10679\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.3747 - val_loss: 0.1650\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.10679\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3765 - val_loss: 0.1668\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.10679\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4154 - val_loss: 0.1690\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.10679\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3931 - val_loss: 0.1705\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.10679\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.3939 - val_loss: 0.1694\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.10679\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3816 - val_loss: 0.1721\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.10679\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3861 - val_loss: 0.1770\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.10679\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4015 - val_loss: 0.1732\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.10679\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.3907 - val_loss: 0.1751\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.10679\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3542 - val_loss: 0.1813\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.10679\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3749 - val_loss: 0.1873\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.10679\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3791 - val_loss: 0.1941\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.10679\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3812 - val_loss: 0.1882\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.10679\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3883 - val_loss: 0.1901\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.10679\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4287 - val_loss: 0.1931\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.10679\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3907 - val_loss: 0.1847\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.10679\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4213 - val_loss: 0.1872\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.10679\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3932 - val_loss: 0.1827\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.10679\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.3729 - val_loss: 0.1700\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.10679\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3626 - val_loss: 0.1680\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.10679\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3978 - val_loss: 0.1532\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.10679\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3825 - val_loss: 0.1556\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.10679\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3816 - val_loss: 0.1563\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.10679\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3581 - val_loss: 0.1465\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.10679\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3750 - val_loss: 0.1480\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.10679\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3989 - val_loss: 0.1486\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.10679\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3953 - val_loss: 0.1506\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.10679\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3793 - val_loss: 0.1461\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.10679\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3717 - val_loss: 0.1485\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.10679\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.4142 - val_loss: 0.1494\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.10679\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3897 - val_loss: 0.1506\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.10679\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4016 - val_loss: 0.1514\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.10679\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3659 - val_loss: 0.1486\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.10679\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4077 - val_loss: 0.1514\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.10679\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3838 - val_loss: 0.1570\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.10679\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3913 - val_loss: 0.1545\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.10679\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3791 - val_loss: 0.1479\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.10679\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3671 - val_loss: 0.1524\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.10679\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3567 - val_loss: 0.1536\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.10679\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.3961 - val_loss: 0.1591\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.10679\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3775 - val_loss: 0.1576\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.10679\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3731 - val_loss: 0.1470\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.10679\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3864 - val_loss: 0.1494\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.10679\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3685 - val_loss: 0.1512\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.10679\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3799 - val_loss: 0.1533\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.10679\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3578 - val_loss: 0.1539\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.10679\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3780 - val_loss: 0.1510\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.10679\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3831 - val_loss: 0.1559\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.10679\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3751 - val_loss: 0.1653\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.10679\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.3900 - val_loss: 0.1728\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.10679\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3811 - val_loss: 0.1801\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.10679\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3876 - val_loss: 0.1832\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.10679\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3830 - val_loss: 0.1866\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.10679\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3648 - val_loss: 0.1888\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.10679\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3588 - val_loss: 0.1924\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.10679\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3948 - val_loss: 0.1962\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.10679\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3969 - val_loss: 0.2072\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.10679\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3759 - val_loss: 0.2014\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.10679\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3750 - val_loss: 0.2023\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.10679\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3588 - val_loss: 0.2058\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.10679\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3747 - val_loss: 0.2151\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.10679\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3820 - val_loss: 0.2186\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.10679\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3633 - val_loss: 0.2227\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.10679\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3745 - val_loss: 0.2264\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.10679\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3745 - val_loss: 0.2348\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.10679\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3919 - val_loss: 0.2384\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.10679\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3698 - val_loss: 0.2197\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.10679\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3987 - val_loss: 0.2229\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.10679\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.3895 - val_loss: 0.2294\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.10679\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3746 - val_loss: 0.2312\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.10679\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3774 - val_loss: 0.2362\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.10679\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3693 - val_loss: 0.2381\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.10679\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3775 - val_loss: 0.2359\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.10679\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3765 - val_loss: 0.2362\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.10679\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3663 - val_loss: 0.2414\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.10679\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3835 - val_loss: 0.2282\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.10679\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3806 - val_loss: 0.2366\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.10679\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3984 - val_loss: 0.2446\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.10679\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3677 - val_loss: 0.2332\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.10679\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.3641 - val_loss: 0.2300\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.10679\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3851 - val_loss: 0.2421\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.10679\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4022 - val_loss: 0.2451\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.10679\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3618 - val_loss: 0.2496\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.10679\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3667 - val_loss: 0.2564\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.10679\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3940 - val_loss: 0.2572\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.10679\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4025 - val_loss: 0.2508\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.10679\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3879 - val_loss: 0.2538\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.10679\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3865 - val_loss: 0.2632\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.10679\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3759 - val_loss: 0.2618\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.10679\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.3610 - val_loss: 0.2482\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.10679\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.3656 - val_loss: 0.2479\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.10679\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3647 - val_loss: 0.2450\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.10679\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4064 - val_loss: 0.2356\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.10679\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3511 - val_loss: 0.2401\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.10679\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3809 - val_loss: 0.2420\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.10679\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3811 - val_loss: 0.2281\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.10679\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3552 - val_loss: 0.2301\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.10679\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3979 - val_loss: 0.2331\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.10679\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3584 - val_loss: 0.2296\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.10679\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3956 - val_loss: 0.2205\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.10679\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3919 - val_loss: 0.2134\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.10679\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3568 - val_loss: 0.2207\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.10679\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3723 - val_loss: 0.2238\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.10679\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3765 - val_loss: 0.2231\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.10679\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3957 - val_loss: 0.2302\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.10679\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3869 - val_loss: 0.2266\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.10679\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.4044 - val_loss: 0.2148\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.10679\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3952 - val_loss: 0.2055\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.10679\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4072 - val_loss: 0.2103\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.10679\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3835 - val_loss: 0.2172\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.10679\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3802 - val_loss: 0.2202\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.10679\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3681 - val_loss: 0.2117\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.10679\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3685 - val_loss: 0.2109\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.10679\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3698 - val_loss: 0.2092\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.10679\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3742 - val_loss: 0.2095\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.10679\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3689 - val_loss: 0.2136\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.10679\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4026 - val_loss: 0.2121\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.10679\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4001 - val_loss: 0.2073\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.10679\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3680 - val_loss: 0.2059\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.10679\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4065 - val_loss: 0.2122\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.10679\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3989 - val_loss: 0.2169\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.10679\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.4057 - val_loss: 0.2182\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.10679\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3841 - val_loss: 0.2069\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.10679\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3824 - val_loss: 0.2102\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.10679\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3798 - val_loss: 0.2115\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.10679\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4085 - val_loss: 0.1996\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.10679\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3719 - val_loss: 0.2016\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.10679\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3781 - val_loss: 0.2001\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.10679\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3860 - val_loss: 0.2009\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.10679\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3884 - val_loss: 0.2053\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.10679\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3739 - val_loss: 0.2098\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.10679\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.4054 - val_loss: 0.2174\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.10679\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3668 - val_loss: 0.2117\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.10679\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3717 - val_loss: 0.2083\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.10679\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3848 - val_loss: 0.1972\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.10679\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3651 - val_loss: 0.1947\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.10679\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3496 - val_loss: 0.1949\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.10679\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3970 - val_loss: 0.2050\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.10679\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4020 - val_loss: 0.2041\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.10679\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.4108 - val_loss: 0.2073\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.10679\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.3890 - val_loss: 0.1956\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.10679\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3744 - val_loss: 0.2065\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.10679\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3926 - val_loss: 0.2128\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.10679\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3698 - val_loss: 0.2004\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.10679\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3896 - val_loss: 0.1938\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.10679\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4059 - val_loss: 0.2016\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.10679\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3802 - val_loss: 0.2043\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.10679\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3764 - val_loss: 0.2059\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.10679\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3801 - val_loss: 0.1911\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.10679\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3820 - val_loss: 0.1915\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.10679\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.3910 - val_loss: 0.1844\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.10679\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3642 - val_loss: 0.1883\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.10679\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3885 - val_loss: 0.1889\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.10679\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3777 - val_loss: 0.1862\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.10679\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3883 - val_loss: 0.1857\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.10679\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3878 - val_loss: 0.1835\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.10679\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3756 - val_loss: 0.1881\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.10679\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3706 - val_loss: 0.1897\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.10679\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3909 - val_loss: 0.1938\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.10679\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3750 - val_loss: 0.1958\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.10679\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3755 - val_loss: 0.2037\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.10679\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3946 - val_loss: 0.2046\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.10679\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3833 - val_loss: 0.2073\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.10679\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3721 - val_loss: 0.1968\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.10679\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.4093 - val_loss: 0.1922\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.10679\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3657 - val_loss: 0.1762\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.10679\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3739 - val_loss: 0.1760\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.10679\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3944 - val_loss: 0.1707\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.10679\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.3544 - val_loss: 0.1695\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.10679\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3930 - val_loss: 0.1732\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.10679\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3951 - val_loss: 0.1621\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.10679\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3648 - val_loss: 0.1569\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.10679\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3691 - val_loss: 0.1545\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.10679\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4038 - val_loss: 0.1487\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.10679\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3873 - val_loss: 0.1552\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.10679\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.4241 - val_loss: 0.1548\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.10679\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3841 - val_loss: 0.1601\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.10679\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3982 - val_loss: 0.1554\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.10679\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3794 - val_loss: 0.1559\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.10679\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3740 - val_loss: 0.1641\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.10679\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.3881 - val_loss: 0.1661\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.10679\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3825 - val_loss: 0.1696\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.10679\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 0.3731 - val_loss: 0.1506\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.10679\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 0.3629 - val_loss: 0.1526\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.10679\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.3707 - val_loss: 0.1499\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.10679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKWCfKQ5QSsT"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/my_keras_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovek_yrZQT3D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "091636a3-f1fe-4400-86cc-33379e832f0c"
      },
      "source": [
        "predict_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "predict_df_inverse['target_price']=np.array(model.predict(test_df_x))\n",
        "\n",
        "actual_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "actual_df_inverse['target_price']=np.array(test_df_y)\n",
        "\n",
        "predicted_value_gru_with_regularization=scaler.inverse_transform(predict_df_inverse)[:,-1]\n",
        "actual_value=scaler.inverse_transform(actual_df_inverse)[:,-1]\n",
        "plt.plot(predicted_value_gru_with_regularization , color=\"red\")\n",
        "plt.plot(actual_value, color=\"green\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f66e96bdd50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRwvz83nOa57"
      },
      "source": [
        "final_df = pd.DataFrame(columns=['Date','Symbol','Open','Close','High','Low','High_quantity_transaction'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "cCnK9dIOOa57",
        "outputId": "a29647fa-0282-4caf-e1eb-d08955cc5b8a"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Date, Symbol, Open, Close, High, Low, High_quantity_transaction]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3fTUi1XOa57"
      },
      "source": [
        "Company_symbol = 'NICA'\n",
        "company_df=df[df['Symbol']==Company_symbol]\n",
        "for index,date in enumerate(company_df['Date'].unique()):\n",
        "  Open=company_df[company_df['Date']==date].iloc[0,4]\n",
        "  Close=company_df[company_df['Date']==date].iloc[-1,4]\n",
        "  High=max(company_df[company_df['Date']==date].iloc[:,4])\n",
        "  Low=min(company_df[company_df['Date']==date].iloc[:,4])\n",
        "  High_quantity_value=company_df['ShareQty'].describe()['75%']*4\n",
        "  High_quantity_transaction=np.sum(company_df[company_df['Date']==date]['ShareQty']>High_quantity_value)\n",
        "  final_df.loc[index,['Date','Symbol','Open','Close','High','Low','High_quantity_transaction']]=[date,Company_symbol,Open,Close,High,Low,High_quantity_transaction]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIm5zXCvOa58"
      },
      "source": [
        "final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "final_df.set_index('Date' , inplace=True)\n",
        "final_df.sort_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "nQXqzr8pOa58",
        "outputId": "199967d4-421a-42f9-907a-4a25ed66404c"
      },
      "source": [
        "final_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1</td>\n",
              "      <td>116</td>\n",
              "      <td>120</td>\n",
              "      <td>122</td>\n",
              "      <td>120</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NICA</td>\n",
              "      <td>580</td>\n",
              "      <td>580</td>\n",
              "      <td>582</td>\n",
              "      <td>570</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>180</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Symbol  Open  Close  High  Low  High_quantity_transaction\n",
              "count     180   180    180   180  180                        180\n",
              "unique      1   116    120   122  120                         46\n",
              "top      NICA   580    580   582  570                          2\n",
              "freq      180     5      7     8    6                         12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3AOYXnBOa58",
        "outputId": "ab6021c8-cf42-4ac0-a2b3-514238cdc34f"
      },
      "source": [
        "final_df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Symbol                       0\n",
              "Open                         0\n",
              "Close                        0\n",
              "High                         0\n",
              "Low                          0\n",
              "High_quantity_transaction    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGnsHp3uOa58"
      },
      "source": [
        "final_df.fillna(0 ,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x51NabeOa58"
      },
      "source": [
        "# RSI 14 DAYS\n",
        "n = 14\n",
        "def rma(x, n, y0):\n",
        "    a = (n-1) / n\n",
        "    ak = a**np.arange(len(x)-1, -1, -1)\n",
        "    return np.r_[np.full(n, np.nan), y0, np.cumsum(ak * x) / ak / n + y0 * a**np.arange(1, len(x)+1)]\n",
        "\n",
        "final_df['change'] = final_df['Close'].diff()\n",
        "final_df['gain'] = final_df.change.mask(final_df.change < 0, 0.0)\n",
        "final_df['loss'] = -final_df.change.mask(final_df.change > 0, -0.0)\n",
        "final_df['avg_gain'] = rma(final_df.gain[n+1:].to_numpy(), n, np.nansum(final_df.gain.to_numpy()[:n+1])/n)\n",
        "final_df['avg_loss'] = rma(final_df.loss[n+1:].to_numpy(), n, np.nansum(final_df.loss.to_numpy()[:n+1])/n)\n",
        "final_df['rs'] = final_df.avg_gain / final_df.avg_loss\n",
        "final_df['rsi_14'] = 100 - (100 / (1 + final_df.rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "HlPcKlHKOa58",
        "outputId": "ef3392a4-65a7-4017-d7c8-468ab60d0d40"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>575</td>\n",
              "      <td>572</td>\n",
              "      <td>589</td>\n",
              "      <td>560</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-10</th>\n",
              "      <td>NICA</td>\n",
              "      <td>526</td>\n",
              "      <td>549</td>\n",
              "      <td>564</td>\n",
              "      <td>526</td>\n",
              "      <td>19</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-11</th>\n",
              "      <td>NICA</td>\n",
              "      <td>567</td>\n",
              "      <td>549</td>\n",
              "      <td>569</td>\n",
              "      <td>520</td>\n",
              "      <td>29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-12</th>\n",
              "      <td>NICA</td>\n",
              "      <td>545</td>\n",
              "      <td>564</td>\n",
              "      <td>569</td>\n",
              "      <td>545</td>\n",
              "      <td>18</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-15</th>\n",
              "      <td>NICA</td>\n",
              "      <td>521</td>\n",
              "      <td>545</td>\n",
              "      <td>547</td>\n",
              "      <td>518</td>\n",
              "      <td>16</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>892</td>\n",
              "      <td>887</td>\n",
              "      <td>927</td>\n",
              "      <td>861</td>\n",
              "      <td>14</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.956010</td>\n",
              "      <td>9.032390</td>\n",
              "      <td>0.880831</td>\n",
              "      <td>46.832016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>880</td>\n",
              "      <td>890</td>\n",
              "      <td>909</td>\n",
              "      <td>862</td>\n",
              "      <td>14</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602009</td>\n",
              "      <td>8.387219</td>\n",
              "      <td>0.906380</td>\n",
              "      <td>47.544567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>NICA</td>\n",
              "      <td>855</td>\n",
              "      <td>862</td>\n",
              "      <td>876</td>\n",
              "      <td>853</td>\n",
              "      <td>25</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.059009</td>\n",
              "      <td>9.788132</td>\n",
              "      <td>0.721180</td>\n",
              "      <td>41.900337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>NICA</td>\n",
              "      <td>893</td>\n",
              "      <td>878</td>\n",
              "      <td>913</td>\n",
              "      <td>868</td>\n",
              "      <td>13</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.697651</td>\n",
              "      <td>9.088980</td>\n",
              "      <td>0.846921</td>\n",
              "      <td>45.855843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NICA</td>\n",
              "      <td>863</td>\n",
              "      <td>865</td>\n",
              "      <td>874</td>\n",
              "      <td>854</td>\n",
              "      <td>4</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.147819</td>\n",
              "      <td>9.368338</td>\n",
              "      <td>0.762976</td>\n",
              "      <td>43.277736</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  Open  Close  High  ...  avg_gain  avg_loss        rs     rsi_14\n",
              "Date                                  ...                                         \n",
              "2020-03-05   NICA   575    572   589  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-10   NICA   526    549   564  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-11   NICA   567    549   569  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-12   NICA   545    564   569  ...       NaN       NaN       NaN        NaN\n",
              "2020-03-15   NICA   521    545   547  ...       NaN       NaN       NaN        NaN\n",
              "...           ...   ...    ...   ...  ...       ...       ...       ...        ...\n",
              "2021-03-01   NICA   892    887   927  ...  7.956010  9.032390  0.880831  46.832016\n",
              "2021-03-02   NICA   880    890   909  ...  7.602009  8.387219  0.906380  47.544567\n",
              "2021-03-03   NICA   855    862   876  ...  7.059009  9.788132  0.721180  41.900337\n",
              "2021-03-04   NICA   893    878   913  ...  7.697651  9.088980  0.846921  45.855843\n",
              "2021-03-17   NICA   863    865   874  ...  7.147819  9.368338  0.762976  43.277736\n",
              "\n",
              "[180 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47gCADozOa58"
      },
      "source": [
        "# Calculating _moving_average\n",
        "final_df['MA_7'] = final_df['Close'].rolling(window=7).mean()\n",
        "final_df['MA_14'] = final_df['Close'].rolling(window=14).mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "sVdwqh3XOa58",
        "outputId": "5a919a5f-1cc0-48e6-8388-27c4165d3ca4"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "      <th>MA_7</th>\n",
              "      <th>MA_14</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>575</td>\n",
              "      <td>572</td>\n",
              "      <td>589</td>\n",
              "      <td>560</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-10</th>\n",
              "      <td>NICA</td>\n",
              "      <td>526</td>\n",
              "      <td>549</td>\n",
              "      <td>564</td>\n",
              "      <td>526</td>\n",
              "      <td>19</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-11</th>\n",
              "      <td>NICA</td>\n",
              "      <td>567</td>\n",
              "      <td>549</td>\n",
              "      <td>569</td>\n",
              "      <td>520</td>\n",
              "      <td>29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-12</th>\n",
              "      <td>NICA</td>\n",
              "      <td>545</td>\n",
              "      <td>564</td>\n",
              "      <td>569</td>\n",
              "      <td>545</td>\n",
              "      <td>18</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-15</th>\n",
              "      <td>NICA</td>\n",
              "      <td>521</td>\n",
              "      <td>545</td>\n",
              "      <td>547</td>\n",
              "      <td>518</td>\n",
              "      <td>16</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>892</td>\n",
              "      <td>887</td>\n",
              "      <td>927</td>\n",
              "      <td>861</td>\n",
              "      <td>14</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.956010</td>\n",
              "      <td>9.032390</td>\n",
              "      <td>0.880831</td>\n",
              "      <td>46.832016</td>\n",
              "      <td>924.857143</td>\n",
              "      <td>916.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>880</td>\n",
              "      <td>890</td>\n",
              "      <td>909</td>\n",
              "      <td>862</td>\n",
              "      <td>14</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602009</td>\n",
              "      <td>8.387219</td>\n",
              "      <td>0.906380</td>\n",
              "      <td>47.544567</td>\n",
              "      <td>913.142857</td>\n",
              "      <td>916.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>NICA</td>\n",
              "      <td>855</td>\n",
              "      <td>862</td>\n",
              "      <td>876</td>\n",
              "      <td>853</td>\n",
              "      <td>25</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.059009</td>\n",
              "      <td>9.788132</td>\n",
              "      <td>0.721180</td>\n",
              "      <td>41.900337</td>\n",
              "      <td>901.142857</td>\n",
              "      <td>914.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>NICA</td>\n",
              "      <td>893</td>\n",
              "      <td>878</td>\n",
              "      <td>913</td>\n",
              "      <td>868</td>\n",
              "      <td>13</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.697651</td>\n",
              "      <td>9.088980</td>\n",
              "      <td>0.846921</td>\n",
              "      <td>45.855843</td>\n",
              "      <td>895.428571</td>\n",
              "      <td>912.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NICA</td>\n",
              "      <td>863</td>\n",
              "      <td>865</td>\n",
              "      <td>874</td>\n",
              "      <td>854</td>\n",
              "      <td>4</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.147819</td>\n",
              "      <td>9.368338</td>\n",
              "      <td>0.762976</td>\n",
              "      <td>43.277736</td>\n",
              "      <td>885.142857</td>\n",
              "      <td>911.142857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  Open  Close  ...     rsi_14        MA_7       MA_14\n",
              "Date                            ...                                   \n",
              "2020-03-05   NICA   575    572  ...        NaN         NaN         NaN\n",
              "2020-03-10   NICA   526    549  ...        NaN         NaN         NaN\n",
              "2020-03-11   NICA   567    549  ...        NaN         NaN         NaN\n",
              "2020-03-12   NICA   545    564  ...        NaN         NaN         NaN\n",
              "2020-03-15   NICA   521    545  ...        NaN         NaN         NaN\n",
              "...           ...   ...    ...  ...        ...         ...         ...\n",
              "2021-03-01   NICA   892    887  ...  46.832016  924.857143  916.285714\n",
              "2021-03-02   NICA   880    890  ...  47.544567  913.142857  916.357143\n",
              "2021-03-03   NICA   855    862  ...  41.900337  901.142857  914.500000\n",
              "2021-03-04   NICA   893    878  ...  45.855843  895.428571  912.928571\n",
              "2021-03-17   NICA   863    865  ...  43.277736  885.142857  911.142857\n",
              "\n",
              "[180 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7LyVBQBOa59"
      },
      "source": [
        "look_back_period=3\n",
        "for i in range(look_back_period):\n",
        "  final_df[f'previous_day_price_{i+1}']=final_df[\"Close\"].shift(i)\n",
        "final_df['target_price']=final_df[\"Close\"].shift(-look_back_period)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "eaWGpWTeOa59",
        "outputId": "0bb4a234-7e76-4d99-de02-90e01c588103"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "      <th>MA_7</th>\n",
              "      <th>MA_14</th>\n",
              "      <th>previous_day_price_1</th>\n",
              "      <th>previous_day_price_2</th>\n",
              "      <th>previous_day_price_3</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-03-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>575</td>\n",
              "      <td>572</td>\n",
              "      <td>589</td>\n",
              "      <td>560</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>572</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>564.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-10</th>\n",
              "      <td>NICA</td>\n",
              "      <td>526</td>\n",
              "      <td>549</td>\n",
              "      <td>564</td>\n",
              "      <td>526</td>\n",
              "      <td>19</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>549</td>\n",
              "      <td>572.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>545.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-11</th>\n",
              "      <td>NICA</td>\n",
              "      <td>567</td>\n",
              "      <td>549</td>\n",
              "      <td>569</td>\n",
              "      <td>520</td>\n",
              "      <td>29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>549</td>\n",
              "      <td>549.0</td>\n",
              "      <td>572.0</td>\n",
              "      <td>517.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-12</th>\n",
              "      <td>NICA</td>\n",
              "      <td>545</td>\n",
              "      <td>564</td>\n",
              "      <td>569</td>\n",
              "      <td>545</td>\n",
              "      <td>18</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>564</td>\n",
              "      <td>549.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-03-15</th>\n",
              "      <td>NICA</td>\n",
              "      <td>521</td>\n",
              "      <td>545</td>\n",
              "      <td>547</td>\n",
              "      <td>518</td>\n",
              "      <td>16</td>\n",
              "      <td>-19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>545</td>\n",
              "      <td>564.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>508.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>892</td>\n",
              "      <td>887</td>\n",
              "      <td>927</td>\n",
              "      <td>861</td>\n",
              "      <td>14</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.956010</td>\n",
              "      <td>9.032390</td>\n",
              "      <td>0.880831</td>\n",
              "      <td>46.832016</td>\n",
              "      <td>924.857143</td>\n",
              "      <td>916.285714</td>\n",
              "      <td>887</td>\n",
              "      <td>900.0</td>\n",
              "      <td>914.0</td>\n",
              "      <td>878.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>880</td>\n",
              "      <td>890</td>\n",
              "      <td>909</td>\n",
              "      <td>862</td>\n",
              "      <td>14</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602009</td>\n",
              "      <td>8.387219</td>\n",
              "      <td>0.906380</td>\n",
              "      <td>47.544567</td>\n",
              "      <td>913.142857</td>\n",
              "      <td>916.357143</td>\n",
              "      <td>890</td>\n",
              "      <td>887.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>865.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-03</th>\n",
              "      <td>NICA</td>\n",
              "      <td>855</td>\n",
              "      <td>862</td>\n",
              "      <td>876</td>\n",
              "      <td>853</td>\n",
              "      <td>25</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>7.059009</td>\n",
              "      <td>9.788132</td>\n",
              "      <td>0.721180</td>\n",
              "      <td>41.900337</td>\n",
              "      <td>901.142857</td>\n",
              "      <td>914.500000</td>\n",
              "      <td>862</td>\n",
              "      <td>890.0</td>\n",
              "      <td>887.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-04</th>\n",
              "      <td>NICA</td>\n",
              "      <td>893</td>\n",
              "      <td>878</td>\n",
              "      <td>913</td>\n",
              "      <td>868</td>\n",
              "      <td>13</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.697651</td>\n",
              "      <td>9.088980</td>\n",
              "      <td>0.846921</td>\n",
              "      <td>45.855843</td>\n",
              "      <td>895.428571</td>\n",
              "      <td>912.928571</td>\n",
              "      <td>878</td>\n",
              "      <td>862.0</td>\n",
              "      <td>890.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-17</th>\n",
              "      <td>NICA</td>\n",
              "      <td>863</td>\n",
              "      <td>865</td>\n",
              "      <td>874</td>\n",
              "      <td>854</td>\n",
              "      <td>4</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.147819</td>\n",
              "      <td>9.368338</td>\n",
              "      <td>0.762976</td>\n",
              "      <td>43.277736</td>\n",
              "      <td>885.142857</td>\n",
              "      <td>911.142857</td>\n",
              "      <td>865</td>\n",
              "      <td>878.0</td>\n",
              "      <td>862.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol  Open  ...  previous_day_price_3  target_price\n",
              "Date                     ...                                    \n",
              "2020-03-05   NICA   575  ...                   NaN         564.0\n",
              "2020-03-10   NICA   526  ...                   NaN         545.0\n",
              "2020-03-11   NICA   567  ...                 572.0         517.0\n",
              "2020-03-12   NICA   545  ...                 549.0         515.0\n",
              "2020-03-15   NICA   521  ...                 549.0         508.0\n",
              "...           ...   ...  ...                   ...           ...\n",
              "2021-03-01   NICA   892  ...                 914.0         878.0\n",
              "2021-03-02   NICA   880  ...                 900.0         865.0\n",
              "2021-03-03   NICA   855  ...                 887.0           NaN\n",
              "2021-03-04   NICA   893  ...                 890.0           NaN\n",
              "2021-03-17   NICA   863  ...                 862.0           NaN\n",
              "\n",
              "[180 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BWSml__Oa59"
      },
      "source": [
        "final_df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSCFgDniOa59"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(final_df.iloc[:,1:])\n",
        "final_df.iloc[:,1:] =scaler.transform(final_df.iloc[:,1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "DCPl6ue9Oa59",
        "outputId": "cb95e052-52fc-4346-ce3e-4f3aae56bc22"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>High_quantity_transaction</th>\n",
              "      <th>change</th>\n",
              "      <th>gain</th>\n",
              "      <th>loss</th>\n",
              "      <th>avg_gain</th>\n",
              "      <th>avg_loss</th>\n",
              "      <th>rs</th>\n",
              "      <th>rsi_14</th>\n",
              "      <th>MA_7</th>\n",
              "      <th>MA_14</th>\n",
              "      <th>previous_day_price_1</th>\n",
              "      <th>previous_day_price_2</th>\n",
              "      <th>previous_day_price_3</th>\n",
              "      <th>target_price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-07-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.052783</td>\n",
              "      <td>-1.085382</td>\n",
              "      <td>-1.108542</td>\n",
              "      <td>-1.140584</td>\n",
              "      <td>-1.107547</td>\n",
              "      <td>2.922840</td>\n",
              "      <td>3.615779</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>0.165952</td>\n",
              "      <td>2.128917</td>\n",
              "      <td>-1.022598</td>\n",
              "      <td>-1.560250</td>\n",
              "      <td>-1.393057</td>\n",
              "      <td>-1.167805</td>\n",
              "      <td>-1.085382</td>\n",
              "      <td>-1.527632</td>\n",
              "      <td>-1.750096</td>\n",
              "      <td>-1.392426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.394220</td>\n",
              "      <td>-1.078486</td>\n",
              "      <td>-1.135386</td>\n",
              "      <td>-1.433049</td>\n",
              "      <td>-0.730972</td>\n",
              "      <td>-0.073816</td>\n",
              "      <td>-0.471062</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>0.040198</td>\n",
              "      <td>1.811555</td>\n",
              "      <td>-1.012681</td>\n",
              "      <td>-1.536300</td>\n",
              "      <td>-1.345201</td>\n",
              "      <td>-1.177574</td>\n",
              "      <td>-1.078486</td>\n",
              "      <td>-1.066067</td>\n",
              "      <td>-1.502946</td>\n",
              "      <td>-1.337257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-05</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.380284</td>\n",
              "      <td>-1.423327</td>\n",
              "      <td>-1.323292</td>\n",
              "      <td>-1.440182</td>\n",
              "      <td>-0.806287</td>\n",
              "      <td>-2.389414</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>3.996616</td>\n",
              "      <td>-0.092016</td>\n",
              "      <td>2.816005</td>\n",
              "      <td>-1.313694</td>\n",
              "      <td>-2.349172</td>\n",
              "      <td>-1.357165</td>\n",
              "      <td>-1.213049</td>\n",
              "      <td>-1.423327</td>\n",
              "      <td>-1.059178</td>\n",
              "      <td>-1.042973</td>\n",
              "      <td>-1.309672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-06</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.289698</td>\n",
              "      <td>-1.347462</td>\n",
              "      <td>-1.336714</td>\n",
              "      <td>-1.383116</td>\n",
              "      <td>-1.107547</td>\n",
              "      <td>0.380223</td>\n",
              "      <td>0.148157</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>-0.044909</td>\n",
              "      <td>2.449565</td>\n",
              "      <td>-1.219214</td>\n",
              "      <td>-2.073424</td>\n",
              "      <td>-1.353177</td>\n",
              "      <td>-1.250581</td>\n",
              "      <td>-1.347462</td>\n",
              "      <td>-1.403630</td>\n",
              "      <td>-1.036108</td>\n",
              "      <td>-1.316568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-07-07</th>\n",
              "      <td>NICA</td>\n",
              "      <td>-1.226985</td>\n",
              "      <td>-1.292287</td>\n",
              "      <td>-1.222628</td>\n",
              "      <td>-1.247583</td>\n",
              "      <td>-1.107547</td>\n",
              "      <td>0.244011</td>\n",
              "      <td>-0.037609</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>-0.047497</td>\n",
              "      <td>2.109300</td>\n",
              "      <td>-1.145215</td>\n",
              "      <td>-1.871284</td>\n",
              "      <td>-1.327255</td>\n",
              "      <td>-1.274231</td>\n",
              "      <td>-1.292287</td>\n",
              "      <td>-1.327850</td>\n",
              "      <td>-1.379371</td>\n",
              "      <td>-1.254503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-24</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.650843</td>\n",
              "      <td>1.728522</td>\n",
              "      <td>1.716768</td>\n",
              "      <td>1.719869</td>\n",
              "      <td>0.700014</td>\n",
              "      <td>0.743454</td>\n",
              "      <td>0.643531</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>0.337622</td>\n",
              "      <td>0.302184</td>\n",
              "      <td>-0.025752</td>\n",
              "      <td>0.213581</td>\n",
              "      <td>1.806310</td>\n",
              "      <td>1.756056</td>\n",
              "      <td>1.728522</td>\n",
              "      <td>1.613766</td>\n",
              "      <td>1.819843</td>\n",
              "      <td>1.338441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-25</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.546321</td>\n",
              "      <td>1.569895</td>\n",
              "      <td>1.790589</td>\n",
              "      <td>1.541536</td>\n",
              "      <td>0.022179</td>\n",
              "      <td>-1.163509</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>1.545173</td>\n",
              "      <td>0.184163</td>\n",
              "      <td>0.712909</td>\n",
              "      <td>-0.455260</td>\n",
              "      <td>-0.420132</td>\n",
              "      <td>1.793349</td>\n",
              "      <td>1.774565</td>\n",
              "      <td>1.569895</td>\n",
              "      <td>1.744658</td>\n",
              "      <td>1.627616</td>\n",
              "      <td>1.359130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-02-28</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.379087</td>\n",
              "      <td>1.473339</td>\n",
              "      <td>1.347666</td>\n",
              "      <td>1.455937</td>\n",
              "      <td>0.549384</td>\n",
              "      <td>-0.754874</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>0.728025</td>\n",
              "      <td>0.041666</td>\n",
              "      <td>0.860450</td>\n",
              "      <td>-0.655335</td>\n",
              "      <td>-0.774883</td>\n",
              "      <td>1.763439</td>\n",
              "      <td>1.784848</td>\n",
              "      <td>1.473339</td>\n",
              "      <td>1.586210</td>\n",
              "      <td>1.758056</td>\n",
              "      <td>1.166038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-01</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.441800</td>\n",
              "      <td>1.383681</td>\n",
              "      <td>1.528861</td>\n",
              "      <td>1.334670</td>\n",
              "      <td>-0.128451</td>\n",
              "      <td>-0.709470</td>\n",
              "      <td>-0.532984</td>\n",
              "      <td>0.637230</td>\n",
              "      <td>-0.090654</td>\n",
              "      <td>0.971470</td>\n",
              "      <td>-0.814272</td>\n",
              "      <td>-1.090853</td>\n",
              "      <td>1.720568</td>\n",
              "      <td>1.778164</td>\n",
              "      <td>1.383681</td>\n",
              "      <td>1.489764</td>\n",
              "      <td>1.600155</td>\n",
              "      <td>1.276376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-02</th>\n",
              "      <td>NICA</td>\n",
              "      <td>1.358182</td>\n",
              "      <td>1.404371</td>\n",
              "      <td>1.408064</td>\n",
              "      <td>1.341804</td>\n",
              "      <td>-0.128451</td>\n",
              "      <td>0.016992</td>\n",
              "      <td>-0.347218</td>\n",
              "      <td>-0.543094</td>\n",
              "      <td>-0.167191</td>\n",
              "      <td>0.736783</td>\n",
              "      <td>-0.774039</td>\n",
              "      <td>-1.007706</td>\n",
              "      <td>1.638814</td>\n",
              "      <td>1.778678</td>\n",
              "      <td>1.404371</td>\n",
              "      <td>1.400206</td>\n",
              "      <td>1.504041</td>\n",
              "      <td>1.186727</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>163 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Symbol      Open  ...  previous_day_price_3  target_price\n",
              "Date                         ...                                    \n",
              "2020-07-01   NICA -1.052783  ...             -1.750096     -1.392426\n",
              "2020-07-02   NICA -1.394220  ...             -1.502946     -1.337257\n",
              "2020-07-05   NICA -1.380284  ...             -1.042973     -1.309672\n",
              "2020-07-06   NICA -1.289698  ...             -1.036108     -1.316568\n",
              "2020-07-07   NICA -1.226985  ...             -1.379371     -1.254503\n",
              "...           ...       ...  ...                   ...           ...\n",
              "2021-02-24   NICA  1.650843  ...              1.819843      1.338441\n",
              "2021-02-25   NICA  1.546321  ...              1.627616      1.359130\n",
              "2021-02-28   NICA  1.379087  ...              1.758056      1.166038\n",
              "2021-03-01   NICA  1.441800  ...              1.600155      1.276376\n",
              "2021-03-02   NICA  1.358182  ...              1.504041      1.186727\n",
              "\n",
              "[163 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqZXEm6FOa59"
      },
      "source": [
        "total_entry= len(final_df)\n",
        "train_df_x=final_df.iloc[:int(.8*total_entry),1:-1]\n",
        "train_df_y=final_df.iloc[:int(.8*total_entry),-1]\n",
        "test_df_x=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "test_df_y=final_df.iloc[int(.8*total_entry):,-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqadc0t7Oa59"
      },
      "source": [
        "train_df_x=np.array(train_df_x)\n",
        "test_df_x=np.array(test_df_x)\n",
        "train_df_y=np.array(train_df_y)\n",
        "test_df_y=np.array(test_df_y)\n",
        "train_df_x =train_df_x.reshape(train_df_x.shape[0],train_df_x.shape[1] , 1)\n",
        "test_df_x = test_df_x.reshape(test_df_x.shape[0],test_df_x.shape[1] , 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0YvWFepPDsR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06kikIrNPSXf",
        "outputId": "dea6f79a-8191-4e81-b24d-7d29612caf0d"
      },
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "# NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(128,input_shape = (train_df_x.shape[1:]),return_sequences=True, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(128,return_sequences=True,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(128,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(tf.keras.layers.Flatten()) \n",
        "\n",
        "# model.add(Dense(32, activation=\"swish\"))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1, activation=\"relu\"))\n",
        "\n",
        "opt=tf.keras.optimizers.Adam()\n",
        "model.compile(loss='huber_loss',\n",
        "              optimizer=opt,\n",
        "              # metrics=[tf.keras.metrics.huber()]\n",
        "              )\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=f'logs/{Company_symbol}')\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True,monitor='val_loss', verbose=1,)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5,min_lr=0.0000001,verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_df_x, train_df_y,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data= (test_df_x,test_df_y),\n",
        "    callbacks = [tensorboard,checkpoint_cb,reduce_lr]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 6s 1s/step - loss: 0.6721 - val_loss: 1.0140\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.01395, saving model to my_keras_model.h5\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5378 - val_loss: 1.0005\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.01395 to 1.00049, saving model to my_keras_model.h5\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5419 - val_loss: 1.0144\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.00049\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4919 - val_loss: 1.0120\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.00049\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5092 - val_loss: 1.0100\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.00049\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5697 - val_loss: 1.0077\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.00049\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5176 - val_loss: 1.0141\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.00049\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4877 - val_loss: 1.0116\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.00049\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5453 - val_loss: 1.0100\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.00049\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4996 - val_loss: 1.0128\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.00049\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.5017 - val_loss: 1.0116\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.00049\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5204 - val_loss: 1.0104\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.00049\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4982 - val_loss: 1.0088\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.00049\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5234 - val_loss: 1.0017\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.00049\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5177 - val_loss: 0.9946\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.00049 to 0.99463, saving model to my_keras_model.h5\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4874 - val_loss: 0.9892\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.99463 to 0.98920, saving model to my_keras_model.h5\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4770 - val_loss: 0.9848\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.98920 to 0.98479, saving model to my_keras_model.h5\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5013 - val_loss: 0.9672\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.98479 to 0.96717, saving model to my_keras_model.h5\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4693 - val_loss: 0.9434\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.96717 to 0.94340, saving model to my_keras_model.h5\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4575 - val_loss: 0.9262\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.94340 to 0.92622, saving model to my_keras_model.h5\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4917 - val_loss: 0.9202\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.92622 to 0.92017, saving model to my_keras_model.h5\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4541 - val_loss: 0.9115\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.92017 to 0.91147, saving model to my_keras_model.h5\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4579 - val_loss: 0.9016\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.91147 to 0.90162, saving model to my_keras_model.h5\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4620 - val_loss: 0.8958\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.90162 to 0.89575, saving model to my_keras_model.h5\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5248 - val_loss: 0.8948\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.89575 to 0.89480, saving model to my_keras_model.h5\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4831 - val_loss: 0.8909\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.89480 to 0.89090, saving model to my_keras_model.h5\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4917 - val_loss: 0.8910\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.89090\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4419 - val_loss: 0.8843\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.89090 to 0.88434, saving model to my_keras_model.h5\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4838 - val_loss: 0.8708\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.88434 to 0.87079, saving model to my_keras_model.h5\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4949 - val_loss: 0.8681\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.87079 to 0.86810, saving model to my_keras_model.h5\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5212 - val_loss: 0.8596\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.86810 to 0.85958, saving model to my_keras_model.h5\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4769 - val_loss: 0.8502\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.85958 to 0.85016, saving model to my_keras_model.h5\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4664 - val_loss: 0.8557\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.85016\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4619 - val_loss: 0.8396\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.85016 to 0.83957, saving model to my_keras_model.h5\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5064 - val_loss: 0.8209\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.83957 to 0.82095, saving model to my_keras_model.h5\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5121 - val_loss: 0.8138\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.82095 to 0.81378, saving model to my_keras_model.h5\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4876 - val_loss: 0.8073\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.81378 to 0.80729, saving model to my_keras_model.h5\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4979 - val_loss: 0.8035\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.80729 to 0.80347, saving model to my_keras_model.h5\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4620 - val_loss: 0.7928\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.80347 to 0.79283, saving model to my_keras_model.h5\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4864 - val_loss: 0.7718\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.79283 to 0.77176, saving model to my_keras_model.h5\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4819 - val_loss: 0.7534\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.77176 to 0.75336, saving model to my_keras_model.h5\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4429 - val_loss: 0.7402\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.75336 to 0.74017, saving model to my_keras_model.h5\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5107 - val_loss: 0.7383\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.74017 to 0.73826, saving model to my_keras_model.h5\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4661 - val_loss: 0.7474\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.73826\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4753 - val_loss: 0.7426\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.73826\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4859 - val_loss: 0.7392\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.73826\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4758 - val_loss: 0.7485\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.73826\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4250 - val_loss: 0.7387\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.73826\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4727 - val_loss: 0.7277\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.73826 to 0.72771, saving model to my_keras_model.h5\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4732 - val_loss: 0.7254\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.72771 to 0.72543, saving model to my_keras_model.h5\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5194 - val_loss: 0.7154\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.72543 to 0.71543, saving model to my_keras_model.h5\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4838 - val_loss: 0.7028\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.71543 to 0.70283, saving model to my_keras_model.h5\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4707 - val_loss: 0.7027\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.70283 to 0.70266, saving model to my_keras_model.h5\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4730 - val_loss: 0.6966\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.70266 to 0.69660, saving model to my_keras_model.h5\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5022 - val_loss: 0.6820\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.69660 to 0.68202, saving model to my_keras_model.h5\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4852 - val_loss: 0.6697\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.68202 to 0.66975, saving model to my_keras_model.h5\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5008 - val_loss: 0.6583\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.66975 to 0.65831, saving model to my_keras_model.h5\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4930 - val_loss: 0.6540\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.65831 to 0.65401, saving model to my_keras_model.h5\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4840 - val_loss: 0.6408\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.65401 to 0.64084, saving model to my_keras_model.h5\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4656 - val_loss: 0.6299\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.64084 to 0.62992, saving model to my_keras_model.h5\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4514 - val_loss: 0.6194\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.62992 to 0.61940, saving model to my_keras_model.h5\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4836 - val_loss: 0.6047\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.61940 to 0.60470, saving model to my_keras_model.h5\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4259 - val_loss: 0.5852\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.60470 to 0.58521, saving model to my_keras_model.h5\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4478 - val_loss: 0.5664\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.58521 to 0.56637, saving model to my_keras_model.h5\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4351 - val_loss: 0.5453\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.56637 to 0.54528, saving model to my_keras_model.h5\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4687 - val_loss: 0.5416\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.54528 to 0.54158, saving model to my_keras_model.h5\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4818 - val_loss: 0.5285\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.54158 to 0.52848, saving model to my_keras_model.h5\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4882 - val_loss: 0.5189\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.52848 to 0.51893, saving model to my_keras_model.h5\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5012 - val_loss: 0.5016\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.51893 to 0.50165, saving model to my_keras_model.h5\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4338 - val_loss: 0.5008\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.50165 to 0.50084, saving model to my_keras_model.h5\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4548 - val_loss: 0.5022\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.50084\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4595 - val_loss: 0.4965\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.50084 to 0.49650, saving model to my_keras_model.h5\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4312 - val_loss: 0.4768\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.49650 to 0.47675, saving model to my_keras_model.h5\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4720 - val_loss: 0.4628\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.47675 to 0.46282, saving model to my_keras_model.h5\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4640 - val_loss: 0.4636\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.46282\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4613 - val_loss: 0.4540\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.46282 to 0.45401, saving model to my_keras_model.h5\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4370 - val_loss: 0.4315\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.45401 to 0.43146, saving model to my_keras_model.h5\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4520 - val_loss: 0.4241\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.43146 to 0.42410, saving model to my_keras_model.h5\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4926 - val_loss: 0.4223\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.42410 to 0.42226, saving model to my_keras_model.h5\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4891 - val_loss: 0.4248\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.42226\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4918 - val_loss: 0.4113\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.42226 to 0.41131, saving model to my_keras_model.h5\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5050 - val_loss: 0.4133\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.41131\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4573 - val_loss: 0.4089\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.41131 to 0.40895, saving model to my_keras_model.h5\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4683 - val_loss: 0.4007\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.40895 to 0.40072, saving model to my_keras_model.h5\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4410 - val_loss: 0.3943\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.40072 to 0.39429, saving model to my_keras_model.h5\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4945 - val_loss: 0.3835\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.39429 to 0.38352, saving model to my_keras_model.h5\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4766 - val_loss: 0.3776\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.38352 to 0.37759, saving model to my_keras_model.h5\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4481 - val_loss: 0.3644\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.37759 to 0.36443, saving model to my_keras_model.h5\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4547 - val_loss: 0.3632\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.36443 to 0.36318, saving model to my_keras_model.h5\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4801 - val_loss: 0.3504\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.36318 to 0.35039, saving model to my_keras_model.h5\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5131 - val_loss: 0.3335\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.35039 to 0.33352, saving model to my_keras_model.h5\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4564 - val_loss: 0.3294\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.33352 to 0.32938, saving model to my_keras_model.h5\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4861 - val_loss: 0.3147\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.32938 to 0.31473, saving model to my_keras_model.h5\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4573 - val_loss: 0.2973\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.31473 to 0.29731, saving model to my_keras_model.h5\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4615 - val_loss: 0.2851\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.29731 to 0.28515, saving model to my_keras_model.h5\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4617 - val_loss: 0.2704\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.28515 to 0.27040, saving model to my_keras_model.h5\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4707 - val_loss: 0.2612\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.27040 to 0.26124, saving model to my_keras_model.h5\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4731 - val_loss: 0.2539\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.26124 to 0.25395, saving model to my_keras_model.h5\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4417 - val_loss: 0.2434\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.25395 to 0.24344, saving model to my_keras_model.h5\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4447 - val_loss: 0.2317\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.24344 to 0.23169, saving model to my_keras_model.h5\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4842 - val_loss: 0.2393\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.23169\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4538 - val_loss: 0.2407\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.23169\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4996 - val_loss: 0.2436\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.23169\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4694 - val_loss: 0.2372\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.23169\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.5173 - val_loss: 0.2318\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.23169\n",
            "\n",
            "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4645 - val_loss: 0.2228\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.23169 to 0.22276, saving model to my_keras_model.h5\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4730 - val_loss: 0.2241\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.22276\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4562 - val_loss: 0.2131\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.22276 to 0.21312, saving model to my_keras_model.h5\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4983 - val_loss: 0.2091\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.21312 to 0.20909, saving model to my_keras_model.h5\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4510 - val_loss: 0.1994\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.20909 to 0.19942, saving model to my_keras_model.h5\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4644 - val_loss: 0.1948\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.19942 to 0.19480, saving model to my_keras_model.h5\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4745 - val_loss: 0.1867\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.19480 to 0.18667, saving model to my_keras_model.h5\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4958 - val_loss: 0.1868\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.18667\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4788 - val_loss: 0.1809\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.18667 to 0.18091, saving model to my_keras_model.h5\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4666 - val_loss: 0.1732\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.18091 to 0.17324, saving model to my_keras_model.h5\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4688 - val_loss: 0.1635\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.17324 to 0.16351, saving model to my_keras_model.h5\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4633 - val_loss: 0.1614\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.16351 to 0.16143, saving model to my_keras_model.h5\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4333 - val_loss: 0.1584\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.16143 to 0.15841, saving model to my_keras_model.h5\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4708 - val_loss: 0.1511\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.15841 to 0.15114, saving model to my_keras_model.h5\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4550 - val_loss: 0.1464\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.15114 to 0.14635, saving model to my_keras_model.h5\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4363 - val_loss: 0.1432\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.14635 to 0.14325, saving model to my_keras_model.h5\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4464 - val_loss: 0.1421\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.14325 to 0.14211, saving model to my_keras_model.h5\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4590 - val_loss: 0.1377\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.14211 to 0.13775, saving model to my_keras_model.h5\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4739 - val_loss: 0.1324\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.13775 to 0.13243, saving model to my_keras_model.h5\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4516 - val_loss: 0.1335\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.13243\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4330 - val_loss: 0.1299\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.13243 to 0.12988, saving model to my_keras_model.h5\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4843 - val_loss: 0.1268\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.12988 to 0.12680, saving model to my_keras_model.h5\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4661 - val_loss: 0.1239\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.12680 to 0.12391, saving model to my_keras_model.h5\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4835 - val_loss: 0.1223\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.12391 to 0.12231, saving model to my_keras_model.h5\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5040 - val_loss: 0.1201\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.12231 to 0.12012, saving model to my_keras_model.h5\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4463 - val_loss: 0.1178\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.12012 to 0.11780, saving model to my_keras_model.h5\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4389 - val_loss: 0.1157\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.11780 to 0.11571, saving model to my_keras_model.h5\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4327 - val_loss: 0.1143\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.11571 to 0.11425, saving model to my_keras_model.h5\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4712 - val_loss: 0.1140\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.11425 to 0.11400, saving model to my_keras_model.h5\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4586 - val_loss: 0.1131\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.11400 to 0.11305, saving model to my_keras_model.h5\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.5161 - val_loss: 0.1143\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.11305\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4243 - val_loss: 0.1141\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.11305\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4690 - val_loss: 0.1132\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.11305\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4820 - val_loss: 0.1118\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.11305 to 0.11180, saving model to my_keras_model.h5\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4669 - val_loss: 0.1107\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.11180 to 0.11074, saving model to my_keras_model.h5\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4648 - val_loss: 0.1106\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.11074 to 0.11065, saving model to my_keras_model.h5\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4625 - val_loss: 0.1100\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.11065 to 0.11005, saving model to my_keras_model.h5\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4721 - val_loss: 0.1097\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.11005 to 0.10966, saving model to my_keras_model.h5\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4452 - val_loss: 0.1095\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.10966 to 0.10947, saving model to my_keras_model.h5\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4634 - val_loss: 0.1098\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.10947\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4711 - val_loss: 0.1098\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.10947\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4510 - val_loss: 0.1125\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.10947\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4216 - val_loss: 0.1124\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.10947\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4929 - val_loss: 0.1108\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.10947\n",
            "\n",
            "Epoch 00149: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4309 - val_loss: 0.1107\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.10947\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4570 - val_loss: 0.1099\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.10947\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4525 - val_loss: 0.1097\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.10947\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4891 - val_loss: 0.1094\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.10947 to 0.10936, saving model to my_keras_model.h5\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4527 - val_loss: 0.1093\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.10936 to 0.10929, saving model to my_keras_model.h5\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4549 - val_loss: 0.1094\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.10929\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.5032 - val_loss: 0.1098\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.10929\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.4397 - val_loss: 0.1107\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.10929\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5150 - val_loss: 0.1109\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.10929\n",
            "\n",
            "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4608 - val_loss: 0.1115\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.10929\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4677 - val_loss: 0.1132\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.10929\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4538 - val_loss: 0.1136\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.10929\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4489 - val_loss: 0.1135\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.10929\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4877 - val_loss: 0.1131\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.10929\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4777 - val_loss: 0.1143\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.10929\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4820 - val_loss: 0.1169\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.10929\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4854 - val_loss: 0.1198\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.10929\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4281 - val_loss: 0.1223\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.10929\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4375 - val_loss: 0.1233\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.10929\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4614 - val_loss: 0.1252\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.10929\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4758 - val_loss: 0.1254\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.10929\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.4927 - val_loss: 0.1262\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.10929\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4913 - val_loss: 0.1298\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.10929\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4848 - val_loss: 0.1282\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.10929\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4977 - val_loss: 0.1247\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.10929\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4488 - val_loss: 0.1238\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.10929\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4465 - val_loss: 0.1255\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.10929\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4815 - val_loss: 0.1289\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.10929\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4789 - val_loss: 0.1280\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.10929\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4403 - val_loss: 0.1256\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.10929\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4715 - val_loss: 0.1260\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.10929\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4653 - val_loss: 0.1308\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.10929\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4925 - val_loss: 0.1351\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.10929\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4910 - val_loss: 0.1338\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.10929\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4575 - val_loss: 0.1377\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.10929\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4296 - val_loss: 0.1376\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.10929\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4879 - val_loss: 0.1390\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.10929\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4663 - val_loss: 0.1391\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.10929\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4527 - val_loss: 0.1396\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.10929\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4705 - val_loss: 0.1391\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.10929\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4685 - val_loss: 0.1388\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.10929\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4758 - val_loss: 0.1404\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.10929\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4711 - val_loss: 0.1388\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.10929\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4580 - val_loss: 0.1365\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.10929\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4771 - val_loss: 0.1367\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.10929\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4637 - val_loss: 0.1383\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.10929\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4652 - val_loss: 0.1383\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.10929\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4217 - val_loss: 0.1373\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.10929\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4865 - val_loss: 0.1411\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.10929\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4749 - val_loss: 0.1392\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.10929\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4481 - val_loss: 0.1407\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.10929\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4577 - val_loss: 0.1391\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.10929\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4995 - val_loss: 0.1368\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.10929\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4302 - val_loss: 0.1382\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.10929\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4625 - val_loss: 0.1390\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.10929\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4271 - val_loss: 0.1450\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.10929\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4056 - val_loss: 0.1451\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.10929\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4875 - val_loss: 0.1475\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.10929\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4444 - val_loss: 0.1501\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.10929\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4966 - val_loss: 0.1548\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.10929\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4731 - val_loss: 0.1585\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.10929\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4787 - val_loss: 0.1537\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.10929\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4491 - val_loss: 0.1580\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.10929\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4803 - val_loss: 0.1580\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.10929\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4556 - val_loss: 0.1604\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.10929\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4544 - val_loss: 0.1602\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.10929\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4600 - val_loss: 0.1591\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.10929\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4861 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.10929\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4452 - val_loss: 0.1604\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.10929\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4368 - val_loss: 0.1616\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.10929\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4801 - val_loss: 0.1618\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.10929\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4259 - val_loss: 0.1600\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.10929\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4459 - val_loss: 0.1611\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.10929\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4265 - val_loss: 0.1655\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.10929\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4705 - val_loss: 0.1637\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.10929\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4633 - val_loss: 0.1633\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.10929\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4533 - val_loss: 0.1654\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.10929\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4495 - val_loss: 0.1575\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.10929\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4878 - val_loss: 0.1552\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.10929\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4787 - val_loss: 0.1564\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.10929\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4801 - val_loss: 0.1565\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.10929\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5206 - val_loss: 0.1596\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.10929\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.5080 - val_loss: 0.1629\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.10929\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4737 - val_loss: 0.1633\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.10929\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4257 - val_loss: 0.1620\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.10929\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4744 - val_loss: 0.1615\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.10929\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4677 - val_loss: 0.1603\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.10929\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4485 - val_loss: 0.1581\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.10929\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4728 - val_loss: 0.1605\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.10929\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4557 - val_loss: 0.1611\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.10929\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4645 - val_loss: 0.1616\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.10929\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4584 - val_loss: 0.1649\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.10929\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4715 - val_loss: 0.1643\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.10929\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4856 - val_loss: 0.1657\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.10929\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4668 - val_loss: 0.1643\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.10929\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4948 - val_loss: 0.1687\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.10929\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5216 - val_loss: 0.1703\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.10929\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4780 - val_loss: 0.1675\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.10929\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4801 - val_loss: 0.1686\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.10929\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4432 - val_loss: 0.1705\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.10929\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4240 - val_loss: 0.1699\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.10929\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4618 - val_loss: 0.1685\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.10929\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4474 - val_loss: 0.1580\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.10929\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4890 - val_loss: 0.1539\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.10929\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4957 - val_loss: 0.1532\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.10929\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4352 - val_loss: 0.1549\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.10929\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4702 - val_loss: 0.1567\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.10929\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4567 - val_loss: 0.1607\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.10929\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4840 - val_loss: 0.1606\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.10929\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4669 - val_loss: 0.1600\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.10929\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4651 - val_loss: 0.1521\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.10929\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4352 - val_loss: 0.1593\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.10929\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4657 - val_loss: 0.1600\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.10929\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4879 - val_loss: 0.1609\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.10929\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4702 - val_loss: 0.1607\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.10929\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4548 - val_loss: 0.1602\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.10929\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4529 - val_loss: 0.1621\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.10929\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4355 - val_loss: 0.1638\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.10929\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.4853 - val_loss: 0.1680\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.10929\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4963 - val_loss: 0.1691\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.10929\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4298 - val_loss: 0.1679\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.10929\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4440 - val_loss: 0.1665\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.10929\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4746 - val_loss: 0.1709\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.10929\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4805 - val_loss: 0.1737\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.10929\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4930 - val_loss: 0.1652\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.10929\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4791 - val_loss: 0.1736\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.10929\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4492 - val_loss: 0.1780\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.10929\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4255 - val_loss: 0.1807\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.10929\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4552 - val_loss: 0.1861\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.10929\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4398 - val_loss: 0.1884\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.10929\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4743 - val_loss: 0.1903\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.10929\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4669 - val_loss: 0.1812\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.10929\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4773 - val_loss: 0.1744\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.10929\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4769 - val_loss: 0.1813\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.10929\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4744 - val_loss: 0.1815\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.10929\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4724 - val_loss: 0.1704\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.10929\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4285 - val_loss: 0.1748\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.10929\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4462 - val_loss: 0.1796\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.10929\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4349 - val_loss: 0.1811\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.10929\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4510 - val_loss: 0.1781\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.10929\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4918 - val_loss: 0.1816\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.10929\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4481 - val_loss: 0.1752\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.10929\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4563 - val_loss: 0.1782\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.10929\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4833 - val_loss: 0.1745\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.10929\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4665 - val_loss: 0.1795\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.10929\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4806 - val_loss: 0.1819\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.10929\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4461 - val_loss: 0.1840\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.10929\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4671 - val_loss: 0.1722\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.10929\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4754 - val_loss: 0.1679\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.10929\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4641 - val_loss: 0.1653\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.10929\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4875 - val_loss: 0.1700\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.10929\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4792 - val_loss: 0.1699\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.10929\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4473 - val_loss: 0.1677\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.10929\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4652 - val_loss: 0.1664\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.10929\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4576 - val_loss: 0.1692\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.10929\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4684 - val_loss: 0.1676\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.10929\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 0.4181 - val_loss: 0.1651\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.10929\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4084 - val_loss: 0.1541\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.10929\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4817 - val_loss: 0.1517\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.10929\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4455 - val_loss: 0.1509\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.10929\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4683 - val_loss: 0.1507\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.10929\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.5171 - val_loss: 0.1510\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.10929\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4705 - val_loss: 0.1495\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.10929\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.5062 - val_loss: 0.1456\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.10929\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4508 - val_loss: 0.1511\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.10929\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4899 - val_loss: 0.1552\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.10929\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4634 - val_loss: 0.1507\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.10929\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4789 - val_loss: 0.1509\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.10929\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5104 - val_loss: 0.1541\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.10929\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4701 - val_loss: 0.1506\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.10929\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4764 - val_loss: 0.1538\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.10929\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4617 - val_loss: 0.1497\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.10929\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4912 - val_loss: 0.1505\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.10929\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4639 - val_loss: 0.1561\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.10929\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4656 - val_loss: 0.1591\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.10929\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4446 - val_loss: 0.1624\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.10929\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4383 - val_loss: 0.1709\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.10929\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4663 - val_loss: 0.1642\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.10929\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4891 - val_loss: 0.1646\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.10929\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4680 - val_loss: 0.1630\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.10929\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4635 - val_loss: 0.1568\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.10929\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4940 - val_loss: 0.1553\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.10929\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4604 - val_loss: 0.1608\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.10929\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4489 - val_loss: 0.1649\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.10929\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4647 - val_loss: 0.1646\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.10929\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4925 - val_loss: 0.1594\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.10929\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4602 - val_loss: 0.1643\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.10929\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4908 - val_loss: 0.1606\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.10929\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4432 - val_loss: 0.1660\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.10929\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4655 - val_loss: 0.1614\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.10929\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4430 - val_loss: 0.1653\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.10929\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4689 - val_loss: 0.1651\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.10929\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5050 - val_loss: 0.1643\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.10929\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4825 - val_loss: 0.1670\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.10929\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4643 - val_loss: 0.1637\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.10929\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.5014 - val_loss: 0.1591\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.10929\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4691 - val_loss: 0.1585\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.10929\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4815 - val_loss: 0.1597\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.10929\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4472 - val_loss: 0.1509\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.10929\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4507 - val_loss: 0.1521\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.10929\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4410 - val_loss: 0.1554\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.10929\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4535 - val_loss: 0.1581\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.10929\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4522 - val_loss: 0.1651\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.10929\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.5020 - val_loss: 0.1629\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.10929\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4767 - val_loss: 0.1638\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.10929\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4888 - val_loss: 0.1644\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.10929\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4166 - val_loss: 0.1672\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.10929\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4751 - val_loss: 0.1732\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.10929\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4757 - val_loss: 0.1757\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.10929\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4678 - val_loss: 0.1779\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.10929\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4501 - val_loss: 0.1787\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.10929\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4578 - val_loss: 0.1826\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.10929\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4573 - val_loss: 0.1790\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.10929\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4618 - val_loss: 0.1827\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.10929\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4594 - val_loss: 0.1800\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.10929\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4437 - val_loss: 0.1846\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.10929\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4862 - val_loss: 0.1897\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.10929\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.5015 - val_loss: 0.1870\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.10929\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5290 - val_loss: 0.1832\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.10929\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4638 - val_loss: 0.1853\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.10929\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4639 - val_loss: 0.1770\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.10929\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4893 - val_loss: 0.1807\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.10929\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4244 - val_loss: 0.1857\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.10929\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4543 - val_loss: 0.1815\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.10929\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4723 - val_loss: 0.1805\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.10929\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4567 - val_loss: 0.1791\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.10929\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4824 - val_loss: 0.1757\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.10929\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4646 - val_loss: 0.1718\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.10929\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4727 - val_loss: 0.1764\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.10929\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.5125 - val_loss: 0.1782\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.10929\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4660 - val_loss: 0.1739\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.10929\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4782 - val_loss: 0.1772\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.10929\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4321 - val_loss: 0.1803\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.10929\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4403 - val_loss: 0.1813\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.10929\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4786 - val_loss: 0.1831\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.10929\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4659 - val_loss: 0.1820\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.10929\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4672 - val_loss: 0.1780\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.10929\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4469 - val_loss: 0.1703\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.10929\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4673 - val_loss: 0.1665\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.10929\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4789 - val_loss: 0.1679\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.10929\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4298 - val_loss: 0.1675\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.10929\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4279 - val_loss: 0.1688\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.10929\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4613 - val_loss: 0.1711\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.10929\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4683 - val_loss: 0.1726\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.10929\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4640 - val_loss: 0.1753\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.10929\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4408 - val_loss: 0.1742\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.10929\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4836 - val_loss: 0.1755\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.10929\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4847 - val_loss: 0.1760\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.10929\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4419 - val_loss: 0.1832\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.10929\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4467 - val_loss: 0.1785\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.10929\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4229 - val_loss: 0.1826\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.10929\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4525 - val_loss: 0.1883\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.10929\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4785 - val_loss: 0.1917\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.10929\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4630 - val_loss: 0.1920\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.10929\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4709 - val_loss: 0.2023\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.10929\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4371 - val_loss: 0.2077\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.10929\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4560 - val_loss: 0.2091\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.10929\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4777 - val_loss: 0.2064\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.10929\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4381 - val_loss: 0.2057\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.10929\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4299 - val_loss: 0.1932\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.10929\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4554 - val_loss: 0.1877\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.10929\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4556 - val_loss: 0.1924\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.10929\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4997 - val_loss: 0.1863\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.10929\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4276 - val_loss: 0.1875\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.10929\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4424 - val_loss: 0.1889\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.10929\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4360 - val_loss: 0.1880\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.10929\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4685 - val_loss: 0.1790\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.10929\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4908 - val_loss: 0.1698\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.10929\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4633 - val_loss: 0.1656\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.10929\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4901 - val_loss: 0.1668\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.10929\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4696 - val_loss: 0.1682\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.10929\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4409 - val_loss: 0.1701\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.10929\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4541 - val_loss: 0.1624\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.10929\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4485 - val_loss: 0.1582\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.10929\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4463 - val_loss: 0.1582\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.10929\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4952 - val_loss: 0.1571\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.10929\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4829 - val_loss: 0.1496\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.10929\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4804 - val_loss: 0.1490\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.10929\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4908 - val_loss: 0.1489\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.10929\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4545 - val_loss: 0.1491\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.10929\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4589 - val_loss: 0.1504\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.10929\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4555 - val_loss: 0.1479\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.10929\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4631 - val_loss: 0.1429\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.10929\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4469 - val_loss: 0.1429\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.10929\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4931 - val_loss: 0.1427\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.10929\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4784 - val_loss: 0.1457\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.10929\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.5007 - val_loss: 0.1457\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.10929\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4583 - val_loss: 0.1481\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.10929\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4568 - val_loss: 0.1505\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.10929\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4457 - val_loss: 0.1493\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.10929\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4785 - val_loss: 0.1482\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.10929\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4754 - val_loss: 0.1465\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.10929\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4328 - val_loss: 0.1432\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.10929\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4948 - val_loss: 0.1465\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.10929\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4945 - val_loss: 0.1448\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.10929\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4889 - val_loss: 0.1468\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.10929\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4794 - val_loss: 0.1473\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.10929\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4590 - val_loss: 0.1421\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.10929\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4287 - val_loss: 0.1405\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.10929\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4803 - val_loss: 0.1430\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.10929\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4697 - val_loss: 0.1392\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.10929\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4861 - val_loss: 0.1424\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.10929\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4601 - val_loss: 0.1435\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.10929\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4590 - val_loss: 0.1457\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.10929\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4691 - val_loss: 0.1455\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.10929\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4869 - val_loss: 0.1489\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.10929\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4646 - val_loss: 0.1540\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.10929\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4827 - val_loss: 0.1467\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.10929\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4791 - val_loss: 0.1530\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.10929\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4634 - val_loss: 0.1562\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.10929\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4630 - val_loss: 0.1593\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.10929\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4628 - val_loss: 0.1609\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.10929\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4519 - val_loss: 0.1679\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.10929\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4693 - val_loss: 0.1630\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.10929\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4238 - val_loss: 0.1612\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.10929\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4553 - val_loss: 0.1511\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.10929\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4574 - val_loss: 0.1584\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.10929\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4679 - val_loss: 0.1551\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.10929\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4422 - val_loss: 0.1564\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.10929\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4445 - val_loss: 0.1588\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.10929\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4700 - val_loss: 0.1625\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.10929\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4553 - val_loss: 0.1680\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.10929\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4693 - val_loss: 0.1639\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.10929\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4478 - val_loss: 0.1682\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.10929\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4428 - val_loss: 0.1665\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.10929\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4423 - val_loss: 0.1678\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.10929\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.4678 - val_loss: 0.1666\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.10929\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4688 - val_loss: 0.1690\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.10929\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4390 - val_loss: 0.1675\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.10929\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4760 - val_loss: 0.1699\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.10929\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4716 - val_loss: 0.1714\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.10929\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4631 - val_loss: 0.1716\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.10929\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4950 - val_loss: 0.1663\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.10929\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4558 - val_loss: 0.1686\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.10929\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4537 - val_loss: 0.1726\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.10929\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4721 - val_loss: 0.1672\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.10929\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4324 - val_loss: 0.1693\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.10929\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4684 - val_loss: 0.1683\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.10929\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4405 - val_loss: 0.1698\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.10929\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4632 - val_loss: 0.1710\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.10929\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4524 - val_loss: 0.1752\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.10929\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4519 - val_loss: 0.1751\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.10929\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4956 - val_loss: 0.1775\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.10929\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4827 - val_loss: 0.1809\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.10929\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4931 - val_loss: 0.1809\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.10929\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4331 - val_loss: 0.1837\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.10929\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4579 - val_loss: 0.1794\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.10929\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4483 - val_loss: 0.1814\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.10929\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.4737 - val_loss: 0.1696\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.10929\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4265 - val_loss: 0.1709\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.10929\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.4549 - val_loss: 0.1736\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.10929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwaPLe-OPSXg"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/my_keras_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "P5HDXgwbPSXg",
        "outputId": "0a2d7c35-5aef-4157-e53d-7638e483cb5d"
      },
      "source": [
        "predict_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "predict_df_inverse['target_price']=np.array(model.predict(test_df_x))\n",
        "\n",
        "actual_df_inverse=final_df.iloc[int(.8*total_entry):,1:-1]\n",
        "\n",
        "actual_df_inverse['target_price']=np.array(test_df_y)\n",
        "\n",
        "predicted_value_gru_with_regularization_without_broker=scaler.inverse_transform(predict_df_inverse)[:,-1]\n",
        "actual_value=scaler.inverse_transform(actual_df_inverse)[:,-1]\n",
        "plt.plot(predicted_value_gru_with_regularization_without_broker , color=\"red\")\n",
        "plt.plot(actual_value, color=\"green\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f66e82d3e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f66e7915c10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "h5XShqKJPx1J",
        "outputId": "24cbe631-feba-4d34-9504-0531e80b6ddd"
      },
      "source": [
        "plt.figure(figsize=(16,16))\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Share Price')\n",
        "plt.title('Share price prediction of NICA with different factors')\n",
        "plt.plot(actual_value,'g-o',color='green',linewidth=3)\n",
        "plt.plot(predicted_value_lstm,color='lightcoral', marker='D', markeredgecolor='black',linewidth=3)\n",
        "plt.plot(predicted_value_GRU,marker='h', markeredgecolor='black',linewidth=3)\n",
        "plt.plot(predicted_value_gru_with_regularization_without_broker,marker='+', markeredgecolor='black',linewidth=3)\n",
        "plt.legend([\"Actual value\", \"predicted_value_lstm\",\"predicted_value_GRU\",\"predicted_value_gru_with_regularization_without_broker\"])\n",
        "plt.savefig(\"nica.jpg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "uyp3YtsARXFC",
        "outputId": "ade30f9d-96a3-49ef-fd34-56560df82f9f"
      },
      "source": [
        "plt.figure(figsize=(16,16))\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Share Price')\n",
        "plt.title('Share price prediction of NICA with different factors')\n",
        "plt.plot(actual_value,'g-o',color='green',linewidth=3)\n",
        "plt.plot(predicted_value_GRU,marker='h', markeredgecolor='black',linewidth=3)\n",
        "plt.plot(predicted_value_gru_with_regularization_without_broker,marker='+', markeredgecolor='black',linewidth=3)\n",
        "plt.legend([\"Actual value\",\"predicted_value_GRU\",\"predicted_value_gru_with_regularization_without_broker\"])\n",
        "plt.savefig(\"nica.jpg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3rbk9NxYkTP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}